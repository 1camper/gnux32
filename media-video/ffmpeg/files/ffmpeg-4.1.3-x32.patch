From 3aca95d9e1dfaa027e4fe7d15678c4983e1fc7ec Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Mon, 17 Sep 2018 19:42:19 +0200
Subject: [PATCH 01/17] x32 support: libavutil/x86/asm.h
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Add MAXOP and PTROP operand modifiers, define PTR_SIZE and
register macros for x32. GPRs are defined with their 32-bit
sizes, this is in order to minimize the required changes to
existing code, where these registers are usually used to work
with pointer values. In general, the use of these macros should
be avoided, use assembler constraints instead.

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavutil/x86/asm.h | 39 +++++++++++++++++++++++++++++++++++++--
 1 file changed, 37 insertions(+), 2 deletions(-)

diff --git a/libavutil/x86/asm.h b/libavutil/x86/asm.h
index 9bff42d628..7e587709e2 100644
--- a/libavutil/x86/asm.h
+++ b/libavutil/x86/asm.h
@@ -27,18 +27,50 @@
 typedef struct xmm_reg { uint64_t a, b; } xmm_reg;
 typedef struct ymm_reg { uint64_t a, b, c, d; } ymm_reg;
 
-#if ARCH_X86_64
+#if ARCH_X86_64 && defined __ILP32__
+#    define ABI_X32 1
+#else
+#    define ABI_X32 0
+#endif
+
+#if ABI_X32
+#    define FF_OPSIZE "l"
+#    define FF_PTROP "k"
+#    define FF_MAXOP "q"
+#    define FF_REG_a "eax"
+#    define FF_REG_b "ebx"
+#    define FF_REG_c "ecx"
+#    define FF_REG_d "edx"
+#    define FF_REG_D "edi"
+#    define FF_REG_S "esi"
+#    define FF_REG_8 "r8d"
+#    define FF_PTR_SIZE "4"
+typedef int64_t x86_reg;
+
+/* FF_REG_SP is defined in Solaris sys headers, so use FF_REG_sp */
+#    define FF_REG_sp "rsp"
+#    define FF_REG_BP "rbp"
+#    define FF_REGBP   ebp
+#    define FF_REGa    eax
+#    define FF_REGb    ebx
+#    define FF_REGc    ecx
+#    define FF_REGd    edx
+#    define FF_REGSP   rsp
+#    define FF_REG8    r8d
+#elif ARCH_X86_64
 #    define FF_OPSIZE "q"
+#    define FF_PTROP "q"
+#    define FF_MAXOP "q"
 #    define FF_REG_a "rax"
 #    define FF_REG_b "rbx"
 #    define FF_REG_c "rcx"
 #    define FF_REG_d "rdx"
 #    define FF_REG_D "rdi"
 #    define FF_REG_S "rsi"
+#    define FF_REG_8 "r8"
 #    define FF_PTR_SIZE "8"
 typedef int64_t x86_reg;
 
-/* FF_REG_SP is defined in Solaris sys headers, so use FF_REG_sp */
 #    define FF_REG_sp "rsp"
 #    define FF_REG_BP "rbp"
 #    define FF_REGBP   rbp
@@ -47,10 +79,13 @@ typedef int64_t x86_reg;
 #    define FF_REGc    rcx
 #    define FF_REGd    rdx
 #    define FF_REGSP   rsp
+#    define FF_REG8    r8
 
 #elif ARCH_X86_32
 
 #    define FF_OPSIZE "l"
+#    define FF_PTROP "k"
+#    define FF_MAXOP "k"
 #    define FF_REG_a "eax"
 #    define FF_REG_b "ebx"
 #    define FF_REG_c "ecx"
-- 
2.21.0


From a74dbcc300c7555d9073a25ace0d2905778aa59a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 18 Sep 2018 19:41:27 +0200
Subject: [PATCH 02/17] x32: libavutil/x86/cpu.c
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Since x32 defines the GPR macros for 32bit, FF_REG_b cannot be
used to save rbx. Since this is the only place this would be needed
and the problem to be solved really only exists with 32bit PIC code,
specialize cpuid function instead.

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavutil/x86/cpu.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/libavutil/x86/cpu.c b/libavutil/x86/cpu.c
index bcd41a50a2..f043e36804 100644
--- a/libavutil/x86/cpu.c
+++ b/libavutil/x86/cpu.c
@@ -38,6 +38,7 @@
 
 #elif HAVE_INLINE_ASM
 
+#if ARCH_X86_64_32 && CONFIG_PIC
 /* ebx saving is necessary for PIC. gcc seems unable to see it alone */
 #define cpuid(index, eax, ebx, ecx, edx)                        \
     __asm__ volatile (                                          \
@@ -46,6 +47,12 @@
         "xchg   %%"FF_REG_b", %%"FF_REG_S                       \
         : "=a" (eax), "=S" (ebx), "=c" (ecx), "=d" (edx)        \
         : "0" (index), "2"(0))
+#else
+#define cpuid(index, eax, ebx, ecx, edx)                        \
+    __asm__ volatile ("cpuid"                                   \
+        : "=a" (eax), "=b" (ebx), "=c" (ecx), "=d" (edx)        \
+        : "0" (index), "2"(0))
+#endif
 
 #define xgetbv(index, eax, edx)                                 \
     __asm__ (".byte 0x0f, 0x01, 0xd0" : "=a"(eax), "=d"(edx) : "c" (index))
-- 
2.21.0


From b7f34d4d504571fc34aea1a54e14ffcab398b0ee Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 18 Sep 2018 19:44:48 +0200
Subject: [PATCH 03/17] x32: libavcodec/x86/cabac.h libavcodec/x86/h264_cabac.c
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavcodec/x86/cabac.h      | 23 ++++++++++++++---------
 libavcodec/x86/h264_cabac.c | 28 ++++++++++++++--------------
 2 files changed, 28 insertions(+), 23 deletions(-)

diff --git a/libavcodec/x86/cabac.h b/libavcodec/x86/cabac.h
index cfd3b759c9..1c4b05aafc 100644
--- a/libavcodec/x86/cabac.h
+++ b/libavcodec/x86/cabac.h
@@ -56,11 +56,16 @@
 #define BRANCHLESS_GET_CABAC_UPDATE(ret, retq, low, range, tmp) \
         "cmp    "low"       , "tmp"                        \n\t"\
         "cmova  %%ecx       , "range"                      \n\t"\
-        "sbb    %%rcx       , %%rcx                        \n\t"\
+        "sbb    %%"FF_REG_c", %%"FF_REG_c"                 \n\t"\
         "and    %%ecx       , "tmp"                        \n\t"\
-        "xor    %%rcx       , "retq"                       \n\t"\
+        "xor    %%"FF_REG_c", "retq"                       \n\t"\
         "sub    "tmp"       , "low"                        \n\t"
 #else /* HAVE_FAST_CMOV */
+#if ARCH_X86_X64
+#define BRANCHLESS_GET_CABAC_UPDATE_SLOW_CMOV_RET_SIGN_EXTEND  "movslq "ret" , "retq" \n\t"
+#else
+#define BRANCHLESS_GET_CABAC_UPDATE_SLOW_CMOV_RET_SIGN_EXTEND
+#endif
 #define BRANCHLESS_GET_CABAC_UPDATE(ret, retq, low, range, tmp) \
 /* P4 Prescott has crappy cmov,sbb,64-bit shift so avoid them */ \
         "sub    "low"       , "tmp"                        \n\t"\
@@ -72,7 +77,7 @@
         "and    "tmp"       , %%ecx                        \n\t"\
         "sub    %%ecx       , "low"                        \n\t"\
         "xor    "tmp"       , "ret"                        \n\t"\
-        "movslq "ret"       , "retq"                       \n\t"
+        BRANCHLESS_GET_CABAC_UPDATE_SLOW_CMOV_RET_SIGN_EXTEND
 #endif /* HAVE_FAST_CMOV */
 
 #define BRANCHLESS_GET_CABAC(ret, retq, statep, low, lowword, range, rangeq, tmp, tmpbyte, byte, end, norm_off, lps_off, mlps_off, tables) \
@@ -80,7 +85,7 @@
         "mov    "range"     , "tmp"                                     \n\t"\
         "and    $0xC0       , "range"                                   \n\t"\
         "lea    ("ret", "range", 2), %%ecx                              \n\t"\
-        "movzbl "lps_off"("tables", %%rcx), "range"                     \n\t"\
+        "movzbl "lps_off"("tables", %%"FF_REG_c"), "range"              \n\t"\
         "sub    "range"     , "tmp"                                     \n\t"\
         "mov    "tmp"       , %%ecx                                     \n\t"\
         "shl    $17         , "tmp"                                     \n\t"\
@@ -102,7 +107,7 @@
         "shr    $15         , %%ecx                                     \n\t"\
         "bswap  "tmp"                                                   \n\t"\
         "shr    $15         , "tmp"                                     \n\t"\
-        "movzbl "norm_off"("tables", %%rcx), %%ecx                      \n\t"\
+        "movzbl "norm_off"("tables", %%"FF_REG_c"), %%ecx               \n\t"\
         "sub    $0xFFFF     , "tmp"                                     \n\t"\
         "neg    %%ecx                                                   \n\t"\
         "add    $7          , %%ecx                                     \n\t"\
@@ -190,8 +195,8 @@ static av_always_inline int get_cabac_inline_x86(CABACContext *c,
 #endif
 
     __asm__ volatile(
-        BRANCHLESS_GET_CABAC("%0", "%q0", "(%4)", "%1", "%w1",
-                             "%2", "%q2", "%3", "%b3",
+        BRANCHLESS_GET_CABAC("%0", "%"FF_PTROP"0", "(%4)", "%1", "%w1",
+                             "%2", "%"FF_PTROP"2", "%3", "%b3",
                              "%c6(%5)", "%c7(%5)",
                              AV_STRINGIFY(H264_NORM_SHIFT_OFFSET),
                              AV_STRINGIFY(H264_LPS_RANGE_OFFSET),
@@ -213,7 +218,7 @@ static av_always_inline int get_cabac_inline_x86(CABACContext *c,
 #define get_cabac_bypass_sign get_cabac_bypass_sign_x86
 static av_always_inline int get_cabac_bypass_sign_x86(CABACContext *c, int val)
 {
-    x86_reg tmp;
+    intptr_t tmp;
     __asm__ volatile(
         "movl        %c6(%2), %k1       \n\t"
         "movl        %c3(%2), %%eax     \n\t"
@@ -259,7 +264,7 @@ static av_always_inline int get_cabac_bypass_sign_x86(CABACContext *c, int val)
 #define get_cabac_bypass get_cabac_bypass_x86
 static av_always_inline int get_cabac_bypass_x86(CABACContext *c)
 {
-    x86_reg tmp;
+    intptr_t tmp;
     int res;
     __asm__ volatile(
         "movl        %c6(%2), %k1       \n\t"
diff --git a/libavcodec/x86/h264_cabac.c b/libavcodec/x86/h264_cabac.c
index 2edc6d7e74..cc4cdde34a 100644
--- a/libavcodec/x86/h264_cabac.c
+++ b/libavcodec/x86/h264_cabac.c
@@ -45,12 +45,12 @@
 #define decode_significance decode_significance_x86
 static int decode_significance_x86(CABACContext *c, int max_coeff,
                                    uint8_t *significant_coeff_ctx_base,
-                                   int *index, x86_reg last_off){
+                                   int *index, intptr_t last_off){
     void *end= significant_coeff_ctx_base + max_coeff - 1;
     int minusstart= -(intptr_t)significant_coeff_ctx_base;
     int minusindex= 4-(intptr_t)index;
     int bit;
-    x86_reg coeff_count;
+    int coeff_count;
 
 #ifdef BROKEN_RELOCATIONS
     void *tables;
@@ -65,8 +65,8 @@ static int decode_significance_x86(CABACContext *c, int max_coeff,
     __asm__ volatile(
         "3:                                     \n\t"
 
-        BRANCHLESS_GET_CABAC("%4", "%q4", "(%1)", "%3", "%w3",
-                             "%5", "%q5", "%k0", "%b0",
+        BRANCHLESS_GET_CABAC("%4", "%"FF_PTROP"4", "(%1)", "%3", "%w3",
+                             "%5", "%"FF_PTROP"5", "%k0", "%b0",
                              "%c11(%6)", "%c12(%6)",
                              AV_STRINGIFY(H264_NORM_SHIFT_OFFSET),
                              AV_STRINGIFY(H264_LPS_RANGE_OFFSET),
@@ -77,8 +77,8 @@ static int decode_significance_x86(CABACContext *c, int max_coeff,
         " jz 4f                                 \n\t"
         "add  %10, %1                           \n\t"
 
-        BRANCHLESS_GET_CABAC("%4", "%q4", "(%1)", "%3", "%w3",
-                             "%5", "%q5", "%k0", "%b0",
+        BRANCHLESS_GET_CABAC("%4", "%"FF_PTROP"4", "(%1)", "%3", "%w3",
+                             "%5", "%"FF_PTROP"5", "%k0", "%b0",
                              "%c11(%6)", "%c12(%6)",
                              AV_STRINGIFY(H264_NORM_SHIFT_OFFSET),
                              AV_STRINGIFY(H264_LPS_RANGE_OFFSET),
@@ -124,9 +124,9 @@ static int decode_significance_8x8_x86(CABACContext *c,
                                        int *index, uint8_t *last_coeff_ctx_base, const uint8_t *sig_off){
     int minusindex= 4-(intptr_t)index;
     int bit;
-    x86_reg coeff_count;
-    x86_reg last=0;
-    x86_reg state;
+    intptr_t coeff_count;
+    intptr_t last=0;
+    intptr_t state;
 
 #ifdef BROKEN_RELOCATIONS
     void *tables;
@@ -146,8 +146,8 @@ static int decode_significance_8x8_x86(CABACContext *c,
         "movzb (%0, %6), %6                     \n\t"
         "add %9, %6                             \n\t"
 
-        BRANCHLESS_GET_CABAC("%4", "%q4", "(%6)", "%3", "%w3",
-                             "%5", "%q5", "%k0", "%b0",
+        BRANCHLESS_GET_CABAC("%4", "%"FF_PTROP"4", "(%6)", "%3", "%w3",
+                             "%5", "%"FF_PTROP"5", "%k0", "%b0",
                              "%c12(%7)", "%c13(%7)",
                              AV_STRINGIFY(H264_NORM_SHIFT_OFFSET),
                              AV_STRINGIFY(H264_LPS_RANGE_OFFSET),
@@ -159,14 +159,14 @@ static int decode_significance_8x8_x86(CABACContext *c,
         " jz 4f                                 \n\t"
 
 #ifdef BROKEN_RELOCATIONS
-        "movzb %c14(%15, %q6), %6\n\t"
+        "movzb %c14(%15, %"FF_PTROP"6), %6\n\t"
 #else
         "movzb "MANGLE(ff_h264_cabac_tables)"+%c14(%6), %6\n\t"
 #endif
         "add %11, %6                            \n\t"
 
-        BRANCHLESS_GET_CABAC("%4", "%q4", "(%6)", "%3", "%w3",
-                             "%5", "%q5", "%k0", "%b0",
+        BRANCHLESS_GET_CABAC("%4", "%"FF_PTROP"4", "(%6)", "%3", "%w3",
+                             "%5", "%"FF_PTROP"5", "%k0", "%b0",
                              "%c12(%7)", "%c13(%7)",
                              AV_STRINGIFY(H264_NORM_SHIFT_OFFSET),
                              AV_STRINGIFY(H264_LPS_RANGE_OFFSET),
-- 
2.21.0


From 3f89a909e5e89e724e9fd82df6b8dfa6333dbea3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 18 Sep 2018 19:48:34 +0200
Subject: [PATCH 04/17] x32 support: inline asm
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Substitute intptr_t/diffptr_t for x86_reg. Or sometimes remove
casts to x86_reg. Only a handful of places actually need x86_reg.

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavcodec/x86/cavsdsp.c                   |  6 +-
 libavcodec/x86/hpeldsp_rnd_template.c      | 10 +--
 libavcodec/x86/lossless_videodsp_init.c    |  4 +-
 libavcodec/x86/lossless_videoencdsp_init.c |  4 +-
 libavcodec/x86/lpc.c                       |  8 +--
 libavcodec/x86/me_cmp_init.c               |  6 +-
 libavcodec/x86/mlpdsp_init.c               |  4 +-
 libavcodec/x86/mpegaudiodsp.c              |  2 +-
 libavcodec/x86/mpegvideo.c                 | 12 ++--
 libavcodec/x86/mpegvideoenc_qns_template.c |  4 +-
 libavcodec/x86/mpegvideoenc_template.c     |  2 +-
 libavcodec/x86/mpegvideoencdsp_init.c      | 14 ++--
 libavcodec/x86/rnd_template.c              |  4 +-
 libavcodec/x86/snowdsp.c                   | 32 +++++-----
 libavfilter/x86/vf_noise.c                 |  6 +-
 libpostproc/postprocess_template.c         | 52 +++++++--------
 libswscale/x86/hscale_fast_bilinear_simd.c | 18 +++---
 libswscale/x86/rgb2rgb_template.c          | 36 +++++------
 libswscale/x86/swscale.c                   |  4 +-
 libswscale/x86/swscale_template.c          | 74 +++++++++++-----------
 libswscale/x86/yuv2rgb_template.c          |  2 +-
 21 files changed, 152 insertions(+), 152 deletions(-)

diff --git a/libavcodec/x86/cavsdsp.c b/libavcodec/x86/cavsdsp.c
index becb3a4808..d80fba1627 100644
--- a/libavcodec/x86/cavsdsp.c
+++ b/libavcodec/x86/cavsdsp.c
@@ -164,7 +164,7 @@ static void cavs_idct8_add_sse2(uint8_t *dst, int16_t *block, ptrdiff_t stride)
         VOP(%%mm1, %%mm2, %%mm3, %%mm4, %%mm5, %%mm0, OP, ADD, MUL1, MUL2)\
         \
         : "+a"(src), "+c"(dst)\
-        : "S"((x86_reg)srcStride), "r"((x86_reg)dstStride)\
+        : "S"(srcStride), "r"(dstStride)\
           NAMED_CONSTRAINTS_ADD(ADD,MUL1,MUL2)\
         : "memory"\
      );\
@@ -180,7 +180,7 @@ static void cavs_idct8_add_sse2(uint8_t *dst, int16_t *block, ptrdiff_t stride)
             VOP(%%mm3, %%mm4, %%mm5, %%mm0, %%mm1, %%mm2, OP, ADD, MUL1, MUL2)\
             \
            : "+a"(src), "+c"(dst)\
-           : "S"((x86_reg)srcStride), "r"((x86_reg)dstStride)\
+           : "S"(srcStride), "r"(dstStride)\
              NAMED_CONSTRAINTS_ADD(ADD,MUL1,MUL2)\
            : "memory"\
         );\
@@ -233,7 +233,7 @@ static void OPNAME ## cavs_qpel8_h_ ## MMX(uint8_t *dst, const uint8_t *src, ptr
         "decl %2                    \n\t"\
         " jnz 1b                    \n\t"\
         : "+a"(src), "+c"(dst), "+m"(h)\
-        : "d"((x86_reg)srcStride), "S"((x86_reg)dstStride)\
+        : "d"(srcStride), "S"(dstStride)\
           NAMED_CONSTRAINTS_ADD(ff_pw_4,ff_pw_5)\
         : "memory"\
     );\
diff --git a/libavcodec/x86/hpeldsp_rnd_template.c b/libavcodec/x86/hpeldsp_rnd_template.c
index 2bff2d2766..cb7dacdbce 100644
--- a/libavcodec/x86/hpeldsp_rnd_template.c
+++ b/libavcodec/x86/hpeldsp_rnd_template.c
@@ -56,7 +56,7 @@ av_unused static void DEF(put, pixels8_x2)(uint8_t *block, const uint8_t *pixels
         "subl   $4, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels), "+D"(block)
-        :"r"((x86_reg)line_size)
+        :"r"(line_size)
         :FF_REG_a, "memory");
 }
 
@@ -102,7 +102,7 @@ av_unused static void DEF(put, pixels16_x2)(uint8_t *block, const uint8_t *pixel
         "subl   $4, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels), "+D"(block)
-        :"r"((x86_reg)line_size)
+        :"r"(line_size)
         :FF_REG_a, "memory");
 }
 
@@ -131,7 +131,7 @@ av_unused static void DEF(put, pixels8_y2)(uint8_t *block, const uint8_t *pixels
         "subl   $4, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels), "+D"(block)
-        :"r"((x86_reg)line_size)
+        :"r"(line_size)
         :FF_REG_a, "memory");
 }
 
@@ -158,7 +158,7 @@ av_unused static void DEF(avg, pixels16_x2)(uint8_t *block, const uint8_t *pixel
             "subl   $1, %0              \n\t"
             "jnz    1b                  \n\t"
             :"+g"(h), "+S"(pixels), "+D"(block)
-            :"r"((x86_reg)line_size)
+            :"r"(line_size)
             :"memory");
 }
 
@@ -197,6 +197,6 @@ av_unused static void DEF(avg, pixels8_y2)(uint8_t *block, const uint8_t *pixels
         "subl   $4, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels), "+D"(block)
-        :"r"((x86_reg)line_size)
+        :"r"(line_size)
         :FF_REG_a, "memory");
 }
diff --git a/libavcodec/x86/lossless_videodsp_init.c b/libavcodec/x86/lossless_videodsp_init.c
index 6d71f14e7f..dd49f89da8 100644
--- a/libavcodec/x86/lossless_videodsp_init.c
+++ b/libavcodec/x86/lossless_videodsp_init.c
@@ -52,8 +52,8 @@ static void add_median_pred_cmov(uint8_t *dst, const uint8_t *top,
                                  const uint8_t *diff, ptrdiff_t w,
                                  int *left, int *left_top)
 {
-    x86_reg w2 = -w;
-    x86_reg x;
+    ptrdiff_t w2 = -w;
+    int *x;
     int l  = *left     & 0xff;
     int tl = *left_top & 0xff;
     int t;
diff --git a/libavcodec/x86/lossless_videoencdsp_init.c b/libavcodec/x86/lossless_videoencdsp_init.c
index 40407add52..07d7a12993 100644
--- a/libavcodec/x86/lossless_videoencdsp_init.c
+++ b/libavcodec/x86/lossless_videoencdsp_init.c
@@ -45,7 +45,7 @@ static void sub_median_pred_mmxext(uint8_t *dst, const uint8_t *src1,
                                    const uint8_t *src2, intptr_t w,
                                    int *left, int *left_top)
 {
-    x86_reg i = 0;
+    intptr_t i = 0;
     uint8_t l, lt;
 
     __asm__ volatile (
@@ -70,7 +70,7 @@ static void sub_median_pred_mmxext(uint8_t *dst, const uint8_t *src1,
         "cmp %4, %0                     \n\t"
         " jb 1b                         \n\t"
         : "+r" (i)
-        : "r" (src1), "r" (src2), "r" (dst), "r" ((x86_reg) w));
+        : "r" (src1), "r" (src2), "r" (dst), "r" (w));
 
     l  = *left;
     lt = *left_top;
diff --git a/libavcodec/x86/lpc.c b/libavcodec/x86/lpc.c
index 6c72e21bac..3fc76c0609 100644
--- a/libavcodec/x86/lpc.c
+++ b/libavcodec/x86/lpc.c
@@ -36,8 +36,8 @@ static void lpc_apply_welch_window_sse2(const int32_t *data, int len,
 {
     double c = 2.0 / (len-1.0);
     int n2 = len>>1;
-    x86_reg i = -n2*sizeof(int32_t);
-    x86_reg j =  n2*sizeof(int32_t);
+    intptr_t i = -n2*sizeof(int32_t);
+    intptr_t j =  n2*sizeof(int32_t);
     __asm__ volatile(
         "movsd   %4,     %%xmm7                \n\t"
         "movapd  "MANGLE(pd_1)", %%xmm6        \n\t"
@@ -84,11 +84,11 @@ static void lpc_compute_autocorr_sse2(const double *data, int len, int lag,
 {
     int j;
 
-    if((x86_reg)data & 15)
+    if((intptr_t)data & 15)
         data++;
 
     for(j=0; j<lag; j+=2){
-        x86_reg i = -len*sizeof(double);
+        intptr_t i = -len*sizeof(double);
         if(j == lag-2) {
             __asm__ volatile(
                 "movsd    "MANGLE(pd_1)", %%xmm0    \n\t"
diff --git a/libavcodec/x86/me_cmp_init.c b/libavcodec/x86/me_cmp_init.c
index 6aec93e55f..054ab24847 100644
--- a/libavcodec/x86/me_cmp_init.c
+++ b/libavcodec/x86/me_cmp_init.c
@@ -279,7 +279,7 @@ DECLARE_ASM_CONST(8, uint64_t, round_tab)[3] = {
 static inline void sad8_1_mmx(uint8_t *blk1, uint8_t *blk2,
                               ptrdiff_t stride, int h)
 {
-    x86_reg len = -stride * h;
+    ptrdiff_t len = -stride * h;
     __asm__ volatile (
         ".p2align 4                     \n\t"
         "1:                             \n\t"
@@ -315,7 +315,7 @@ static inline void sad8_1_mmx(uint8_t *blk1, uint8_t *blk2,
 static inline void sad8_2_mmx(uint8_t *blk1a, uint8_t *blk1b, uint8_t *blk2,
                               ptrdiff_t stride, int h)
 {
-    x86_reg len = -stride * h;
+    ptrdiff_t len = -stride * h;
     __asm__ volatile (
         ".p2align 4                     \n\t"
         "1:                             \n\t"
@@ -354,7 +354,7 @@ static inline void sad8_2_mmx(uint8_t *blk1a, uint8_t *blk1b, uint8_t *blk2,
 static inline void sad8_4_mmx(uint8_t *blk1, uint8_t *blk2,
                               ptrdiff_t stride, int h)
 {
-    x86_reg len = -stride * h;
+    ptrdiff_t len = -stride * h;
     __asm__ volatile (
         "movq  (%1, %%"FF_REG_a"), %%mm0\n\t"
         "movq 1(%1, %%"FF_REG_a"), %%mm2\n\t"
diff --git a/libavcodec/x86/mlpdsp_init.c b/libavcodec/x86/mlpdsp_init.c
index cb90ca24f0..49786c1770 100644
--- a/libavcodec/x86/mlpdsp_init.c
+++ b/libavcodec/x86/mlpdsp_init.c
@@ -134,8 +134,8 @@ static void mlp_filter_channel_x86(int32_t *state, const int32_t *coeff,
                                    unsigned int filter_shift, int32_t mask,
                                    int blocksize, int32_t *sample_buffer)
 {
-    const void *firjump = firtable[firorder];
-    const void *iirjump = iirtable[iirorder];
+    x86_reg firjump = (uintptr_t)firtable[firorder];
+    x86_reg iirjump = (uintptr_t)iirtable[iirorder];
 
     blocksize = -blocksize;
 
diff --git a/libavcodec/x86/mpegaudiodsp.c b/libavcodec/x86/mpegaudiodsp.c
index f46a5c4f3d..277ecf1aa7 100644
--- a/libavcodec/x86/mpegaudiodsp.c
+++ b/libavcodec/x86/mpegaudiodsp.c
@@ -67,7 +67,7 @@ DECLARE_ALIGNED(16, static float, mdct_win_sse)[2][4][4*40];
 static void apply_window(const float *buf, const float *win1,
                          const float *win2, float *sum1, float *sum2, int len)
 {
-    x86_reg count = - 4*len;
+    intptr_t count = - 4*len;
     const float *win1a = win1+len;
     const float *win2a = win2+len;
     const float *bufa  = buf+len;
diff --git a/libavcodec/x86/mpegvideo.c b/libavcodec/x86/mpegvideo.c
index 73967cafda..d13fd7d781 100644
--- a/libavcodec/x86/mpegvideo.c
+++ b/libavcodec/x86/mpegvideo.c
@@ -32,7 +32,7 @@
 static void dct_unquantize_h263_intra_mmx(MpegEncContext *s,
                                   int16_t *block, int n, int qscale)
 {
-    x86_reg level, qmul, qadd, nCoeffs;
+    intptr_t level, qmul, qadd, nCoeffs;
 
     qmul = qscale << 1;
 
@@ -107,7 +107,7 @@ __asm__ volatile(
 static void dct_unquantize_h263_inter_mmx(MpegEncContext *s,
                                   int16_t *block, int n, int qscale)
 {
-    x86_reg qmul, qadd, nCoeffs;
+    intptr_t qmul, qadd, nCoeffs;
 
     qmul = qscale << 1;
     qadd = (qscale - 1) | 1;
@@ -168,7 +168,7 @@ __asm__ volatile(
 static void dct_unquantize_mpeg1_intra_mmx(MpegEncContext *s,
                                      int16_t *block, int n, int qscale)
 {
-    x86_reg nCoeffs;
+    intptr_t nCoeffs;
     const uint16_t *quant_matrix;
     int block0;
 
@@ -237,7 +237,7 @@ __asm__ volatile(
 static void dct_unquantize_mpeg1_inter_mmx(MpegEncContext *s,
                                      int16_t *block, int n, int qscale)
 {
-    x86_reg nCoeffs;
+    intptr_t nCoeffs;
     const uint16_t *quant_matrix;
 
     av_assert2(s->block_last_index[n]>=0);
@@ -303,7 +303,7 @@ __asm__ volatile(
 static void dct_unquantize_mpeg2_intra_mmx(MpegEncContext *s,
                                      int16_t *block, int n, int qscale)
 {
-    x86_reg nCoeffs;
+    intptr_t nCoeffs;
     const uint16_t *quant_matrix;
     int block0;
 
@@ -372,7 +372,7 @@ __asm__ volatile(
 static void dct_unquantize_mpeg2_inter_mmx(MpegEncContext *s,
                                      int16_t *block, int n, int qscale)
 {
-    x86_reg nCoeffs;
+    intptr_t nCoeffs;
     const uint16_t *quant_matrix;
 
     av_assert2(s->block_last_index[n]>=0);
diff --git a/libavcodec/x86/mpegvideoenc_qns_template.c b/libavcodec/x86/mpegvideoenc_qns_template.c
index 882d486205..d117adf7ef 100644
--- a/libavcodec/x86/mpegvideoenc_qns_template.c
+++ b/libavcodec/x86/mpegvideoenc_qns_template.c
@@ -34,7 +34,7 @@
 
 static int DEF(try_8x8basis)(int16_t rem[64], int16_t weight[64], int16_t basis[64], int scale)
 {
-    x86_reg i=0;
+    intptr_t i=0;
 
     av_assert2(FFABS(scale) < MAX_ABS);
     scale<<= 16 + SCALE_OFFSET - BASIS_SHIFT + RECON_SHIFT;
@@ -76,7 +76,7 @@ static int DEF(try_8x8basis)(int16_t rem[64], int16_t weight[64], int16_t basis[
 
 static void DEF(add_8x8basis)(int16_t rem[64], int16_t basis[64], int scale)
 {
-    x86_reg i=0;
+    intptr_t i=0;
 
     if(FFABS(scale) < MAX_ABS){
         scale<<= 16 + SCALE_OFFSET - BASIS_SHIFT + RECON_SHIFT;
diff --git a/libavcodec/x86/mpegvideoenc_template.c b/libavcodec/x86/mpegvideoenc_template.c
index 1201be514b..51a4df3901 100644
--- a/libavcodec/x86/mpegvideoenc_template.c
+++ b/libavcodec/x86/mpegvideoenc_template.c
@@ -103,7 +103,7 @@ static int RENAME(dct_quantize)(MpegEncContext *s,
                             int16_t *block, int n,
                             int qscale, int *overflow)
 {
-    x86_reg last_non_zero_p1;
+    intptr_t last_non_zero_p1;
     int level=0, q; //=0 is because gcc says uninitialized ...
     const uint16_t *qmat, *bias;
     LOCAL_ALIGNED_16(int16_t, temp_block, [64]);
diff --git a/libavcodec/x86/mpegvideoencdsp_init.c b/libavcodec/x86/mpegvideoencdsp_init.c
index 532836cec9..edfc90f119 100644
--- a/libavcodec/x86/mpegvideoencdsp_init.c
+++ b/libavcodec/x86/mpegvideoencdsp_init.c
@@ -126,7 +126,7 @@ static void draw_edges_mmx(uint8_t *buf, int wrap, int width, int height,
             "cmp               %3, %0       \n\t"
             "jb                1b           \n\t"
             : "+r" (ptr)
-            : "r" ((x86_reg) wrap), "r" ((x86_reg) width),
+            : "r" ((intptr_t) wrap), "r" ((intptr_t) width),
               "r" (ptr + wrap * height));
     } else if (w == 16) {
         __asm__ volatile (
@@ -147,7 +147,7 @@ static void draw_edges_mmx(uint8_t *buf, int wrap, int width, int height,
             "cmp               %3, %0           \n\t"
             "jb                1b               \n\t"
             : "+r"(ptr)
-            : "r"((x86_reg)wrap), "r"((x86_reg)width), "r"(ptr + wrap * height)
+            : "r"((intptr_t)wrap), "r"((intptr_t)width), "r"(ptr + wrap * height)
             );
     } else {
         av_assert1(w == 4);
@@ -166,7 +166,7 @@ static void draw_edges_mmx(uint8_t *buf, int wrap, int width, int height,
             "cmp               %3, %0       \n\t"
             "jb                1b           \n\t"
             : "+r" (ptr)
-            : "r" ((x86_reg) wrap), "r" ((x86_reg) width),
+            : "r" ((intptr_t) wrap), "r" ((intptr_t) width),
               "r" (ptr + wrap * height));
     }
 
@@ -185,8 +185,8 @@ static void draw_edges_mmx(uint8_t *buf, int wrap, int width, int height,
                 "cmp        %4, %0              \n\t"
                 "jb         1b                  \n\t"
                 : "+r" (ptr)
-                : "r" ((x86_reg) buf - (x86_reg) ptr - w),
-                  "r" ((x86_reg) - wrap), "r" ((x86_reg) - wrap * 3),
+                : "r" ((intptr_t) buf - (intptr_t) ptr - w),
+                  "r" ((intptr_t) - wrap), "r" ((intptr_t) - wrap * 3),
                   "r" (ptr + width + 2 * w));
         }
     }
@@ -205,8 +205,8 @@ static void draw_edges_mmx(uint8_t *buf, int wrap, int width, int height,
                 "cmp        %4, %0              \n\t"
                 "jb         1b                  \n\t"
                 : "+r" (ptr)
-                : "r" ((x86_reg) last_line - (x86_reg) ptr - w),
-                  "r" ((x86_reg) wrap), "r" ((x86_reg) wrap * 3),
+                : "r" ((intptr_t) last_line - (intptr_t) ptr - w),
+                  "r" ((intptr_t) wrap), "r" ((intptr_t) wrap * 3),
                   "r" (ptr + width + 2 * w));
         }
     }
diff --git a/libavcodec/x86/rnd_template.c b/libavcodec/x86/rnd_template.c
index 09946bd23f..cecd1c0df9 100644
--- a/libavcodec/x86/rnd_template.c
+++ b/libavcodec/x86/rnd_template.c
@@ -93,7 +93,7 @@ av_unused STATIC void DEF(put, pixels8_xy2)(uint8_t *block, const uint8_t *pixel
         "subl   $2, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels)
-        :"D"(block), "r"((x86_reg)line_size)
+        :"D"(block), "r"(line_size)
         :FF_REG_a, "memory");
 }
 
@@ -170,6 +170,6 @@ av_unused STATIC void DEF(avg, pixels8_xy2)(uint8_t *block, const uint8_t *pixel
         "subl   $2, %0                  \n\t"
         "jnz    1b                      \n\t"
         :"+g"(h), "+S"(pixels)
-        :"D"(block), "r"((x86_reg)line_size)
+        :"D"(block), "r"(line_size)
         :FF_REG_a, "memory");
 }
diff --git a/libavcodec/x86/snowdsp.c b/libavcodec/x86/snowdsp.c
index 218e6864db..0942970374 100644
--- a/libavcodec/x86/snowdsp.c
+++ b/libavcodec/x86/snowdsp.c
@@ -76,7 +76,7 @@ static void ff_snow_horizontal_compose97i_sse2(IDWTELEM *b, IDWTELEM *temp, int
         IDWTELEM * const dst = b+w2;
 
         i = 0;
-        for(; (((x86_reg)&dst[i]) & 0x1F) && i<w_r; i++){
+        for(; (((ptrdiff_t)&dst[i]) & 0x1F) && i<w_r; i++){
             dst[i] = dst[i] - (b[i] + b[i + 1]);
         }
         for(; i<w_r-15; i+=16){
@@ -149,7 +149,7 @@ static void ff_snow_horizontal_compose97i_sse2(IDWTELEM *b, IDWTELEM *temp, int
         IDWTELEM * const src = b+w2;
 
         i = 0;
-        for(; (((x86_reg)&temp[i]) & 0x1F) && i<w_r; i++){
+        for(; (((ptrdiff_t)&temp[i]) & 0x1F) && i<w_r; i++){
             temp[i] = src[i] - ((-W_AM*(b[i] + b[i+1]))>>W_AS);
         }
         for(; i<w_r-7; i+=8){
@@ -438,7 +438,7 @@ static void ff_snow_horizontal_compose97i_mmx(IDWTELEM *b, IDWTELEM *temp, int w
         "movdqa %%"s3", %%"t3" \n\t"
 
 static void ff_snow_vertical_compose97i_sse2(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2, IDWTELEM *b3, IDWTELEM *b4, IDWTELEM *b5, int width){
-    x86_reg i = width;
+    ptrdiff_t i = width;
 
     while(i & 0x1F)
     {
@@ -536,7 +536,7 @@ static void ff_snow_vertical_compose97i_sse2(IDWTELEM *b0, IDWTELEM *b1, IDWTELE
 
 
 static void ff_snow_vertical_compose97i_mmx(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2, IDWTELEM *b3, IDWTELEM *b4, IDWTELEM *b5, int width){
-    x86_reg i = width;
+    ptrdiff_t i = width;
     while(i & 15)
     {
         i--;
@@ -608,7 +608,7 @@ static void ff_snow_vertical_compose97i_mmx(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM
 #if HAVE_6REGS
 #define snow_inner_add_yblock_sse2_header \
     IDWTELEM * * dst_array = sb->line + src_y;\
-    x86_reg tmp;\
+    ptrdiff_t tmp;\
     __asm__ volatile(\
              "mov  %7, %%"FF_REG_c"          \n\t"\
              "mov  %6, %2                    \n\t"\
@@ -670,7 +670,7 @@ static void ff_snow_vertical_compose97i_mmx(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM
              "jnz 1b                         \n\t"\
              :"+m"(dst8),"+m"(dst_array),"=&r"(tmp)\
              :\
-             "rm"((x86_reg)(src_x<<1)),"m"(obmc),"a"(block),"m"(b_h),"m"(src_stride):\
+             "rm"((ptrdiff_t)(src_x<<1)),"m"(obmc),"a"(block),"m"(b_h),"m"(src_stride):\
              XMM_CLOBBERS("%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6", "%xmm7", )\
              "%"FF_REG_c"","%"FF_REG_S"","%"FF_REG_D"","%"FF_REG_d"");
 
@@ -688,8 +688,8 @@ static void ff_snow_vertical_compose97i_mmx(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM
              "dec %2                         \n\t"\
              snow_inner_add_yblock_sse2_end_common2
 
-static void inner_add_yblock_bw_8_obmc_16_bh_even_sse2(const uint8_t *obmc, const x86_reg obmc_stride, uint8_t * * block, int b_w, x86_reg b_h,
-                      int src_x, int src_y, x86_reg src_stride, slice_buffer * sb, int add, uint8_t * dst8){
+static void inner_add_yblock_bw_8_obmc_16_bh_even_sse2(const uint8_t *obmc, const ptrdiff_t obmc_stride, uint8_t * * block, int b_w, ptrdiff_t b_h,
+                      int src_x, int src_y, ptrdiff_t src_stride, slice_buffer * sb, int add, uint8_t * dst8){
 snow_inner_add_yblock_sse2_header
 snow_inner_add_yblock_sse2_start_8("xmm1", "xmm5", "3", "0")
 snow_inner_add_yblock_sse2_accum_8("2", "8")
@@ -736,8 +736,8 @@ snow_inner_add_yblock_sse2_accum_8("0", "136")
 snow_inner_add_yblock_sse2_end_8
 }
 
-static void inner_add_yblock_bw_16_obmc_32_sse2(const uint8_t *obmc, const x86_reg obmc_stride, uint8_t * * block, int b_w, x86_reg b_h,
-                      int src_x, int src_y, x86_reg src_stride, slice_buffer * sb, int add, uint8_t * dst8){
+static void inner_add_yblock_bw_16_obmc_32_sse2(const uint8_t *obmc, const ptrdiff_t obmc_stride, uint8_t * * block, int b_w, ptrdiff_t b_h,
+                      int src_x, int src_y, ptrdiff_t src_stride, slice_buffer * sb, int add, uint8_t * dst8){
 snow_inner_add_yblock_sse2_header
 snow_inner_add_yblock_sse2_start_16("xmm1", "xmm5", "3", "0")
 snow_inner_add_yblock_sse2_accum_16("2", "16")
@@ -762,7 +762,7 @@ snow_inner_add_yblock_sse2_end_16
 
 #define snow_inner_add_yblock_mmx_header \
     IDWTELEM * * dst_array = sb->line + src_y;\
-    x86_reg tmp;\
+    ptrdiff_t tmp;\
     __asm__ volatile(\
              "mov  %7, %%"FF_REG_c"          \n\t"\
              "mov  %6, %2                    \n\t"\
@@ -819,11 +819,11 @@ snow_inner_add_yblock_sse2_end_16
              "jnz 1b                         \n\t"\
              :"+m"(dst8),"+m"(dst_array),"=&r"(tmp)\
              :\
-             "rm"((x86_reg)(src_x<<1)),"m"(obmc),"a"(block),"m"(b_h),"m"(src_stride):\
+             "rm"((intptr_t)(src_x<<1)),"m"(obmc),"a"(block),"m"(b_h),"m"(src_stride):\
              "%"FF_REG_c"","%"FF_REG_S"","%"FF_REG_D"","%"FF_REG_d"");
 
-static void inner_add_yblock_bw_8_obmc_16_mmx(const uint8_t *obmc, const x86_reg obmc_stride, uint8_t * * block, int b_w, x86_reg b_h,
-                      int src_x, int src_y, x86_reg src_stride, slice_buffer * sb, int add, uint8_t * dst8){
+static void inner_add_yblock_bw_8_obmc_16_mmx(const uint8_t *obmc, const ptrdiff_t obmc_stride, uint8_t * * block, int b_w, ptrdiff_t b_h,
+                      int src_x, int src_y, ptrdiff_t src_stride, slice_buffer * sb, int add, uint8_t * dst8){
 snow_inner_add_yblock_mmx_header
 snow_inner_add_yblock_mmx_start("mm1", "mm5", "3", "0", "0")
 snow_inner_add_yblock_mmx_accum("2", "8", "0")
@@ -833,8 +833,8 @@ snow_inner_add_yblock_mmx_mix("0", "0")
 snow_inner_add_yblock_mmx_end("16")
 }
 
-static void inner_add_yblock_bw_16_obmc_32_mmx(const uint8_t *obmc, const x86_reg obmc_stride, uint8_t * * block, int b_w, x86_reg b_h,
-                      int src_x, int src_y, x86_reg src_stride, slice_buffer * sb, int add, uint8_t * dst8){
+static void inner_add_yblock_bw_16_obmc_32_mmx(const uint8_t *obmc, const ptrdiff_t obmc_stride, uint8_t * * block, int b_w, ptrdiff_t b_h,
+                      int src_x, int src_y, ptrdiff_t src_stride, slice_buffer * sb, int add, uint8_t * dst8){
 snow_inner_add_yblock_mmx_header
 snow_inner_add_yblock_mmx_start("mm1", "mm5", "3", "0", "0")
 snow_inner_add_yblock_mmx_accum("2", "16", "0")
diff --git a/libavfilter/x86/vf_noise.c b/libavfilter/x86/vf_noise.c
index f7a4d00336..62184eee14 100644
--- a/libavfilter/x86/vf_noise.c
+++ b/libavfilter/x86/vf_noise.c
@@ -28,7 +28,7 @@
 static void line_noise_mmx(uint8_t *dst, const uint8_t *src,
                            const int8_t *noise, int len, int shift)
 {
-    x86_reg mmx_len= len & (~7);
+    intptr_t mmx_len= len & (~7);
     noise += shift;
 
     __asm__ volatile(
@@ -57,7 +57,7 @@ static void line_noise_mmx(uint8_t *dst, const uint8_t *src,
 static void line_noise_avg_mmx(uint8_t *dst, const uint8_t *src,
                                       int len, const int8_t * const *shift)
 {
-    x86_reg mmx_len = len & (~7);
+    intptr_t mmx_len = len & (~7);
 
     __asm__ volatile(
             "mov %5, %%"FF_REG_a"           \n\t"
@@ -100,7 +100,7 @@ static void line_noise_avg_mmx(uint8_t *dst, const uint8_t *src,
 static void line_noise_mmxext(uint8_t *dst, const uint8_t *src,
                               const int8_t *noise, int len, int shift)
 {
-    x86_reg mmx_len = len & (~7);
+    intptr_t mmx_len = len & (~7);
     noise += shift;
 
     __asm__ volatile(
diff --git a/libpostproc/postprocess_template.c b/libpostproc/postprocess_template.c
index b0adfd168c..f4a69f056e 100644
--- a/libpostproc/postprocess_template.c
+++ b/libpostproc/postprocess_template.c
@@ -206,7 +206,7 @@ static inline int RENAME(vertClassify)(const uint8_t src[], int stride, PPContex
         "movd %%mm4, %1                         \n\t"
 
         : "=r" (numEq), "=r" (dcOk)
-        : "r" (src), "r" ((x86_reg)stride), "m" (c->pQPb)
+        : "r" (src), "r" ((intptr_t)stride), "m" (c->pQPb)
         : "%"FF_REG_a
         );
 
@@ -351,7 +351,7 @@ static inline void RENAME(doVertLowPass)(uint8_t *src, int stride, PPContext *c)
         "sub %1, %0                             \n\t"
 
         :
-        : "r" (src), "r" ((x86_reg)stride), "m" (c->pQPb)
+        : "r" (src), "r" ((intptr_t)stride), "m" (c->pQPb)
         : "%"FF_REG_a, "%"FF_REG_c
     );
 #else //TEMPLATE_PP_MMXEXT || TEMPLATE_PP_3DNOW
@@ -489,7 +489,7 @@ static inline void RENAME(vertX1Filter)(uint8_t *src, int stride, PPContext *co)
         "movq %%mm0, (%%"FF_REG_c", %1, 2)      \n\t" // line 7
 
         :
-        : "r" (src), "r" ((x86_reg)stride), "m" (co->pQPb)
+        : "r" (src), "r" ((intptr_t)stride), "m" (co->pQPb)
           NAMED_CONSTRAINTS_ADD(b01)
         : "%"FF_REG_a, "%"FF_REG_c
     );
@@ -755,7 +755,7 @@ static inline void RENAME(doVertDefFilter)(uint8_t src[], int stride, PPContext
         "movq %%mm2, (%0, %1, 4)                \n\t"
 
         :
-        : "r" (src), "r" ((x86_reg)stride), "m" (c->pQPb)
+        : "r" (src), "r" ((intptr_t)stride), "m" (c->pQPb)
           NAMED_CONSTRAINTS_ADD(b80,b00,b01)
         : "%"FF_REG_a, "%"FF_REG_c
     );
@@ -1043,7 +1043,7 @@ static inline void RENAME(doVertDefFilter)(uint8_t src[], int stride, PPContext
         "movq %%mm0, (%0, %1)                   \n\t"
 
         : "+r" (src)
-        : "r" ((x86_reg)stride), "m" (c->pQPb), "r"(tmp)
+        : "r" ((intptr_t)stride), "m" (c->pQPb), "r"(tmp)
           NAMED_CONSTRAINTS_ADD(w05,w20)
         : "%"FF_REG_a
     );
@@ -1315,7 +1315,7 @@ DERING_CORE((%%FF_REGd, %1, 2),(%0, %1, 8)       ,%%mm0,%%mm2,%%mm4,%%mm1,%%mm3,
 DERING_CORE((%0, %1, 8)       ,(%%FF_REGd, %1, 4),%%mm2,%%mm4,%%mm0,%%mm3,%%mm5,%%mm1,%%mm6,%%mm7)
 
         "1:                        \n\t"
-        : : "r" (src), "r" ((x86_reg)stride), "m" (c->pQPb), "m"(c->pQPb2), "q"(tmp)
+        : : "r" (src), "r" ((intptr_t)stride), "m" (c->pQPb), "m"(c->pQPb2), "q"((x86_reg)tmp)
           NAMED_CONSTRAINTS_ADD(deringThreshold,b00,b02,b08)
         : "%"FF_REG_a, "%"FF_REG_d
     );
@@ -1471,7 +1471,7 @@ static inline void RENAME(deInterlaceInterpolateLinear)(uint8_t src[], int strid
         PAVGB(%%mm0, %%mm1)
         "movq %%mm1, (%%"FF_REG_c", %1, 2)      \n\t"
 
-        : : "r" (src), "r" ((x86_reg)stride)
+        : : "r" (src), "r" ((intptr_t)stride)
         : "%"FF_REG_a, "%"FF_REG_c
     );
 #else
@@ -1559,7 +1559,7 @@ DEINT_CUBIC((%%FF_REGa, %1), (%0, %1, 4)    , (%%FF_REGd)       , (%%FF_REGd, %1
 DEINT_CUBIC((%0, %1, 4)    , (%%FF_REGd, %1), (%%FF_REGd, %1, 2), (%0, %1, 8)    , (%%FF_REGc))
 DEINT_CUBIC((%%FF_REGd, %1), (%0, %1, 8)    , (%%FF_REGd, %1, 4), (%%FF_REGc)    , (%%FF_REGc, %1, 2))
 
-        : : "r" (src), "r" ((x86_reg)stride)
+        : : "r" (src), "r" ((intptr_t)stride)
         :
 #if TEMPLATE_PP_SSE2
         XMM_CLOBBERS("%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm7",)
@@ -1635,7 +1635,7 @@ DEINT_FF((%0, %1, 4)    , (%%FF_REGd)       , (%%FF_REGd, %1), (%%FF_REGd, %1, 2
 DEINT_FF((%%FF_REGd, %1), (%%FF_REGd, %1, 2), (%0, %1, 8)    , (%%FF_REGd, %1, 4))
 
         "movq %%mm0, (%2)                       \n\t"
-        : : "r" (src), "r" ((x86_reg)stride), "r"(tmp)
+        : : "r" (src), "r" ((intptr_t)stride), "r"(tmp)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 #else //TEMPLATE_PP_MMXEXT || TEMPLATE_PP_3DNOW
@@ -1725,7 +1725,7 @@ DEINT_L5(%%mm1, %%mm0, (%%FF_REGd, %1, 2), (%0, %1, 8)       , (%%FF_REGd, %1, 4
 
         "movq %%mm0, (%2)                       \n\t"
         "movq %%mm1, (%3)                       \n\t"
-        : : "r" (src), "r" ((x86_reg)stride), "r"(tmp), "r"(tmp2)
+        : : "r" (src), "r" ((intptr_t)stride), "r"(tmp), "r"(tmp2)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 #else //(TEMPLATE_PP_MMXEXT || TEMPLATE_PP_3DNOW) && HAVE_6REGS
@@ -1813,7 +1813,7 @@ static inline void RENAME(deInterlaceBlendLinear)(uint8_t src[], int stride, uin
         "movq %%mm2, (%%"FF_REG_d", %1, 2)      \n\t"
         "movq %%mm1, (%2)                       \n\t"
 
-        : : "r" (src), "r" ((x86_reg)stride), "r" (tmp)
+        : : "r" (src), "r" ((intptr_t)stride), "r" (tmp)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 #else //TEMPLATE_PP_MMXEXT || TEMPLATE_PP_3DNOW
@@ -1917,7 +1917,7 @@ static inline void RENAME(deInterlaceMedian)(uint8_t src[], int stride)
         "movq %%mm2, (%%"FF_REG_d", %1, 2)      \n\t"
 
 
-        : : "r" (src), "r" ((x86_reg)stride)
+        : : "r" (src), "r" ((intptr_t)stride)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 
@@ -1959,7 +1959,7 @@ MEDIAN((%%FF_REGa, %1), (%%FF_REGa, %1, 2), (%0, %1, 4))
 MEDIAN((%0, %1, 4)    , (%%FF_REGd)       , (%%FF_REGd, %1))
 MEDIAN((%%FF_REGd, %1), (%%FF_REGd, %1, 2), (%0, %1, 8))
 
-        : : "r" (src), "r" ((x86_reg)stride)
+        : : "r" (src), "r" ((intptr_t)stride)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 #endif //TEMPLATE_PP_MMXEXT
@@ -2066,7 +2066,7 @@ static inline void RENAME(transpose1)(uint8_t *dst1, uint8_t *dst2, const uint8_
         "movd %%mm1, 116(%3)                    \n\t"
 
 
-        :: "r" (src), "r" ((x86_reg)srcStride), "r" (dst1), "r" (dst2)
+        :: "r" (src), "r" ((intptr_t)srcStride), "r" (dst1), "r" (dst2)
         : "%"FF_REG_a
     );
 }
@@ -2146,7 +2146,7 @@ static inline void RENAME(transpose2)(uint8_t *dst, int dstStride, const uint8_t
         "psrlq $32, %%mm1                       \n\t"
         "movd %%mm1, 4(%%"FF_REG_d", %1, 2)     \n\t"
 
-        :: "r" (dst), "r" ((x86_reg)dstStride), "r" (src)
+        :: "r" (dst), "r" ((intptr_t)dstStride), "r" (src)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 }
@@ -2449,7 +2449,7 @@ L2_DIFF_CORE((%0, %%FF_REGc)  , (%1, %%FF_REGc))
 
         "4:                                     \n\t"
 
-        :: "r" (src), "r" (tempBlurred), "r"((x86_reg)stride), "m" (tempBlurredPast)
+        :: "r" (src), "r" (tempBlurred), "r"((intptr_t)stride), "m" (tempBlurredPast)
           NAMED_CONSTRAINTS_ADD(b80)
         : "%"FF_REG_a, "%"FF_REG_d, "%"FF_REG_c, "memory"
     );
@@ -2650,14 +2650,14 @@ static av_always_inline void RENAME(do_a_deblock)(uint8_t *src, int step, int st
         "movq %%mm6, %0                         \n\t"
 
         : "=m" (eq_mask), "=m" (dc_mask)
-        : "r" (src), "r" ((x86_reg)step), "m" (c->pQPb), "m"(c->ppMode.flatnessThreshold)
+        : "r" (src), "r" ((intptr_t)step), "m" (c->pQPb), "m"(c->ppMode.flatnessThreshold)
         : "%"FF_REG_a
     );
 
     both_masks = dc_mask & eq_mask;
 
     if(both_masks){
-        x86_reg offset= -8*step;
+        intptr_t offset= -8*step;
         int64_t *temp_sums= sums;
 
         __asm__ volatile(
@@ -2794,7 +2794,7 @@ static av_always_inline void RENAME(do_a_deblock)(uint8_t *src, int step, int st
             "mov %4, %0                             \n\t" //FIXME
 
             : "+&r"(src)
-            : "r" ((x86_reg)step), "m" (c->pQPb), "r"(sums), "g"(src)
+            : "r" ((intptr_t)step), "m" (c->pQPb), "r"(sums), "g"(src)
               NAMED_CONSTRAINTS_ADD(w04)
         );
 
@@ -2832,7 +2832,7 @@ static av_always_inline void RENAME(do_a_deblock)(uint8_t *src, int step, int st
             " js 1b                                 \n\t"
 
             : "+r"(offset), "+r"(temp_sums)
-            : "r" ((x86_reg)step), "r"(src - offset), "m"(both_masks)
+            : "r" ((intptr_t)step), "r"(src - offset), "m"(both_masks)
         );
     }else
         src+= step; // src points to begin of the 8x8 Block
@@ -3066,7 +3066,7 @@ static av_always_inline void RENAME(do_a_deblock)(uint8_t *src, int step, int st
             "movq %%mm0, (%0, %1)                   \n\t"
 
             : "+r" (temp_src)
-            : "r" ((x86_reg)step), "m" (c->pQPb), "m"(eq_mask), "r"(tmp)
+            : "r" ((intptr_t)step), "m" (c->pQPb), "m"(eq_mask), "r"(tmp)
               NAMED_CONSTRAINTS_ADD(w05,w20)
             : "%"FF_REG_a
         );
@@ -3170,8 +3170,8 @@ SCALED_CPY((%%FF_REGa, %4), (%%FF_REGa, %4, 2), (%%FF_REGd, %5), (%%FF_REGd, %5,
         : "0" (packedOffsetAndScale),
         "r"(src),
         "r"(dst),
-        "r" ((x86_reg)srcStride),
-        "r" ((x86_reg)dstStride)
+        "r" ((intptr_t)srcStride),
+        "r" ((intptr_t)dstStride)
         : "%"FF_REG_d
     );
 #else //TEMPLATE_PP_MMX && HAVE_6REGS
@@ -3203,8 +3203,8 @@ SIMPLE_CPY((%%FF_REGa, %2), (%%FF_REGa, %2, 2), (%%FF_REGd, %3), (%%FF_REGd, %3,
 
         : : "r" (src),
         "r" (dst),
-        "r" ((x86_reg)srcStride),
-        "r" ((x86_reg)dstStride)
+        "r" ((intptr_t)srcStride),
+        "r" ((intptr_t)dstStride)
         : "%"FF_REG_a, "%"FF_REG_d
     );
 #else //TEMPLATE_PP_MMX && HAVE_6REGS
@@ -3230,7 +3230,7 @@ static inline void RENAME(duplicate)(uint8_t src[], int stride)
         "movq %%mm0, (%0, %1, 2)        \n\t"
         "movq %%mm0, (%0, %1, 4)        \n\t"
         : "+r" (src)
-        : "r" ((x86_reg)-stride)
+        : "r" ((intptr_t)-stride)
     );
 #else
     int i;
diff --git a/libswscale/x86/hscale_fast_bilinear_simd.c b/libswscale/x86/hscale_fast_bilinear_simd.c
index 60a2cbfc50..af232ada8b 100644
--- a/libswscale/x86/hscale_fast_bilinear_simd.c
+++ b/libswscale/x86/hscale_fast_bilinear_simd.c
@@ -31,13 +31,13 @@ av_cold int ff_init_hscaler_mmxext(int dstW, int xInc, uint8_t *filterCode,
                                        int numSplits)
 {
     uint8_t *fragmentA;
-    x86_reg imm8OfPShufW1A;
-    x86_reg imm8OfPShufW2A;
-    x86_reg fragmentLengthA;
+    intptr_t imm8OfPShufW1A;
+    intptr_t imm8OfPShufW2A;
+    intptr_t fragmentLengthA;
     uint8_t *fragmentB;
-    x86_reg imm8OfPShufW1B;
-    x86_reg imm8OfPShufW2B;
-    x86_reg fragmentLengthB;
+    intptr_t imm8OfPShufW1B;
+    intptr_t imm8OfPShufW2B;
+    intptr_t fragmentLengthB;
     int fragmentPos;
 
     int xpos, i;
@@ -140,9 +140,9 @@ av_cold int ff_init_hscaler_mmxext(int dstW, int xInc, uint8_t *filterCode,
             int d                  = ((xpos + xInc * 3) >> 16) - xx;
             int inc                = (d + 1 < 4);
             uint8_t *fragment      = inc ? fragmentB : fragmentA;
-            x86_reg imm8OfPShufW1  = inc ? imm8OfPShufW1B : imm8OfPShufW1A;
-            x86_reg imm8OfPShufW2  = inc ? imm8OfPShufW2B : imm8OfPShufW2A;
-            x86_reg fragmentLength = inc ? fragmentLengthB : fragmentLengthA;
+            intptr_t imm8OfPShufW1  = inc ? imm8OfPShufW1B : imm8OfPShufW1A;
+            intptr_t imm8OfPShufW2  = inc ? imm8OfPShufW2B : imm8OfPShufW2A;
+            intptr_t fragmentLength = inc ? fragmentLengthB : fragmentLengthA;
             int maxShift           = 3 - (d + inc);
             int shift              = 0;
 
diff --git a/libswscale/x86/rgb2rgb_template.c b/libswscale/x86/rgb2rgb_template.c
index ae2469e663..0c3192418b 100644
--- a/libswscale/x86/rgb2rgb_template.c
+++ b/libswscale/x86/rgb2rgb_template.c
@@ -1037,7 +1037,7 @@ static inline void RENAME(rgb16to32)(const uint8_t *src, uint8_t *dst, int src_s
 static inline void RENAME(rgb24tobgr24)(const uint8_t *src, uint8_t *dst, int src_size)
 {
     unsigned i;
-    x86_reg mmx_size= 23 - src_size;
+    intptr_t mmx_size= 23 - src_size;
     __asm__ volatile (
         "test             %%"FF_REG_a", %%"FF_REG_a"    \n\t"
         "jns                     2f                     \n\t"
@@ -1107,7 +1107,7 @@ static inline void RENAME(yuvPlanartoyuy2)(const uint8_t *ysrc, const uint8_t *u
                                            int lumStride, int chromStride, int dstStride, int vertLumPerChroma)
 {
     int y;
-    const x86_reg chromWidth= width>>1;
+    const intptr_t chromWidth= width>>1;
     for (y=0; y<height; y++) {
         //FIXME handle 2 lines at once (fewer prefetches, reuse some chroma, but very likely memory-limited anyway)
         __asm__ volatile(
@@ -1172,7 +1172,7 @@ static inline void RENAME(yuvPlanartouyvy)(const uint8_t *ysrc, const uint8_t *u
                                            int lumStride, int chromStride, int dstStride, int vertLumPerChroma)
 {
     int y;
-    const x86_reg chromWidth= width>>1;
+    const intptr_t chromWidth= width>>1;
     for (y=0; y<height; y++) {
         //FIXME handle 2 lines at once (fewer prefetches, reuse some chroma, but very likely memory-limited anyway)
         __asm__ volatile(
@@ -1261,7 +1261,7 @@ static inline void RENAME(yuy2toyv12)(const uint8_t *src, uint8_t *ydst, uint8_t
                                       int lumStride, int chromStride, int srcStride)
 {
     int y;
-    const x86_reg chromWidth= width>>1;
+    const intptr_t chromWidth= width>>1;
     for (y=0; y<height; y+=2) {
         __asm__ volatile(
             "xor              %%"FF_REG_a", %%"FF_REG_a"\n\t"
@@ -1372,7 +1372,7 @@ static inline void RENAME(planar2x)(const uint8_t *src, uint8_t *dst, int srcWid
     dst+= dstStride;
 
     for (y=1; y<srcHeight; y++) {
-        x86_reg mmxSize= srcWidth&~15;
+        intptr_t mmxSize= srcWidth&~15;
 
         if (mmxSize) {
         __asm__ volatile(
@@ -1467,7 +1467,7 @@ static inline void RENAME(uyvytoyv12)(const uint8_t *src, uint8_t *ydst, uint8_t
                                       int lumStride, int chromStride, int srcStride)
 {
     int y;
-    const x86_reg chromWidth= width>>1;
+    const intptr_t chromWidth= width>>1;
     for (y=0; y<height; y+=2) {
         __asm__ volatile(
             "xor          %%"FF_REG_a", %%"FF_REG_a" \n\t"
@@ -1578,7 +1578,7 @@ static inline void RENAME(rgb24toyv12)(const uint8_t *src, uint8_t *ydst, uint8_
 #define BGR2U_IDX "16*4+16*33"
 #define BGR2V_IDX "16*4+16*34"
     int y;
-    const x86_reg chromWidth= width>>1;
+    const intptr_t chromWidth= width>>1;
 
     if (height > 2) {
         ff_rgb24toyv12_c(src, ydst, udst, vdst, width, 2, lumStride, chromStride, srcStride, rgb2yuv);
@@ -1654,7 +1654,7 @@ static inline void RENAME(rgb24toyv12)(const uint8_t *src, uint8_t *ydst, uint8_
                 MOVNTQ"                  %%mm0, (%1, %%"FF_REG_a") \n\t"
                 "add                        $8,      %%"FF_REG_a"  \n\t"
                 " js                        1b                     \n\t"
-                : : "r" (src+width*3), "r" (ydst+width), "g" ((x86_reg)-width), "r"(rgb2yuv)
+                : : "r" (src+width*3), "r" (ydst+width), "g" ((intptr_t)-width), "r"(rgb2yuv)
                   NAMED_CONSTRAINTS_ADD(ff_w1111,ff_bgr2YOffset)
                 : "%"FF_REG_a, "%"FF_REG_d
             );
@@ -1850,7 +1850,7 @@ static void RENAME(interleaveBytes)(const uint8_t *src1, const uint8_t *src2, ui
             "add                    $16, %%"FF_REG_a"            \n\t"
             "cmp                     %3, %%"FF_REG_a"            \n\t"
             " jb                     1b             \n\t"
-            ::"r"(dest), "r"(src1), "r"(src2), "r" ((x86_reg)width-15)
+            ::"r"(dest), "r"(src1), "r"(src2), "r" ((intptr_t)width-15)
             : "memory", XMM_CLOBBERS("xmm0", "xmm1", "xmm2",) "%"FF_REG_a
         );
             } else
@@ -1877,7 +1877,7 @@ static void RENAME(interleaveBytes)(const uint8_t *src1, const uint8_t *src2, ui
             "add                    $16, %%"FF_REG_a"            \n\t"
             "cmp                     %3, %%"FF_REG_a"            \n\t"
             " jb                     1b                          \n\t"
-            ::"r"(dest), "r"(src1), "r"(src2), "r" ((x86_reg)width-15)
+            ::"r"(dest), "r"(src1), "r"(src2), "r" ((intptr_t)width-15)
             : "memory", "%"FF_REG_a
         );
 
@@ -1937,7 +1937,7 @@ static inline void RENAME(vu9_to_vu12)(const uint8_t *src1, const uint8_t *src2,
                                        int srcStride1, int srcStride2,
                                        int dstStride1, int dstStride2)
 {
-    x86_reg x, y;
+    intptr_t x, y;
     int w,h;
     w=width/2; h=height/2;
     __asm__ volatile(
@@ -2029,7 +2029,7 @@ static inline void RENAME(yvu9_to_yuy2)(const uint8_t *src1, const uint8_t *src2
                                         int srcStride1, int srcStride2,
                                         int srcStride3, int dstStride)
 {
-    x86_reg x;
+    intptr_t x;
     int y,w,h;
     w=width/2; h=height;
     for (y=0;y<h;y++) {
@@ -2110,7 +2110,7 @@ static inline void RENAME(yvu9_to_yuy2)(const uint8_t *src1, const uint8_t *src2
 }
 #endif /* !COMPILE_TEMPLATE_AMD3DNOW */
 
-static void RENAME(extract_even)(const uint8_t *src, uint8_t *dst, x86_reg count)
+static void RENAME(extract_even)(const uint8_t *src, uint8_t *dst, intptr_t count)
 {
     dst +=   count;
     src += 2*count;
@@ -2147,7 +2147,7 @@ static void RENAME(extract_even)(const uint8_t *src, uint8_t *dst, x86_reg count
     }
 }
 
-static void RENAME(extract_odd)(const uint8_t *src, uint8_t *dst, x86_reg count)
+static void RENAME(extract_odd)(const uint8_t *src, uint8_t *dst, intptr_t count)
 {
     src ++;
     dst +=   count;
@@ -2186,7 +2186,7 @@ static void RENAME(extract_odd)(const uint8_t *src, uint8_t *dst, x86_reg count)
 }
 
 #if !COMPILE_TEMPLATE_AMD3DNOW
-static void RENAME(extract_even2)(const uint8_t *src, uint8_t *dst0, uint8_t *dst1, x86_reg count)
+static void RENAME(extract_even2)(const uint8_t *src, uint8_t *dst0, uint8_t *dst1, intptr_t count)
 {
     dst0+=   count;
     dst1+=   count;
@@ -2233,7 +2233,7 @@ static void RENAME(extract_even2)(const uint8_t *src, uint8_t *dst0, uint8_t *ds
 }
 #endif /* !COMPILE_TEMPLATE_AMD3DNOW */
 
-static void RENAME(extract_even2avg)(const uint8_t *src0, const uint8_t *src1, uint8_t *dst0, uint8_t *dst1, x86_reg count)
+static void RENAME(extract_even2avg)(const uint8_t *src0, const uint8_t *src1, uint8_t *dst0, uint8_t *dst1, intptr_t count)
 {
     dst0 +=   count;
     dst1 +=   count;
@@ -2287,7 +2287,7 @@ static void RENAME(extract_even2avg)(const uint8_t *src0, const uint8_t *src1, u
 }
 
 #if !COMPILE_TEMPLATE_AMD3DNOW
-static void RENAME(extract_odd2)(const uint8_t *src, uint8_t *dst0, uint8_t *dst1, x86_reg count)
+static void RENAME(extract_odd2)(const uint8_t *src, uint8_t *dst0, uint8_t *dst1, intptr_t count)
 {
     dst0+=   count;
     dst1+=   count;
@@ -2335,7 +2335,7 @@ static void RENAME(extract_odd2)(const uint8_t *src, uint8_t *dst0, uint8_t *dst
 }
 #endif /* !COMPILE_TEMPLATE_AMD3DNOW */
 
-static void RENAME(extract_odd2avg)(const uint8_t *src0, const uint8_t *src1, uint8_t *dst0, uint8_t *dst1, x86_reg count)
+static void RENAME(extract_odd2avg)(const uint8_t *src0, const uint8_t *src1, uint8_t *dst0, uint8_t *dst1, intptr_t count)
 {
     dst0 +=   count;
     dst1 +=   count;
diff --git a/libswscale/x86/swscale.c b/libswscale/x86/swscale.c
index 7dc2d70574..877864fed6 100644
--- a/libswscale/x86/swscale.c
+++ b/libswscale/x86/swscale.c
@@ -256,7 +256,7 @@ static void yuv2yuvX_sse3(const int16_t *filter, int filterSize,
             "por       %%xmm4, %%xmm3  \n\t"
             MAIN_FUNCTION
               :: "g" (filter),
-              "r" (dest-offset), "g" ((x86_reg)(dstW+offset)), "m" (offset),
+              "r" (dest-offset), "g" ((intptr_t)(dstW+offset)), "m" (offset),
               "m"(filterSize), "m"(((uint64_t *) dither)[0])
               : XMM_CLOBBERS("%xmm0" , "%xmm1" , "%xmm2" , "%xmm3" , "%xmm4" , "%xmm5" , "%xmm7" ,)
                 "%"FF_REG_d, "%"FF_REG_S, "%"FF_REG_c
@@ -266,7 +266,7 @@ static void yuv2yuvX_sse3(const int16_t *filter, int filterSize,
             "movq          %5, %%xmm3   \n\t"
             MAIN_FUNCTION
               :: "g" (filter),
-              "r" (dest-offset), "g" ((x86_reg)(dstW+offset)), "m" (offset),
+              "r" (dest-offset), "g" ((intptr_t)(dstW+offset)), "m" (offset),
               "m"(filterSize), "m"(((uint64_t *) dither)[0])
               : XMM_CLOBBERS("%xmm0" , "%xmm1" , "%xmm2" , "%xmm3" , "%xmm4" , "%xmm5" , "%xmm7" ,)
                 "%"FF_REG_d, "%"FF_REG_S, "%"FF_REG_c
diff --git a/libswscale/x86/swscale_template.c b/libswscale/x86/swscale_template.c
index 7c30470679..24226f8efc 100644
--- a/libswscale/x86/swscale_template.c
+++ b/libswscale/x86/swscale_template.c
@@ -115,7 +115,7 @@ static void RENAME(yuv2yuvX)(const int16_t *filter, int filterSize,
         "mov                        (%%"FF_REG_d"), %%"FF_REG_S"  \n\t"\
         "jb                                  1b                   \n\t"\
         :: "g" (filter),
-           "r" (dest-offset), "g" ((x86_reg)(dstW+offset)), "m" (offset)
+           "r" (dest-offset), "g" ((intptr_t)(dstW+offset)), "m" (offset)
         : "%"FF_REG_d, "%"FF_REG_S, "%"FF_REG_c
     );
 }
@@ -344,9 +344,9 @@ static void RENAME(yuv2rgb32_X_ar)(SwsContext *c, const int16_t *lumFilter,
                                    int chrFilterSize, const int16_t **alpSrc,
                                    uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     if (CONFIG_SWSCALE_ALPHA && c->needAlpha) {
         YSCALEYUV2PACKEDX_ACCURATE
@@ -377,9 +377,9 @@ static void RENAME(yuv2rgb32_X)(SwsContext *c, const int16_t *lumFilter,
                                 int chrFilterSize, const int16_t **alpSrc,
                                 uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     if (CONFIG_SWSCALE_ALPHA && c->needAlpha) {
         YSCALEYUV2PACKEDX
@@ -406,9 +406,9 @@ static void RENAME(yuv2bgr32_X)(SwsContext *c, const int16_t *lumFilter,
                                 int chrFilterSize, const int16_t **alpSrc,
                                 uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     if (CONFIG_SWSCALE_ALPHA && c->needAlpha) {
         YSCALEYUV2PACKEDX
@@ -463,9 +463,9 @@ static void RENAME(yuv2rgb565_X_ar)(SwsContext *c, const int16_t *lumFilter,
                                     int chrFilterSize, const int16_t **alpSrc,
                                     uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX_ACCURATE
     YSCALEYUV2RGBX
@@ -487,9 +487,9 @@ static void RENAME(yuv2rgb565_X)(SwsContext *c, const int16_t *lumFilter,
                                  int chrFilterSize, const int16_t **alpSrc,
                                  uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX
     YSCALEYUV2RGBX
@@ -540,9 +540,9 @@ static void RENAME(yuv2rgb555_X_ar)(SwsContext *c, const int16_t *lumFilter,
                                     int chrFilterSize, const int16_t **alpSrc,
                                     uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX_ACCURATE
     YSCALEYUV2RGBX
@@ -564,9 +564,9 @@ static void RENAME(yuv2rgb555_X)(SwsContext *c, const int16_t *lumFilter,
                                  int chrFilterSize, const int16_t **alpSrc,
                                  uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX
     YSCALEYUV2RGBX
@@ -698,9 +698,9 @@ static void RENAME(yuv2bgr24_X_ar)(SwsContext *c, const int16_t *lumFilter,
                                    int chrFilterSize, const int16_t **alpSrc,
                                    uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX_ACCURATE
     YSCALEYUV2RGBX
@@ -723,9 +723,9 @@ static void RENAME(yuv2bgr24_X)(SwsContext *c, const int16_t *lumFilter,
                                 int chrFilterSize, const int16_t **alpSrc,
                                 uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX
     YSCALEYUV2RGBX
@@ -766,9 +766,9 @@ static void RENAME(yuv2yuyv422_X_ar)(SwsContext *c, const int16_t *lumFilter,
                                      int chrFilterSize, const int16_t **alpSrc,
                                      uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX_ACCURATE
     /* mm2=B, %%mm4=G, %%mm5=R, %%mm7=0 */
@@ -787,9 +787,9 @@ static void RENAME(yuv2yuyv422_X)(SwsContext *c, const int16_t *lumFilter,
                                   int chrFilterSize, const int16_t **alpSrc,
                                   uint8_t *dest, int dstW, int dstY)
 {
-    x86_reg dummy=0;
-    x86_reg dstW_reg = dstW;
-    x86_reg uv_off = c->uv_offx2;
+    intptr_t dummy=0;
+    intptr_t dstW_reg = dstW;
+    intptr_t uv_off = c->uv_offx2;
 
     YSCALEYUV2PACKEDX
     /* mm2=B, %%mm4=G, %%mm5=R, %%mm7=0 */
@@ -893,12 +893,12 @@ static void RENAME(yuv2rgb32_2)(SwsContext *c, const int16_t *buf[2],
         const int16_t *abuf0 = abuf[0], *abuf1 = abuf[1];
 #if ARCH_X86_64
         __asm__ volatile(
-            YSCALEYUV2RGB(%%r8, %5)
-            YSCALEYUV2RGB_YA(%%r8, %5, %6, %7)
+            YSCALEYUV2RGB(%%FF_REG8, %5)
+            YSCALEYUV2RGB_YA(%%FF_REG8, %5, %6, %7)
             "psraw                  $3, %%mm1       \n\t" /* abuf0[eax] - abuf1[eax] >>7*/
             "psraw                  $3, %%mm7       \n\t" /* abuf0[eax] - abuf1[eax] >>7*/
             "packuswb            %%mm7, %%mm1       \n\t"
-            WRITEBGR32(%4, DSTW_OFFSET"(%5)", %%r8, %%mm2, %%mm4, %%mm5, %%mm1, %%mm0, %%mm7, %%mm3, %%mm6)
+            WRITEBGR32(%4, DSTW_OFFSET"(%5)", %%FF_REG8, %%mm2, %%mm4, %%mm5, %%mm1, %%mm0, %%mm7, %%mm3, %%mm6)
             :: "c" (buf0), "d" (buf1), "S" (ubuf0), "D" (ubuf1), "r" (dest),
                "a" (&c->redDither),
                "r" (abuf0), "r" (abuf1)
diff --git a/libswscale/x86/yuv2rgb_template.c b/libswscale/x86/yuv2rgb_template.c
index acb78f520e..1588a5d514 100644
--- a/libswscale/x86/yuv2rgb_template.c
+++ b/libswscale/x86/yuv2rgb_template.c
@@ -56,7 +56,7 @@
         const uint8_t *py = src[0] +               y * srcStride[0]; \
         const uint8_t *pu = src[1] +   (y >> vshift) * srcStride[1]; \
         const uint8_t *pv = src[2] +   (y >> vshift) * srcStride[2]; \
-        x86_reg index = -h_size / 2;                                 \
+        intptr_t index = -h_size / 2;                                \
 
 #define YUV2RGB_INITIAL_LOAD          \
     __asm__ volatile (                \
-- 
2.21.0


From b76dbd8ee9c359b95a3699ed9543fbc4098ab0b0 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 16 Apr 2019 17:58:34 +0200
Subject: [PATCH 05/17] cabac
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavcodec/x86/h264_cabac.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/libavcodec/x86/h264_cabac.c b/libavcodec/x86/h264_cabac.c
index cc4cdde34a..99fc85db0e 100644
--- a/libavcodec/x86/h264_cabac.c
+++ b/libavcodec/x86/h264_cabac.c
@@ -50,7 +50,7 @@ static int decode_significance_x86(CABACContext *c, int max_coeff,
     int minusstart= -(intptr_t)significant_coeff_ctx_base;
     int minusindex= 4-(intptr_t)index;
     int bit;
-    int coeff_count;
+    intptr_t coeff_count;
 
 #ifdef BROKEN_RELOCATIONS
     void *tables;
-- 
2.21.0


From 46efcfd972f6b60bfc81b2aee3d13b14feb6ec97 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 8 Sep 2018 10:00:42 +0200
Subject: [PATCH 06/17] configure: x32 support
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 configure | 9 +++------
 1 file changed, 3 insertions(+), 6 deletions(-)

diff --git a/configure b/configure
index a9644e211b..550e9f4550 100755
--- a/configure
+++ b/configure
@@ -5089,13 +5089,10 @@ case "$arch" in
     ;;
     x86)
         check_64bit x86_32 x86_64
-        # Treat x32 as x64 for now. Note it also needs pic if shared
+        enabled x86_64 && objformat=elf64
         test "$subarch" = "x86_32" && test_cpp_condition stddef.h 'defined(__x86_64__)' &&
-            subarch=x86_64 && enable x86_64 && disable x86_32
-        if enabled x86_64; then
-            enabled shared && enable_weak pic
-            objformat=elf64
-        fi
+            subarch=x86_64 && enable x86_64 && disable x86_32 && objformat=elfx32
+        enabled x86_64 && enabled shared && enable_weak pic
     ;;
 esac
 
-- 
2.21.0


From 5848b5f135eaca194cf73fdb1f861603a25ebd79 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Mon, 17 Sep 2018 18:40:12 +0200
Subject: [PATCH 07/17] x32 support: libavutil/x86/x86inc.asm
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavutil/x86/x86inc.asm | 330 +++++++++++++++++++++++++++++++++------
 1 file changed, 284 insertions(+), 46 deletions(-)

diff --git a/libavutil/x86/x86inc.asm b/libavutil/x86/x86inc.asm
index 5044ee86f0..a29c1bc41d 100644
--- a/libavutil/x86/x86inc.asm
+++ b/libavutil/x86/x86inc.asm
@@ -68,10 +68,14 @@
 %endif
 
 %define FORMAT_ELF 0
+%define ABI_X32 0
 %ifidn __OUTPUT_FORMAT__,elf
     %define FORMAT_ELF 1
 %elifidn __OUTPUT_FORMAT__,elf32
     %define FORMAT_ELF 1
+%elifidn __OUTPUT_FORMAT__,elfx32
+    %define FORMAT_ELF 1
+    %define ABI_X32 1
 %elifidn __OUTPUT_FORMAT__,elf64
     %define FORMAT_ELF 1
 %endif
@@ -141,9 +145,44 @@
 ; src) into registers, uses one additional register (tmp) plus 7 vector
 ; registers (m0-m6) and allocates 0x40 bytes of stack space.
 
-; TODO Some functions can use some args directly from the stack. If they're the
-; last args then you can just not declare them, but if they're in the middle
-; we need more flexible macro.
+; The names in the list to can optionally be preceded by type-specifiers, which
+; are strings of up to 3 characters: type [extend] [delay].
+; type declares the size of the the argument:
+; 'b' byte size (int8_t)
+; 'w' word size (int16_t)
+; 'd' dword size (int32_t)
+; 'q' qword size (int64_t)
+; 'p' pointer size (T*, size_t, ptrdiff_t, intptr_t, uintptr_t)
+; extend specifies how the register is to be loaded from the passed argument
+; by PROLOGUE or LOAD_ARG:
+; if omitted:
+;     The argument is loaded using mov of appropriate size if passed on the stack.
+;     No action is taken if the argument is already present in a register.
+;     That means that while smaller types are passed as if being promoted to int
+;     first, that while the lower 32 bits of a 64 register/stack argument are
+;     determined by the value of the argument, the same is not true for the upper
+;     32 bits unless the argument had 64 bit size originally, except that with the
+;     x32 ABI, pointers that were passed by register, will have the upper half
+;     of the register cleared, so that 64-bit adressing mode can be used.
+; '+' the argument is zero extended to register size if smaller
+; '-' the argument is sign extended to register size if smaller
+; delay
+; '*' the register will not be loaded by PROLOGUE
+;
+; cheat sheet: typical specifiers for certain types:
+; "b+" uint8_t
+; "b-" int8_t
+; "w+" uint16_t
+; "w-" int16_t
+; "d+" uint32_t
+; "d-" int32_t
+; "q+" uint64_t
+; "q-" int64_t
+; "p+" size_t, uintptr_t
+; "p-" ptrdiff_t, intptr_t
+; +/- should be omitted if the the 64-bit register is never used or if the argument
+;     is extended later
+; pointers always use just "p"
 
 ; RET:
 ; Pops anything that was pushed by PROLOGUE, and returns.
@@ -153,27 +192,48 @@
 
 ; registers:
 ; rN and rNq are the native-size register holding function argument N
-; rNd, rNw, rNb are dword, word, and byte size
+; rNp, rNd, rNw, rNb are pointer, dword, word, and byte size
 ; rNh is the high 8 bits of the word size
 ; rNm is the original location of arg N (a register or on the stack), dword
-; rNmp is native size
+; rNmp, rNmq are pointer and native size
+
+%macro DECLARE_MEM_REG 1-2
+    %if %0 == 2
+        %assign r%1_mem_reg_off %2
+    %else
+        %assign r%1_mem_reg_off r %+ declare_mem_reg_last %+ _mem_reg_off + gprsize
+    %endif
+    %define r%1m [rstk + stack_offset + r %+ %1 %+ _mem_reg_off]
+    %define r%1md dword r %+ %1 %+ m
+    %define r%1mp pword r %+ %1 %+ m
+    %if ARCH_X86_64
+        %define r%1mq qword r %+ %1 %+ m
+    %else
+        %define r%1mq dword r %+ %1 %+ m
+    %endif
+    %assign declare_mem_reg_last %1
+%endmacro
 
 %macro DECLARE_REG 2-3
     %define r%1q %2
+    %define r%1p %2p
     %define r%1d %2d
     %define r%1w %2w
     %define r%1b %2b
     %define r%1h %2h
     %define %2q %2
+    %if ABI_X32
+        %define %2p %2d
+    %else
+        %define %2p %2
+    %endif
     %if %0 == 2
         %define r%1m  %2d
-        %define r%1mp %2
-    %elif ARCH_X86_64 ; memory
-        %define r%1m [rstk + stack_offset + %3]
-        %define r%1mp qword r %+ %1 %+ m
+        %define r%1md %2d
+        %define r%1mp %2p
+        %define r%1mq %2
     %else
-        %define r%1m [rstk + stack_offset + %3]
-        %define r%1mp dword r %+ %1 %+ m
+        DECLARE_MEM_REG %1, %3
     %endif
     %define r%1  %2
 %endmacro
@@ -189,6 +249,13 @@
     %define e%1h %3
     %define r%1b %2
     %define e%1b %2
+    %if ABI_X32
+        %define r%1p e%1
+        %define e%1p e%1
+    %else
+        %define r%1p r%1
+        %define e%1p r%1
+    %endif
     %if ARCH_X86_64 == 0
         %define r%1 e%1
     %endif
@@ -201,6 +268,7 @@ DECLARE_REG_SIZE dx, dl, dh
 DECLARE_REG_SIZE si, sil, null
 DECLARE_REG_SIZE di, dil, null
 DECLARE_REG_SIZE bp, bpl, null
+DECLARE_REG_SIZE sp, spl, null
 
 ; t# defines for when per-arch register allocation is more complex than just function arguments
 
@@ -216,6 +284,7 @@ DECLARE_REG_SIZE bp, bpl, null
 %macro DECLARE_REG_TMP_SIZE 0-*
     %rep %0
         %define t%1q t%1 %+ q
+        %define t%1p t%1 %+ p
         %define t%1d t%1 %+ d
         %define t%1w t%1 %+ w
         %define t%1h t%1 %+ h
@@ -232,6 +301,18 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
     %define gprsize 4
 %endif
 
+%if ARCH_X86_64 && ABI_X32 == 0
+    %define ptrsize 8
+    %define pword qword
+    %define resp resq
+    %define dp dq
+%else
+    %define ptrsize 4
+    %define pword dword
+    %define resp resd
+    %define dp dd
+%endif
+
 %macro PUSH 1
     push %1
     %ifidn rstk, rsp
@@ -267,7 +348,25 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
 %macro LOAD_IF_USED 1-*
     %rep %0
         %if %1 < num_args
-            mov r%1, r %+ %1 %+ mp
+            %if argload_delayed_%1 == 0
+                argload_%1
+            %endif
+        %endif
+        %rotate 1
+    %endrep
+%endmacro
+
+%macro LOAD_ARG 1-*
+    %rep %0
+        %ifnum %1
+            %assign %%i %1
+        %else
+            %assign %%i %1 %+ _reg_num
+        %endif
+        %if %%i < regs_used
+            argload_ %+ %%i
+        %else
+            %error "argument not in regs used <regs_used>"
         %endif
         %rotate 1
     %endrep
@@ -277,6 +376,8 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
     sub %1, %2
     %ifidn %1, rstk
         %assign stack_offset stack_offset+(%2)
+    %elifidn %1, rstkp
+        %assign stack_offset stack_offset+(%2)
     %endif
 %endmacro
 
@@ -284,6 +385,8 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
     add %1, %2
     %ifidn %1, rstk
         %assign stack_offset stack_offset-(%2)
+    %elifidn %1, rstkp
+        %assign stack_offset stack_offset-(%2)
     %endif
 %endmacro
 
@@ -305,17 +408,37 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
     %endif
 %endmacro
 
+%macro RESTORE_DEFAULT_ARGS 1
+    %assign %%i 0
+    %rep %1
+        CAT_XDEFINE argload_insn, %%i, movifnidn
+        CAT_XDEFINE argload_dst, %%i, r %+ %%i %+ q
+        CAT_XDEFINE argload_src, %%i, r %+ %%i %+ mq
+        CAT_XDEFINE argload_, %%i, movifnidn r %+ %%i %+ q, r %+ %%i %+ mq
+        CAT_XDEFINE argload_delayed_, %%i, 0
+        CAT_XDEFINE argsuffix_, %%i, q
+        %assign %%i %%i + 1
+    %endrep
+%endmacro
+
+; arg$ is the argument with the size as declared
 %macro DEFINE_ARGS 0-*
     %ifdef n_arg_names
         %assign %%i 0
         %rep n_arg_names
             CAT_UNDEF arg_name %+ %%i, q
+            CAT_UNDEF arg_name %+ %%i, p
             CAT_UNDEF arg_name %+ %%i, d
             CAT_UNDEF arg_name %+ %%i, w
             CAT_UNDEF arg_name %+ %%i, h
             CAT_UNDEF arg_name %+ %%i, b
             CAT_UNDEF arg_name %+ %%i, m
+            CAT_UNDEF arg_name %+ %%i, md
             CAT_UNDEF arg_name %+ %%i, mp
+            CAT_UNDEF arg_name %+ %%i, mq
+            CAT_UNDEF arg_name %+ %%i, $
+            CAT_UNDEF arg_name %+ %%i, _reg_num
+            CAT_UNDEF arg_name %+ %%i, _src_num
             CAT_UNDEF arg_name, %%i
             %assign %%i %%i+1
         %endrep
@@ -324,22 +447,135 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
     %xdefine %%stack_offset stack_offset
     %undef stack_offset ; so that the current value of stack_offset doesn't get baked in by xdefine
     %assign %%i 0
+    %assign %%argsize gprsize
+    %define %%loadinsn movifnidn
+    %define %%loaddst q
+    %define %%loadsrc mq
+    %define %%suffix q
+    %assign %%delayload 0
     %rep %0
-        %xdefine %1q r %+ %%i %+ q
-        %xdefine %1d r %+ %%i %+ d
-        %xdefine %1w r %+ %%i %+ w
-        %xdefine %1h r %+ %%i %+ h
-        %xdefine %1b r %+ %%i %+ b
-        %xdefine %1m r %+ %%i %+ m
-        %xdefine %1mp r %+ %%i %+ mp
-        CAT_XDEFINE arg_name, %%i, %1
-        %assign %%i %%i+1
+        CAT_XDEFINE argload_insn, %%i, %%loadinsn
+        CAT_XDEFINE argload_dst, %%i, %%loaddst
+        CAT_XDEFINE argload_src, %%i, %%loadsrc
+        CAT_XDEFINE argload_, %%i, %%loadinsn r %+ %%i %+ %%loaddst , r %+ %%i %+ %%loadsrc
+        CAT_XDEFINE argload_delayed_, %%i, %%delayload
+        CAT_XDEFINE argsuffix_, %%i, %%suffix
+        %ifstr %1
+            %strlen %%count %1
+            ASSERT %%count <= 3
+            %substr %%size %1 1
+            %assign %%argsize 4
+            %if (%%size == 'b' )
+                %define %%suffix b
+            %elif (%%size == 'w' )
+                %define %%suffix w
+            %elif (%%size == 'd' )
+                %define %%suffix d
+            %elif (%%size == 'p' )
+                %define %%suffix p
+                %assign %%argsize ptrsize
+            %elif (%%size == 'q' )
+                %define %%suffix q
+                %assign %%argsize gprsize
+            %else
+                %error "unknown argument type!"
+            %endif
+            %assign %%delayload 0
+            %substr %%sign %1 2
+            %define %%loadinsn movifnidn
+            %define %%loaddst q
+            %define %%loadsrc mq
+            %if %%argsize != gprsize
+                %define %%loaddst d
+                %define %%loadsrc md
+            %endif
+            %if %%sign == '+'
+                %if %%argsize != gprsize
+                    %define %%loadinsn mov
+                %endif
+            %elif %%sign == '-'
+                %if %%argsize != gprsize
+                    %define %%loadinsn movsxd
+                    %define %%loaddst q
+                %endif
+            %elif %%sign == '*'
+                %assign %%delayload 1
+            %elif %%sign == ''
+            %else
+                %error "unknown sign extension!"
+            %endif
+            %if %%count == 3
+                %substr %%delay %1 3
+                ASSERT %%delay == '*'
+                %assign %%delayload 1
+            %endif
+        %else
+            %if %%i > declare_mem_reg_last
+                DECLARE_MEM_REG %%i
+            %endif
+            CAT_XDEFINE arg_name, %%i, %1
+            %xdefine %1q  r %+ %%i %+ q
+            %xdefine %1p  r %+ %%i %+ p
+            %xdefine %1d  r %+ %%i %+ d
+            %xdefine %1w  r %+ %%i %+ w
+            %xdefine %1h  r %+ %%i %+ h
+            %xdefine %1b  r %+ %%i %+ b
+            %xdefine %1m  r %+ %%i %+ m
+            %xdefine %1md r %+ %%i %+ md
+            %xdefine %1mp r %+ %%i %+ mp
+            %xdefine %1mq r %+ %%i %+ mq
+            %xdefine %1$  r %+ %%i %+ %%suffix
+            %xdefine %1_reg_num %%i
+            %xdefine %1_src_num %%i
+
+            %assign %%i %%i+1
+            %assign %%argsize 4
+            %define %%loadinsn movifnidn
+            %define %%loaddst q
+            %define %%loadsrc mq
+        %endif
         %rotate 1
     %endrep
     %xdefine stack_offset %%stack_offset
     %assign n_arg_names %0
 %endmacro
 
+; Assigns a named argument to a particular register, identified
+; by number or argument name
+%macro ASSIGN_ARG 2 ; arg_name, #reg_num|old_arg_name
+    %ifnum %2
+        %ifidn r%2q,%1 %+ q
+            %xdefine __err__ %1
+            %error "self assignment __err__"
+        %endif
+        %assign %%i %2
+    %else
+        %ifidn %2q,%1 %+ q
+            %xdefine __err__ %1
+            %error "self assignment: __err__"
+        %endif
+        %assign %%i %2 %+ _reg_num
+    %endif
+    %assign %%j %1 %+ _reg_num
+    %assign %%k %1 %+ _src_num
+    %xdefine %%suffix argsuffix_ %+ %%j
+    %xdefine %1q r %+ %%i %+ q
+    %xdefine %1p r %+ %%i %+ p
+    %xdefine %1d r %+ %%i %+ d
+    %xdefine %1w r %+ %%i %+ w
+    %xdefine %1h r %+ %%i %+ h
+    %xdefine %1b r %+ %%i %+ b
+    %xdefine %1$ r %+ %%i %+ %%suffix
+    %xdefine %1_reg_num %%i
+    %xdefine %%loadinsn argload_insn %+ %%j
+    %xdefine %%loaddst argload_dst %+ %%j
+    %xdefine %%loadsrc argload_src %+ %%k
+    CAT_XDEFINE argsuffix_, %%i, argsuffix_ %+ %%j
+    CAT_XDEFINE argload_insn, %%i, %%loadinsn
+    CAT_XDEFINE argload_dst, %%i, %%loaddst
+    CAT_XDEFINE argload_, %%i, %%loadinsn r %+ %%i %+ %%loaddst , r %+ %%k %+ %%loadsrc
+%endmacro
+
 %define required_stack_alignment ((mmsize + 15) & ~15)
 %define vzeroupper_required (mmsize > 16 && (ARCH_X86_64 == 0 || xmm_regs_used > 16 || notcpuflag(avx512)))
 %define high_mm_regs (16*cpuflag(avx512))
@@ -364,25 +600,26 @@ DECLARE_REG_TMP_SIZE 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
             %if required_stack_alignment <= STACK_ALIGNMENT
                 ; maintain the current stack alignment
                 %assign stack_size_padded stack_size + %%pad + ((-%%pad-stack_offset-gprsize) & (STACK_ALIGNMENT-1))
-                SUB rsp, stack_size_padded
+                SUB rspp, stack_size_padded
             %else
                 %assign %%reg_num (regs_used - 1)
                 %xdefine rstk r %+ %%reg_num
+                %xdefine rstkp r %+ %%reg_num %+ p
                 ; align stack, and save original stack location directly above
                 ; it, i.e. in [rsp+stack_size_padded], so we can restore the
-                ; stack in a single instruction (i.e. mov rsp, rstk or mov
-                ; rsp, [rsp+stack_size_padded])
+                ; stack in a single instruction (i.e. mov rspp, rstkp or mov
+                ; rspp, [rsp+stack_size_padded])
                 %if %1 < 0 ; need to store rsp on stack
                     %xdefine rstkm [rsp + stack_size + %%pad]
                     %assign %%pad %%pad + gprsize
                 %else ; can keep rsp in rstk during whole function
-                    %xdefine rstkm rstk
+                    %xdefine rstkm rstkp
                 %endif
                 %assign stack_size_padded stack_size + ((%%pad + required_stack_alignment-1) & ~(required_stack_alignment-1))
-                mov rstk, rsp
-                and rsp, ~(required_stack_alignment-1)
-                sub rsp, stack_size_padded
-                movifnidn rstkm, rstk
+                mov rstkp, rspp
+                and rspp, ~(required_stack_alignment-1)
+                sub rspp, stack_size_padded
+                movifnidn rstkm, rstkp
             %endif
             WIN64_PUSH_XMM
         %endif
@@ -450,8 +687,9 @@ DECLARE_REG 14, R13, 120
     %if mmsize != 8 && stack_size == 0
         WIN64_SPILL_XMM %3
     %endif
-    LOAD_IF_USED 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
+    RESTORE_DEFAULT_ARGS regs_used
     DEFINE_ARGS_INTERNAL %0, %4, %5
+    LOAD_IF_USED 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
 %endmacro
 
 %macro WIN64_PUSH_XMM 0
@@ -556,8 +794,9 @@ DECLARE_REG 14, R13, 72
     ASSERT regs_used <= 15
     PUSH_IF_USED 9, 10, 11, 12, 13, 14
     ALLOC_STACK %4
-    LOAD_IF_USED 6, 7, 8, 9, 10, 11, 12, 13, 14
+    RESTORE_DEFAULT_ARGS regs_used
     DEFINE_ARGS_INTERNAL %0, %4, %5
+    LOAD_IF_USED 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
 %endmacro
 
 %define has_epilogue regs_used > 9 || stack_size > 0 || vzeroupper_required
@@ -565,9 +804,9 @@ DECLARE_REG 14, R13, 72
 %macro RET 0
     %if stack_size_padded > 0
         %if required_stack_alignment > STACK_ALIGNMENT
-            mov rsp, rstkm
+            mov rspp, rstkm
         %else
-            add rsp, stack_size_padded
+            add rspp, stack_size_padded
         %endif
     %endif
     POP_IF_USED 14, 13, 12, 11, 10, 9
@@ -586,17 +825,14 @@ DECLARE_REG 3, ebx, 16
 DECLARE_REG 4, esi, 20
 DECLARE_REG 5, edi, 24
 DECLARE_REG 6, ebp, 28
-%define rsp esp
-
-%macro DECLARE_ARG 1-*
-    %rep %0
-        %define r%1m [rstk + stack_offset + 4*%1 + 4]
-        %define r%1mp dword r%1m
-        %rotate 1
-    %endrep
-%endmacro
-
-DECLARE_ARG 7, 8, 9, 10, 11, 12, 13, 14
+DECLARE_MEM_REG 7
+DECLARE_MEM_REG 8
+DECLARE_MEM_REG 9
+DECLARE_MEM_REG 10
+DECLARE_MEM_REG 11
+DECLARE_MEM_REG 12
+DECLARE_MEM_REG 13
+DECLARE_MEM_REG 14
 
 %macro PROLOGUE 2-5+ ; #args, #regs, #xmm_regs, [stack_size,] arg_names...
     %assign num_args %1
@@ -612,8 +848,9 @@ DECLARE_ARG 7, 8, 9, 10, 11, 12, 13, 14
     ASSERT regs_used <= 7
     PUSH_IF_USED 3, 4, 5, 6
     ALLOC_STACK %4
-    LOAD_IF_USED 0, 1, 2, 3, 4, 5, 6
+    RESTORE_DEFAULT_ARGS regs_used
     DEFINE_ARGS_INTERNAL %0, %4, %5
+    LOAD_IF_USED 0, 1, 2, 3, 4, 5, 6
 %endmacro
 
 %define has_epilogue regs_used > 3 || stack_size > 0 || vzeroupper_required
@@ -734,6 +971,7 @@ BRANCH_INSTR jz, je, jnz, jne, jl, jle, jnl, jnle, jg, jge, jng, jnge, ja, jae,
     %2:
     RESET_MM_PERMUTATION        ; needed for x86-64, also makes disassembly somewhat nicer
     %xdefine rstk rsp           ; copy of the original stack pointer, used when greater alignment than the known stack alignment is required
+    %xdefine rstkp rspp
     %assign stack_offset 0      ; stack pointer offset relative to the return address
     %assign stack_size 0        ; amount of stack space that can be freely used inside a function
     %assign stack_size_padded 0 ; total amount of allocated stack space, including space for callee-saved xmm registers on WIN64 and alignment padding
@@ -883,7 +1121,7 @@ BRANCH_INSTR jz, je, jnz, jne, jl, jle, jnl, jnle, jg, jge, jng, jnge, ja, jae,
 ; zm# is the corresponding zmm register if mmsize >= 64, otherwise the same as m#
 ; (All 4 remain in sync through SWAP.)
 
-%macro CAT_XDEFINE 3
+%macro CAT_XDEFINE 3+
     %xdefine %1%2 %3
 %endmacro
 
-- 
2.21.0


From 91766cba63a1f541e3bea4d32bc9be88bedd4874 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 18 Sep 2018 07:46:15 +0200
Subject: [PATCH 08/17] x32: libavutil/x86/imgutils.asm
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavutil/x86/imgutils.asm | 9 ++++++++-
 1 file changed, 8 insertions(+), 1 deletion(-)

diff --git a/libavutil/x86/imgutils.asm b/libavutil/x86/imgutils.asm
index 3cca56cdca..ae0ca3b8b2 100644
--- a/libavutil/x86/imgutils.asm
+++ b/libavutil/x86/imgutils.asm
@@ -22,8 +22,15 @@
 
 SECTION .text
 
+;-----------------------------------------------------------------------------
+; void ff_image_copy_plane_uc_from_sse4(uint8_t *dst, ptrdiff_t dst_linesize,
+;                                  const uint8_t *src, ptrdiff_t src_linesize,
+;                                  ptrdiff_t bytewidth, int height);
+;-----------------------------------------------------------------------------
 INIT_XMM sse4
-cglobal image_copy_plane_uc_from, 6, 7, 4, dst, dst_linesize, src, src_linesize, bw, height, rowpos
+cglobal image_copy_plane_uc_from, 6, 7, 4, "p", dst, "p-", dst_linesize, \
+                                           "p", src, "p-", src_linesize, \
+                                           "p+", bw, "d", height, rowpos
     add dstq, bwq
     add srcq, bwq
     neg bwq
-- 
2.21.0


From af9a6d236bfa061053dac265441503a5ce91291a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Tue, 18 Sep 2018 08:37:51 +0200
Subject: [PATCH 09/17] x32: libavutil/x86/pixelutils.asm

---
 libavutil/x86/pixelutils.asm | 87 ++++++++++++++++++++----------------
 1 file changed, 48 insertions(+), 39 deletions(-)

diff --git a/libavutil/x86/pixelutils.asm b/libavutil/x86/pixelutils.asm
index 36c57c5f7f..6b9213e8af 100644
--- a/libavutil/x86/pixelutils.asm
+++ b/libavutil/x86/pixelutils.asm
@@ -30,7 +30,8 @@ SECTION .text
 ;                               const uint8_t *src2, ptrdiff_t stride2);
 ;-------------------------------------------------------------------------------
 INIT_MMX mmx
-cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
+cglobal pixelutils_sad_8x8, 4,4,0, "p", src1, "p-", stride1, \
+                                   "p", src2, "p-", stride2
     pxor        m7, m7
     pxor        m6, m6
 %rep 4
@@ -52,8 +53,8 @@ cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
     paddw       m2, m3
     paddw       m0, m2
     paddw       m6, m0
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
+    lea         src1p, [src1q + 2*stride1q]
+    lea         src2p, [src2q + 2*stride2q]
 %endrep
     psrlq       m0, m6, 32
     paddw       m6, m0
@@ -68,7 +69,8 @@ cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
 ;                                  const uint8_t *src2, ptrdiff_t stride2);
 ;-------------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
+cglobal pixelutils_sad_8x8, 4,4,0, "p", src1, "p-", stride1, \
+                                   "p", src2, "p-", stride2
     pxor        m2, m2
 %rep 4
     mova        m0, [src1q]
@@ -77,8 +79,8 @@ cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
     psadbw      m1, [src2q + stride2q]
     paddw       m2, m0
     paddw       m2, m1
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
+    lea         src1p, [src1q + 2*stride1q]
+    lea         src2p, [src2q + 2*stride2q]
 %endrep
     movd        eax, m2
     RET
@@ -88,7 +90,8 @@ cglobal pixelutils_sad_8x8, 4,4,0, src1, stride1, src2, stride2
 ;                                    const uint8_t *src2, ptrdiff_t stride2);
 ;-------------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal pixelutils_sad_16x16, 4,4,0, src1, stride1, src2, stride2
+cglobal pixelutils_sad_16x16, 4,4,0, "p", src1, "p-", stride1, \
+                                     "p", src2, "p-", stride2
     pxor        m2, m2
 %rep 16
     mova        m0, [src1q]
@@ -97,8 +100,8 @@ cglobal pixelutils_sad_16x16, 4,4,0, src1, stride1, src2, stride2
     psadbw      m1, [src2q + 8]
     paddw       m2, m0
     paddw       m2, m1
-    add         src1q, stride1q
-    add         src2q, stride2q
+    add         src1p, stride1p
+    add         src2p, stride2p
 %endrep
     movd        eax, m2
     RET
@@ -108,7 +111,8 @@ cglobal pixelutils_sad_16x16, 4,4,0, src1, stride1, src2, stride2
 ;                                  const uint8_t *src2, ptrdiff_t stride2);
 ;-------------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pixelutils_sad_16x16, 4,4,5, src1, stride1, src2, stride2
+cglobal pixelutils_sad_16x16, 4,4,5, "p", src1, "p-", stride1, \
+                                     "p", src2, "p-", stride2
     movu        m4, [src1q]
     movu        m2, [src2q]
     movu        m1, [src1q + stride1q]
@@ -117,8 +121,8 @@ cglobal pixelutils_sad_16x16, 4,4,5, src1, stride1, src2, stride2
     psadbw      m1, m3
     paddw       m4, m1
 %rep 7
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
+    lea         src1p, [src1q + 2*stride1q]
+    lea         src2p, [src2q + 2*stride2q]
     movu        m0, [src1q]
     movu        m2, [src2q]
     movu        m1, [src1q + stride1q]
@@ -139,15 +143,16 @@ cglobal pixelutils_sad_16x16, 4,4,5, src1, stride1, src2, stride2
 ;-------------------------------------------------------------------------------
 %macro SAD_XMM_16x16 1
 INIT_XMM sse2
-cglobal pixelutils_sad_%1_16x16, 4,4,3, src1, stride1, src2, stride2
+cglobal pixelutils_sad_%1_16x16, 4,4,3, "p", src1, "p-", stride1, \
+                                        "p", src2, "p-", stride2
     mov%1       m2, [src2q]
     psadbw      m2, [src1q]
     mov%1       m1, [src2q + stride2q]
     psadbw      m1, [src1q + stride1q]
     paddw       m2, m1
 %rep 7
-    lea         src1q, [src1q + 2*stride1q]
-    lea         src2q, [src2q + 2*stride2q]
+    lea         src1p, [src1q + 2*stride1q]
+    lea         src2p, [src2q + 2*stride2q]
     mov%1       m0, [src2q]
     psadbw      m0, [src1q]
     mov%1       m1, [src2q + stride2q]
@@ -174,8 +179,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  m4
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     movu    m1,  [r2]
     movu    m2,  [r2 + 16]
@@ -185,8 +190,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  m4
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     movu    m1,  [r2]
     movu    m2,  [r2 + 16]
@@ -196,8 +201,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  m4
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     movu    m1,  [r2]
     movu    m2,  [r2 + 16]
@@ -207,8 +212,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  m4
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 %endmacro
 
 %macro PROCESS_SAD_32x4 1
@@ -218,8 +223,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  [r0 + 16]
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     mov%1   m1,  [r2]
     mov%1   m2,  [r2 + 16]
@@ -227,8 +232,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  [r0 + 16]
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     mov%1   m1,  [r2]
     mov%1   m2,  [r2 + 16]
@@ -236,8 +241,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  [r0 + 16]
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 
     mov%1   m1,  [r2]
     mov%1   m2,  [r2 + 16]
@@ -245,8 +250,8 @@ SAD_XMM_16x16 u
     psadbw  m2,  [r0 + 16]
     paddd   m1,  m2
     paddd   m0,  m1
-    lea     r2,  [r2 + r3]
-    lea     r0,  [r0 + r1]
+    lea     r2p, [r2 + r3]
+    lea     r0p, [r0 + r1]
 %endmacro
 
 ;-----------------------------------------------------------------------------
@@ -254,7 +259,8 @@ SAD_XMM_16x16 u
 ;                                  const uint8_t *src2, ptrdiff_t stride2);
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pixelutils_sad_32x32, 4,5,5, src1, stride1, src2, stride2
+cglobal pixelutils_sad_32x32, 4,5,5, "p", src1, "p-", stride1, \
+                                     "p", src2, "p-", stride2
     pxor  m0,  m0
     mov   r4d, 4
 .loop:
@@ -274,7 +280,8 @@ cglobal pixelutils_sad_32x32, 4,5,5, src1, stride1, src2, stride2
 ;-------------------------------------------------------------------------------
 %macro SAD_XMM_32x32 1
 INIT_XMM sse2
-cglobal pixelutils_sad_%1_32x32, 4,5,3, src1, stride1, src2, stride2
+cglobal pixelutils_sad_%1_32x32, 4,5,3, "p", src1, "p-", stride1, \
+                                        "p", src2, "p-", stride2
     pxor  m0,  m0
     mov   r4d, 4
 .loop:
@@ -298,7 +305,8 @@ SAD_XMM_32x32 u
 ;                                  const uint8_t *src2, ptrdiff_t stride2);
 ;-------------------------------------------------------------------------------
 INIT_YMM avx2
-cglobal pixelutils_sad_32x32, 4,7,5, src1, stride1, src2, stride2
+cglobal pixelutils_sad_32x32, 4,7,5, "p", src1, "p-", stride1, \
+                                     "p", src2, "p-", stride2
     pxor            m0, m0
     mov             r4d, 32/4
     lea             r5, [stride1q * 3]
@@ -325,8 +333,8 @@ cglobal pixelutils_sad_32x32, 4,7,5, src1, stride1, src2, stride2
     paddd          m0, m1
     paddd          m0, m3
 
-    lea            src2q,     [src2q + 4 * stride2q]
-    lea            src1q,     [src1q + 4 * stride1q]
+    lea            src2p,     [src2q + 4 * stride2q]
+    lea            src1p,     [src1q + 4 * stride1q]
 
     dec            r4d
     jnz           .loop
@@ -344,7 +352,8 @@ cglobal pixelutils_sad_32x32, 4,7,5, src1, stride1, src2, stride2
 ;-------------------------------------------------------------------------------
 %macro SAD_AVX2_32x32 1
 INIT_YMM avx2
-cglobal pixelutils_sad_%1_32x32, 4,7,3, src1, stride1, src2, stride2
+cglobal pixelutils_sad_%1_32x32, 4,7,3, "p", src1, "p-", stride1, \
+                                        "p", src2, "p-", stride2
     pxor           m0, m0
     mov            r4d, 32/4
     lea            r5, [stride1q * 3]
@@ -367,8 +376,8 @@ cglobal pixelutils_sad_%1_32x32, 4,7,3, src1, stride1, src2, stride2
     paddd          m0, m1
     paddd          m0, m2
 
-    lea            src2q,     [src2q + 4 * stride2q]
-    lea            src1q,     [src1q + 4 * stride1q]
+    lea            src2p,     [src2q + 4 * stride2q]
+    lea            src1p,     [src1q + 4 * stride1q]
 
     dec            r4d
     jnz           .loop
-- 
2.21.0


From 06fd75f13d1813285b0b7594a02a2bdef165e14a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:04:57 +0200
Subject: [PATCH 10/17] x32: tests/checkasm.asm
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 tests/checkasm/x86/checkasm.asm | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/tests/checkasm/x86/checkasm.asm b/tests/checkasm/x86/checkasm.asm
index 683aae80e3..ef7265024a 100644
--- a/tests/checkasm/x86/checkasm.asm
+++ b/tests/checkasm/x86/checkasm.asm
@@ -104,13 +104,13 @@ cglobal checked_call%1, 2,15,16,max_args*8+8
 
     ; All arguments have been pushed on the stack instead of registers in order to
     ; test for incorrect assumptions that 32-bit ints are zero-extended to 64-bit.
-    mov  r0, r6mp
-    mov  r1, r7mp
-    mov  r2, r8mp
-    mov  r3, r9mp
+    mov  r0, r6mq
+    mov  r1, r7mq
+    mov  r2, r8mq
+    mov  r3, r9mq
 %if UNIX64
-    mov  r4, r10mp
-    mov  r5, r11mp
+    mov  r4, r10mq
+    mov  r5, r11mq
     %assign i 6
     %rep max_args-6
         mov  r9, [rsp+stack_offset+(i+1)*8]
-- 
2.21.0


From d0c39d1c1d10775d0dea34e1f237c274ef09c7c3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:05:28 +0200
Subject: [PATCH 11/17] x32: libavutil
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavutil/x86/asm.h         |  3 +++
 libavutil/x86/cpuid.asm     |  4 ++--
 libavutil/x86/fixed_dsp.asm |  2 +-
 libavutil/x86/float_dsp.asm | 34 +++++++++++++++++-----------------
 libavutil/x86/lls.asm       |  6 +++---
 5 files changed, 26 insertions(+), 23 deletions(-)

diff --git a/libavutil/x86/asm.h b/libavutil/x86/asm.h
index 7e587709e2..5c68c78ebd 100644
--- a/libavutil/x86/asm.h
+++ b/libavutil/x86/asm.h
@@ -38,6 +38,7 @@ typedef struct ymm_reg { uint64_t a, b, c, d; } ymm_reg;
 #    define FF_PTROP "k"
 #    define FF_MAXOP "q"
 #    define FF_REG_a "eax"
+#    define FF_REG_A "rax"
 #    define FF_REG_b "ebx"
 #    define FF_REG_c "ecx"
 #    define FF_REG_d "edx"
@@ -62,6 +63,7 @@ typedef int64_t x86_reg;
 #    define FF_PTROP "q"
 #    define FF_MAXOP "q"
 #    define FF_REG_a "rax"
+#    define FF_REG_A "rax"
 #    define FF_REG_b "rbx"
 #    define FF_REG_c "rcx"
 #    define FF_REG_d "rdx"
@@ -87,6 +89,7 @@ typedef int64_t x86_reg;
 #    define FF_PTROP "k"
 #    define FF_MAXOP "k"
 #    define FF_REG_a "eax"
+#    define FF_REG_A "eax"
 #    define FF_REG_b "ebx"
 #    define FF_REG_c "ecx"
 #    define FF_REG_d "edx"
diff --git a/libavutil/x86/cpuid.asm b/libavutil/x86/cpuid.asm
index c3f7866ec7..194c5a5589 100644
--- a/libavutil/x86/cpuid.asm
+++ b/libavutil/x86/cpuid.asm
@@ -28,7 +28,7 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 ; void ff_cpu_cpuid(int index, int *eax, int *ebx, int *ecx, int *edx)
 ;-----------------------------------------------------------------------------
-cglobal cpu_cpuid, 5,7
+cglobal cpu_cpuid, 5,7, "d", index, "p", eax_, "p", ebx_, "p", ecx_, "p", edx_
     push rbx
     push  r4
     push  r3
@@ -51,7 +51,7 @@ cglobal cpu_cpuid, 5,7
 ;-----------------------------------------------------------------------------
 ; void ff_cpu_xgetbv(int op, int *eax, int *edx)
 ;-----------------------------------------------------------------------------
-cglobal cpu_xgetbv, 3,7
+cglobal cpu_xgetbv, 3,7, "d", op, "p", eax_, "p", edx_
     push  r2
     push  r1
     mov  ecx, r0d
diff --git a/libavutil/x86/fixed_dsp.asm b/libavutil/x86/fixed_dsp.asm
index 979dd5c334..df0c785afb 100644
--- a/libavutil/x86/fixed_dsp.asm
+++ b/libavutil/x86/fixed_dsp.asm
@@ -28,7 +28,7 @@ SECTION .text
 ; void ff_butterflies_fixed(float *src0, float *src1, int len);
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal butterflies_fixed, 3,3,3, src0, src1, len
+cglobal butterflies_fixed, 3,3,3, "p", src0, "p", src1, "d", len
     shl       lend, 2
     add      src0q, lenq
     add      src1q, lenq
diff --git a/libavutil/x86/float_dsp.asm b/libavutil/x86/float_dsp.asm
index 517fd63638..873881c606 100644
--- a/libavutil/x86/float_dsp.asm
+++ b/libavutil/x86/float_dsp.asm
@@ -31,7 +31,7 @@ SECTION .text
 ; void vector_fmul(float *dst, const float *src0, const float *src1, int len)
 ;-----------------------------------------------------------------------------
 %macro VECTOR_FMUL 0
-cglobal vector_fmul, 4,4,2, dst, src0, src1, len
+cglobal vector_fmul, 4,4,2, "p", dst, "p", src0, "p", src1, "d", len
     lea       lenq, [lend*4 - 64]
 ALIGN 16
 .loop:
@@ -62,7 +62,7 @@ VECTOR_FMUL
 ; void vector_dmul(double *dst, const double *src0, const double *src1, int len)
 ;-----------------------------------------------------------------------------
 %macro VECTOR_DMUL 0
-cglobal vector_dmul, 4,4,4, dst, src0, src1, len
+cglobal vector_dmul, 4,4,4, "p", dst, "p", src0, "p", src1, "d", len
     lea       lend, [lenq*8 - mmsize*4]
 ALIGN 16
 .loop:
@@ -97,9 +97,9 @@ VECTOR_DMUL
 
 %macro VECTOR_FMAC_SCALAR 0
 %if UNIX64
-cglobal vector_fmac_scalar, 3,3,5, dst, src, len
+cglobal vector_fmac_scalar, 3,3,5, "p", dst, "p", src, "d", len
 %else
-cglobal vector_fmac_scalar, 4,4,5, dst, src, mul, len
+cglobal vector_fmac_scalar, 4,4,5, "p", dst, "p", src, "d*", mul, "p", len
 %endif
 %if ARCH_X86_32
     VBROADCASTSS m0, mulm
@@ -161,9 +161,9 @@ VECTOR_FMAC_SCALAR
 
 %macro VECTOR_FMUL_SCALAR 0
 %if UNIX64
-cglobal vector_fmul_scalar, 3,3,2, dst, src, len
+cglobal vector_fmul_scalar, 3,3,2, "p", dst, "p", src, "d", len
 %else
-cglobal vector_fmul_scalar, 4,4,3, dst, src, mul, len
+cglobal vector_fmul_scalar, 4,4,3, "p", dst, "p", src, "d*", mul, "d", len
 %endif
 %if ARCH_X86_32
     movss    m0, mulm
@@ -191,14 +191,14 @@ VECTOR_FMUL_SCALAR
 
 %macro VECTOR_DMAC_SCALAR 0
 %if ARCH_X86_32
-cglobal vector_dmac_scalar, 2,4,5, dst, src, mul, len, lenaddr
+cglobal vector_dmac_scalar, 2,4,5, "p", dst, "p", src, "q*", mul, "d*", len, lenaddr
     mov          lenq, lenaddrm
     VBROADCASTSD m0, mulm
 %else
 %if UNIX64
-cglobal vector_dmac_scalar, 3,3,5, dst, src, len
+cglobal vector_dmac_scalar, 3,3,5, "p", dst, "p", src, "d", len
 %else
-cglobal vector_dmac_scalar, 4,4,5, dst, src, mul, len
+cglobal vector_dmac_scalar, 4,4,5, "p", dst, "p", src, "q*", mul, "d", len
     SWAP 0, 2
 %endif
     movlhps     xm0, xm0
@@ -254,12 +254,12 @@ VECTOR_DMAC_SCALAR
 
 %macro VECTOR_DMUL_SCALAR 0
 %if ARCH_X86_32
-cglobal vector_dmul_scalar, 3,4,3, dst, src, mul, len, lenaddr
+cglobal vector_dmul_scalar, 2,4,3, "p", dst, "p", src, "q*", mul, "d*", len, lenaddr
     mov          lenq, lenaddrm
 %elif UNIX64
-cglobal vector_dmul_scalar, 3,3,3, dst, src, len
+cglobal vector_dmul_scalar, 3,3,3, "p", dst, "p", src, "d", len
 %else
-cglobal vector_dmul_scalar, 4,4,3, dst, src, mul, len
+cglobal vector_dmul_scalar, 4,4,3, "p", dst, "p", src, "q*", mul, "d", len
 %endif
 %if ARCH_X86_32
     VBROADCASTSD   m0, mulm
@@ -295,7 +295,7 @@ VECTOR_DMUL_SCALAR
 ;                    const float *src1, const float *win, int len);
 ;-----------------------------------------------------------------------------
 %macro VECTOR_FMUL_WINDOW 0
-cglobal vector_fmul_window, 5, 6, 6, dst, src0, src1, win, len, len1
+cglobal vector_fmul_window, 5, 6, 6, "p", dst, "p", src0, "p", src1, "p", win, "d", len, len1
     shl     lend, 2
     lea    len1q, [lenq - mmsize]
     add    src0q, lenq
@@ -353,7 +353,7 @@ VECTOR_FMUL_WINDOW
 ;                 const float *src2, int len)
 ;-----------------------------------------------------------------------------
 %macro VECTOR_FMUL_ADD 0
-cglobal vector_fmul_add, 5,5,4, dst, src0, src1, src2, len
+cglobal vector_fmul_add, 5,5,4, "p", dst, "p", src0, "p", src1, "p", src2, "d", len
     lea       lenq, [lend*4 - 2*mmsize]
 ALIGN 16
 .loop:
@@ -394,7 +394,7 @@ VECTOR_FMUL_ADD
 ;                          int len)
 ;-----------------------------------------------------------------------------
 %macro VECTOR_FMUL_REVERSE 0
-cglobal vector_fmul_reverse, 4,4,2, dst, src0, src1, len
+cglobal vector_fmul_reverse, 4,4,2, "p", dst, "p", src0, "p", src1, "d", len
 %if cpuflag(avx2)
     movaps  m2, [pd_reverse]
 %endif
@@ -440,7 +440,7 @@ VECTOR_FMUL_REVERSE
 
 ; float scalarproduct_float_sse(const float *v1, const float *v2, int len)
 INIT_XMM sse
-cglobal scalarproduct_float, 3,3,2, v1, v2, offset
+cglobal scalarproduct_float, 3,3,2, "p", v1, "p", v2, "d", offset
     shl   offsetd, 2
     add       v1q, offsetq
     add       v2q, offsetq
@@ -467,7 +467,7 @@ cglobal scalarproduct_float, 3,3,2, v1, v2, offset
 ; void ff_butterflies_float(float *src0, float *src1, int len);
 ;-----------------------------------------------------------------------------
 INIT_XMM sse
-cglobal butterflies_float, 3,3,3, src0, src1, len
+cglobal butterflies_float, 3,3,3, "p", src0, "p", src1, "d", len
     shl       lend, 2
     add      src0q, lenq
     add      src1q, lenq
diff --git a/libavutil/x86/lls.asm b/libavutil/x86/lls.asm
index 317fba6fca..52a5c4e25e 100644
--- a/libavutil/x86/lls.asm
+++ b/libavutil/x86/lls.asm
@@ -47,7 +47,7 @@ endstruc
 
 INIT_XMM sse2
 %define movdqa movaps
-cglobal update_lls, 2,5,8, ctx, var, i, j, covar2
+cglobal update_lls, 2,5,8, "p", ctx, "p", var, i, j, covar2
     %define covarq ctxq
     mov     id, [ctxq + LLSModel.indep_count]
     lea   varq, [varq + iq*8]
@@ -126,7 +126,7 @@ cglobal update_lls, 2,5,8, ctx, var, i, j, covar2
     REP_RET
 
 %macro UPDATE_LLS 0
-cglobal update_lls, 3,6,8, ctx, var, count, i, j, count2
+cglobal update_lls, 2,6,8, "p", ctx, "p", var, count, i, j, count2
     %define covarq ctxq
     mov  countd, [ctxq + LLSModel.indep_count]
     lea count2d, [countq-2]
@@ -253,7 +253,7 @@ UPDATE_LLS
 %endif
 
 INIT_XMM sse2
-cglobal evaluate_lls, 3,4,2, ctx, var, order, i
+cglobal evaluate_lls, 3,4,2, "p", ctx, "p", var, "d", order, i
     ; This function is often called on the same buffer as update_lls, but with
     ; an offset. They can't both be aligned.
     ; Load halves rather than movu to avoid store-forwarding stalls, since the
-- 
2.21.0


From fc68860986949acf8019c9cdcfc18f0526bd1d5b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:05:40 +0200
Subject: [PATCH 12/17] x32: libswscale
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libswscale/x86/input.asm     | 47 +++++++++---------------------------
 libswscale/x86/output.asm    |  9 +++----
 libswscale/x86/rgb_2_rgb.asm | 14 +++--------
 libswscale/x86/scale.asm     | 14 +++++------
 4 files changed, 26 insertions(+), 58 deletions(-)

diff --git a/libswscale/x86/input.asm b/libswscale/x86/input.asm
index af9afcaa53..35cafdde74 100644
--- a/libswscale/x86/input.asm
+++ b/libswscale/x86/input.asm
@@ -108,7 +108,7 @@ SECTION .text
 ; %1 = nr. of XMM registers
 ; %2 = rgb or bgr
 %macro RGB24_TO_Y_FN 2-3
-cglobal %2 %+ 24ToY, 6, 6, %1, dst, src, u1, u2, w, table
+cglobal %2 %+ 24ToY, 6, 6, %1, "p", dst, "p", src, "p*", u1, "p*", u2, "d-", w, "p", table
 %if mmsize == 8
     mova           m5, [%2_Ycoeff_12x4]
     mova           m6, [%2_Ycoeff_3x56]
@@ -137,9 +137,6 @@ cglobal %2 %+ 24ToY, 6, 6, %1, dst, src, u1, u2, w, table
 %define shuf_rgb2 [shuf_rgb_3x56]
 %endif ; x86-32/64
 %endif ; cpuflag(ssse3)
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
     add            wq, wq
     add          dstq, wq
     neg            wq
@@ -197,7 +194,7 @@ cglobal %2 %+ 24ToY, 6, 6, %1, dst, src, u1, u2, w, table
 ; %1 = nr. of XMM registers
 ; %2 = rgb or bgr
 %macro RGB24_TO_UV_FN 2-3
-cglobal %2 %+ 24ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
+cglobal %2 %+ 24ToUV, 7, 7, %1, "p", dstU, "p", dstV, "p*", u1, "p", src, "p*", u2, "d-", w, "p", table
 %if ARCH_X86_64
     mova           m8, [%2_Ucoeff_12x4]
     mova           m9, [%2_Ucoeff_3x56]
@@ -227,11 +224,6 @@ cglobal %2 %+ 24ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
 %define shuf_rgb2 [shuf_rgb_3x56]
 %endif ; x86-32/64
 %endif ; cpuflag(ssse3)
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
     add            wq, wq
     add         dstUq, wq
     add         dstVq, wq
@@ -337,16 +329,13 @@ RGB24_FUNCS 11, 13
 ; %1 = nr. of XMM registers
 ; %2-5 = rgba, bgra, argb or abgr (in individual characters)
 %macro RGB32_TO_Y_FN 5-6
-cglobal %2%3%4%5 %+ ToY, 6, 6, %1, dst, src, u1, u2, w, table
+cglobal %2%3%4%5 %+ ToY, 6, 6, %1, "p", dst, "p", src, "p*", u1, "p*", u2, "d-", w, "p", table
     mova           m5, [rgba_Ycoeff_%2%4]
     mova           m6, [rgba_Ycoeff_%3%5]
 %if %0 == 6
     jmp mangle(private_prefix %+ _ %+ %6 %+ ToY %+ SUFFIX).body
 %else ; %0 == 6
 .body:
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
     add            wq, wq
     sub            wq, mmsize - 1
     lea          srcq, [srcq+wq*2]
@@ -398,7 +387,7 @@ cglobal %2%3%4%5 %+ ToY, 6, 6, %1, dst, src, u1, u2, w, table
 ; %1 = nr. of XMM registers
 ; %2-5 = rgba, bgra, argb or abgr (in individual characters)
 %macro RGB32_TO_UV_FN 5-6
-cglobal %2%3%4%5 %+ ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
+cglobal %2%3%4%5 %+ ToUV, 7, 7, %1, "p", dstU, "p", dstV, "p*", u1, "p", src, "p*", u2, "d-", w, "p", table
 %if ARCH_X86_64
     mova           m8, [rgba_Ucoeff_%2%4]
     mova           m9, [rgba_Ucoeff_%3%5]
@@ -418,11 +407,6 @@ cglobal %2%3%4%5 %+ ToUV, 7, 7, %1, dstU, dstV, u1, src, u2, w, table
     jmp mangle(private_prefix %+ _ %+ %6 %+ ToUV %+ SUFFIX).body
 %else ; ARCH_X86_64 && %0 == 6
 .body:
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
     add            wq, wq
     sub            wq, mmsize - 1
     add         dstUq, wq
@@ -559,10 +543,7 @@ RGB32_FUNCS 8, 12
 ;      will be the same (i.e. YUYV+AVX), and thus we don't need to
 ;      split the loop in an aligned and unaligned case
 %macro YUYV_TO_Y_FN 2-3
-cglobal %2ToY, 5, 5, %1, dst, unused0, unused1, src, w
-%if ARCH_X86_64
-    movsxd         wq, wd
-%endif
+cglobal %2ToY, 5, 5, %1, "p", dst, "p*", unused0, "p*", unused1, "p", src, "d-", w
     add          dstq, wq
 %if mmsize == 16
     test         srcq, 15
@@ -629,12 +610,9 @@ cglobal %2ToY, 5, 5, %1, dst, unused0, unused1, src, w
 ;      will be the same (i.e. UYVY+AVX), and thus we don't need to
 ;      split the loop in an aligned and unaligned case
 %macro YUYV_TO_UV_FN 2-3
-cglobal %2ToUV, 4, 5, %1, dstU, dstV, unused, src, w
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
+cglobal %2ToUV, 4, 5, %1, "p", dstU, "p", dstV, "p*", unused, "p", src, "p*", unused2, "d-*", w
+    ASSIGN_ARG w, 4
+    LOAD_ARG w
     add         dstUq, wq
     add         dstVq, wq
 %if mmsize == 16 && %0 == 2
@@ -684,12 +662,9 @@ cglobal %2ToUV, 4, 5, %1, dstU, dstV, unused, src, w
 ; %1 = nr. of XMM registers
 ; %2 = nv12 or nv21
 %macro NVXX_TO_UV_FN 2
-cglobal %2ToUV, 4, 5, %1, dstU, dstV, unused, src, w
-%if ARCH_X86_64
-    movsxd         wq, dword r5m
-%else ; x86-32
-    mov            wq, r5m
-%endif
+cglobal %2ToUV, 4, 5, %1, "p", dstU, "p", dstV, "p*", unused, "p", src, "p*", unused2, "d-*", w
+    ASSIGN_ARG w, 4
+    LOAD_ARG w
     add         dstUq, wq
     add         dstVq, wq
 %if mmsize == 16
diff --git a/libswscale/x86/output.asm b/libswscale/x86/output.asm
index db3e9934f8..07b978258c 100644
--- a/libswscale/x86/output.asm
+++ b/libswscale/x86/output.asm
@@ -84,14 +84,14 @@ SECTION .text
     movsx     cntr_reg,  fltsizem
 .filterloop_%2_ %+ %%i:
     ; input pixels
-    mov             r6, [srcq+gprsize*cntr_reg-2*gprsize]
+    mov             r6p, [srcq+ptrsize*cntr_reg-2*ptrsize]
 %if %1 == 16
     mova            m3, [r6+r5*4]
     mova            m5, [r6+r5*4+mmsize]
 %else ; %1 == 8/9/10
     mova            m3, [r6+r5*2]
 %endif ; %1 == 8/9/10/16
-    mov             r6, [srcq+gprsize*cntr_reg-gprsize]
+    mov             r6p, [srcq+ptrsize*cntr_reg-ptrsize]
 %if %1 == 16
     mova            m4, [r6+r5*4]
     mova            m6, [r6+r5*4+mmsize]
@@ -177,7 +177,7 @@ SECTION .text
 %define movsx movsxd
 %endif
 
-cglobal yuv2planeX_%1, %3, 8, %2, filter, fltsize, src, dst, w, dither, offset
+cglobal yuv2planeX_%1, %3, 8, %2, "p", filter, "d-*", fltsize, "p", src, "p", dst, "d", w, "p", dither, "d", offset
 %if %1 == 8 || %1 == 9 || %1 == 10
     pxor            m6,  m6
 %endif ; %1 == 8/9/10
@@ -330,8 +330,7 @@ yuv2planeX_fn 10,  7, 5
 %endmacro
 
 %macro yuv2plane1_fn 3
-cglobal yuv2plane1_%1, %3, %3, %2, src, dst, w, dither, offset
-    movsxdifnidn    wq, wd
+cglobal yuv2plane1_%1, %3, %3, %2, "p", src, "p", dst, "d-", w, "p", dither, "d", offset
     add             wq, mmsize - 1
     and             wq, ~(mmsize - 1)
 %if %1 == 8
diff --git a/libswscale/x86/rgb_2_rgb.asm b/libswscale/x86/rgb_2_rgb.asm
index 29b856e281..ba93dcbc3c 100644
--- a/libswscale/x86/rgb_2_rgb.asm
+++ b/libswscale/x86/rgb_2_rgb.asm
@@ -48,12 +48,11 @@ SECTION .text
 ; shuffle_bytes_2103_mmext (const uint8_t *src, uint8_t *dst, int src_size)
 ;------------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal shuffle_bytes_2103, 3, 5, 8, src, dst, w, tmp, x
+cglobal shuffle_bytes_2103, 3, 5, 8, "p", src, "p", dst, "d-", w, tmp, x
     mova   m6, [pb_mask_shuffle2103_mmx]
     mova   m7, m6
     psllq  m7, 8
 
-    movsxdifnidn wq, wd
     mov xq, wq
 
     add        srcq, wq
@@ -111,9 +110,8 @@ jge .end
 ;------------------------------------------------------------------------------
 ; %1-4 index shuffle
 %macro SHUFFLE_BYTES 4
-cglobal shuffle_bytes_%1%2%3%4, 3, 5, 2, src, dst, w, tmp, x
+cglobal shuffle_bytes_%1%2%3%4, 3, 5, 2, "p", src, "p", dst, "d-", w, tmp, x
     VBROADCASTI128    m0, [pb_shuffle%1%2%3%4]
-    movsxdifnidn wq, wd
     mov xq, wq
 
     add        srcq, wq
@@ -165,16 +163,12 @@ SHUFFLE_BYTES 3, 2, 1, 0
 ;              int lumStride, int chromStride, int srcStride)
 ;-----------------------------------------------------------------------------------------------
 %macro UYVY_TO_YUV422 0
-cglobal uyvytoyuv422, 9, 14, 8, ydst, udst, vdst, src, w, h, lum_stride, chrom_stride, src_stride, wtwo, whalf, tmp, x, back_w
+cglobal uyvytoyuv422, 9, 14, 8, "p", ydst, "p", udst, "p", vdst, "p", src, "d-", w, "d", h, \
+                                "d-", lum_stride, "d-", chrom_stride, "d-", src_stride, wtwo, whalf, tmp, x, back_w
     pxor         m0, m0
     pcmpeqw      m1, m1
     psrlw        m1, 8
 
-    movsxdifnidn            wq, wd
-    movsxdifnidn   lum_strideq, lum_strided
-    movsxdifnidn chrom_strideq, chrom_strided
-    movsxdifnidn   src_strideq, src_strided
-
     mov     back_wq, wq
     mov      whalfq, wq
     shr      whalfq, 1     ; whalf = width / 2
diff --git a/libswscale/x86/scale.asm b/libswscale/x86/scale.asm
index 83cabff722..4367f6eb5b 100644
--- a/libswscale/x86/scale.asm
+++ b/libswscale/x86/scale.asm
@@ -50,12 +50,11 @@ SECTION .text
 ; SCALE_FUNC source_width, intermediate_nbits, filtersize, filtersuffix, n_args, n_xmm
 %macro SCALE_FUNC 6
 %ifnidn %3, X
-cglobal hscale%1to%2_%4, %5, 7, %6, pos0, dst, w, src, filter, fltpos, pos1
+cglobal hscale%1to%2_%4, %5, 7, %6, "p", pos0, "p", dst, "d-", w, "p", src, "p", filter, "p", fltpos, pos1
 %else
-cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsize
+cglobal hscale%1to%2_%4, %5, 10, %6, "p", pos0, "p", dst, "d-", w, "p", srcmem, "p", filter, "p", fltpos, "d-", fltsize
 %endif
 %if ARCH_X86_64
-    movsxd        wq, wd
 %define mov32 movsxd
 %else ; x86-32
 %define mov32 mov
@@ -245,12 +244,13 @@ cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsi
 %endif ; %4 ==/!= X4
 %if ARCH_X86_64
 %define srcq    r8
+%define srcp    r8p
 %define pos1q   r7
 %define srcendq r9
-    movsxd  fltsizeq, fltsized                  ; filterSize
     lea      srcendq, [srcmemq+(fltsizeq-dlt)*srcmul] ; &src[filterSize&~4]
 %else ; x86-32
 %define srcq    srcmemq
+%define srcp    srcmemp
 %define pos1q   dstq
 %define srcendq r6m
     lea        pos0q, [srcmemq+(fltsizeq-dlt)*srcmul] ; &src[filterSize&~4]
@@ -262,7 +262,7 @@ cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsi
 %else ; %2 == 19
     lea         dstq, [dstq+wq*4]
 %endif ; %2 == 15/19
-    movifnidn  dstmp, dstq
+    movifnidn  dstmp, dstp
     neg           wq
 
 .loop:
@@ -271,7 +271,7 @@ cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsi
     ; FIXME maybe do 4px/iteration on x86-64 (x86-32 wouldn't have enough regs)?
     pxor          m4, m4
     pxor          m5, m5
-    mov         srcq, srcmemmp
+    mov         srcp, srcmemmp
 
 .innerloop:
     ; load 2x4 (mmx) or 2x8 (sse) source pixels into m0/m1 -> m4/m5
@@ -354,7 +354,7 @@ cglobal hscale%1to%2_%4, %5, 10, %6, pos0, dst, w, srcmem, filter, fltpos, fltsi
     ; clip, store
     psrad         m0, 14 + %1 - %2
 %ifidn %3, X
-    movifnidn   dstq, dstmp
+    movifnidn   dstp, dstmp
 %endif ; %3 == X
 %if %2 == 15
     packssdw      m0, m0
-- 
2.21.0


From 5e4d8819935e2e7386c9b9096a420d5573ca3fba Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:05:53 +0200
Subject: [PATCH 13/17] x32: libswresample
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libswresample/x86/audio_convert.asm | 77 +++++++++++++++--------------
 libswresample/x86/rematrix.asm      |  8 +--
 libswresample/x86/resample.asm      | 32 +++++-------
 3 files changed, 57 insertions(+), 60 deletions(-)

diff --git a/libswresample/x86/audio_convert.asm b/libswresample/x86/audio_convert.asm
index d441636d3c..81ac376fbd 100644
--- a/libswresample/x86/audio_convert.asm
+++ b/libswresample/x86/audio_convert.asm
@@ -32,10 +32,10 @@ SECTION .text
 
 ;to, from, a/u, log2_outsize, log_intsize, const
 %macro PACK_2CH 5-7
-cglobal pack_2ch_%2_to_%1_%3, 3, 4, 6, dst, src, len, src2
-    mov src2q   , [srcq+gprsize]
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
+cglobal pack_2ch_%2_to_%1_%3, 3, 4, 6, "p", dst, "p", src, "d-", len, src2
+    mov src2p   , [srcq+ptrsize]
+    mov srcp    , [srcq]
+    mov dstp    , [dstq]
 %ifidn %3, a
     test dstq, mmsize-1
         jne pack_2ch_%2_to_%1_u_int %+ SUFFIX
@@ -89,10 +89,10 @@ pack_2ch_%2_to_%1_u_int %+ SUFFIX:
 %endmacro
 
 %macro UNPACK_2CH 5-7
-cglobal unpack_2ch_%2_to_%1_%3, 3, 4, 7, dst, src, len, dst2
-    mov dst2q   , [dstq+gprsize]
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
+cglobal unpack_2ch_%2_to_%1_%3, 3, 4, 7, "p", dst, "p", src, "d-", len, dst2
+    mov dst2p   , [dstq+ptrsize]
+    mov srcp    , [srcq]
+    mov dstp    , [dstq]
 %ifidn %3, a
     test dstq, mmsize-1
         jne unpack_2ch_%2_to_%1_u_int %+ SUFFIX
@@ -161,9 +161,9 @@ unpack_2ch_%2_to_%1_u_int %+ SUFFIX:
 %endmacro
 
 %macro CONV 5-7
-cglobal %2_to_%1_%3, 3, 3, 6, dst, src, len
-    mov srcq    , [srcq]
-    mov dstq    , [dstq]
+cglobal %2_to_%1_%3, 3, 3, 6, "p", dst, "p", src, "d-", len
+    mov srcp    , [srcq]
+    mov dstp    , [dstq]
 %ifidn %3, a
     test dstq, mmsize-1
         jne %2_to_%1_u_int %+ SUFFIX
@@ -203,19 +203,19 @@ cglobal %2_to_%1_%3, 3, 3, 6, dst, src, len
 %endmacro
 
 %macro PACK_6CH 8
-cglobal pack_6ch_%2_to_%1_%3, 2, 8, %6, dst, src, src1, src2, src3, src4, src5, len
+cglobal pack_6ch_%2_to_%1_%3, 2, 8, %6, "p", dst, "p", src, src1, src2, src3, src4, src5, len
 %if ARCH_X86_64
     mov     lend, r2d
 %else
     %define lend dword r2m
 %endif
-    mov    src1q, [srcq+1*gprsize]
-    mov    src2q, [srcq+2*gprsize]
-    mov    src3q, [srcq+3*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src5q, [srcq+5*gprsize]
-    mov     srcq, [srcq]
-    mov     dstq, [dstq]
+    mov    src1p, [srcq+1*ptrsize]
+    mov    src2p, [srcq+2*ptrsize]
+    mov    src3p, [srcq+3*ptrsize]
+    mov    src4p, [srcq+4*ptrsize]
+    mov    src5p, [srcq+5*ptrsize]
+    mov     srcp, [srcq]
+    mov     dstp, [dstq]
 %ifidn %3, a
     test dstq, mmsize-1
         jne pack_6ch_%2_to_%1_u_int %+ SUFFIX
@@ -306,19 +306,19 @@ pack_6ch_%2_to_%1_u_int %+ SUFFIX:
 %endmacro
 
 %macro UNPACK_6CH 8
-cglobal unpack_6ch_%2_to_%1_%3, 2, 8, %6, dst, src, dst1, dst2, dst3, dst4, dst5, len
+cglobal unpack_6ch_%2_to_%1_%3, 2, 8, %6, "p", dst, "p", src, dst1, dst2, dst3, dst4, dst5, len
 %if ARCH_X86_64
     mov     lend, r2d
 %else
     %define lend dword r2m
 %endif
-    mov    dst1q, [dstq+1*gprsize]
-    mov    dst2q, [dstq+2*gprsize]
-    mov    dst3q, [dstq+3*gprsize]
-    mov    dst4q, [dstq+4*gprsize]
-    mov    dst5q, [dstq+5*gprsize]
-    mov     dstq, [dstq]
-    mov     srcq, [srcq]
+    mov    dst1p, [dstq+1*ptrsize]
+    mov    dst2p, [dstq+2*ptrsize]
+    mov    dst3p, [dstq+3*ptrsize]
+    mov    dst4p, [dstq+4*ptrsize]
+    mov    dst5p, [dstq+5*ptrsize]
+    mov     dstp, [dstq]
+    mov     srcp, [srcq]
 %ifidn %3, a
     test dstq, mmsize-1
         jne unpack_6ch_%2_to_%1_u_int %+ SUFFIX
@@ -381,36 +381,39 @@ unpack_6ch_%2_to_%1_u_int %+ SUFFIX:
 %define PACK_8CH_GPRS (10 * ARCH_X86_64) + ((6 + HAVE_ALIGNED_STACK) * ARCH_X86_32)
 
 %macro PACK_8CH 8
-cglobal pack_8ch_%2_to_%1_%3, 2, PACK_8CH_GPRS, %6, ARCH_X86_32*48, dst, src, len, src1, src2, src3, src4, src5, src6, src7
-    mov     dstq, [dstq]
+cglobal pack_8ch_%2_to_%1_%3, 2, PACK_8CH_GPRS, %6, ARCH_X86_32*48, "p", dst, "p", src, len, src1, src2, src3, src4, src5, src6, src7
+    mov     dstp, [dstq]
 %if ARCH_X86_32
     DEFINE_ARGS dst, src, src2, src3, src4, src5, src6
     %define lend dword r2m
     %define src1q r0q
+    %define src1p r0p
     %define src1m dword [rsp+32]
 %if HAVE_ALIGNED_STACK == 0
     DEFINE_ARGS dst, src, src2, src3, src5, src6
     %define src4q r0q
+    %define src4p r0p
     %define src4m dword [rsp+36]
 %endif
     %define src7q r0q
+    %define src7p r0p
     %define src7m dword [rsp+40]
     mov     dstm, dstq
 %endif
-    mov    src7q, [srcq+7*gprsize]
-    mov    src6q, [srcq+6*gprsize]
+    mov    src7p, [srcq+7*ptrsize]
+    mov    src6p, [srcq+6*ptrsize]
 %if ARCH_X86_32
     mov    src7m, src7q
 %endif
-    mov    src5q, [srcq+5*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src3q, [srcq+3*gprsize]
+    mov    src5p, [srcq+5*ptrsize]
+    mov    src4p, [srcq+4*ptrsize]
+    mov    src3p, [srcq+3*ptrsize]
 %if ARCH_X86_32 && HAVE_ALIGNED_STACK == 0
     mov    src4m, src4q
 %endif
-    mov    src2q, [srcq+2*gprsize]
-    mov    src1q, [srcq+1*gprsize]
-    mov     srcq, [srcq]
+    mov    src2p, [srcq+2*ptrsize]
+    mov    src1p, [srcq+1*ptrsize]
+    mov     srcp, [srcq]
 %ifidn %3, a
 %if ARCH_X86_32
     test dstmp, mmsize-1
diff --git a/libswresample/x86/rematrix.asm b/libswresample/x86/rematrix.asm
index 7984b9a729..744af4a88d 100644
--- a/libswresample/x86/rematrix.asm
+++ b/libswresample/x86/rematrix.asm
@@ -28,7 +28,7 @@ w1 : times 16 dw 1
 SECTION .text
 
 %macro MIX2_FLT 1
-cglobal mix_2_1_%1_float, 7, 7, 6, out, in1, in2, coeffp, index1, index2, len
+cglobal mix_2_1_%1_float, 7, 7, 6, "p", out, "p", in1, "p", in2, "p", coeffp, "q", index1, "q", index2, "q", len
 %ifidn %1, a
     test in1q, mmsize-1
         jne mix_2_1_float_u_int %+ SUFFIX
@@ -72,7 +72,7 @@ mix_2_1_float_u_int %+ SUFFIX:
 %endmacro
 
 %macro MIX1_FLT 1
-cglobal mix_1_1_%1_float, 5, 5, 3, out, in, coeffp, index, len
+cglobal mix_1_1_%1_float, 5, 5, 3, "p", out, "p", in, "p", coeffp, "q", index, "q", len
 %ifidn %1, a
     test inq, mmsize-1
         jne mix_1_1_float_u_int %+ SUFFIX
@@ -104,7 +104,7 @@ mix_1_1_float_u_int %+ SUFFIX:
 %endmacro
 
 %macro MIX1_INT16 1
-cglobal mix_1_1_%1_int16, 5, 5, 6, out, in, coeffp, index, len
+cglobal mix_1_1_%1_int16, 5, 5, 6, "p", out, "p", in, "p", coeffp, "q", index, "q", len
 %ifidn %1, a
     test inq, mmsize-1
         jne mix_1_1_int16_u_int %+ SUFFIX
@@ -157,7 +157,7 @@ mix_1_1_int16_u_int %+ SUFFIX:
 %endmacro
 
 %macro MIX2_INT16 1
-cglobal mix_2_1_%1_int16, 7, 7, 8, out, in1, in2, coeffp, index1, index2, len
+cglobal mix_2_1_%1_int16, 7, 7, 8, "p", out, "p", in1, "p", in2, "p", coeffp, "q", index1, "q", index2, "q", len
 %ifidn %1, a
     test in1q, mmsize-1
         jne mix_2_1_int16_u_int %+ SUFFIX
diff --git a/libswresample/x86/resample.asm b/libswresample/x86/resample.asm
index 7107cf9d42..3fbdb1abc6 100644
--- a/libswresample/x86/resample.asm
+++ b/libswresample/x86/resample.asm
@@ -22,15 +22,9 @@
 
 %include "libavutil/x86/x86util.asm"
 
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
 struc ResampleContext
-    .av_class:              pointer 1
-    .filter_bank:           pointer 1
+    .av_class:              resp 1
+    .filter_bank:           resp 1
     .filter_length:         resd 1
     .filter_alloc:          resd 1
     .ideal_dst_incr:        resd 1
@@ -59,7 +53,7 @@ SECTION .text
 ; int resample_common_$format(ResampleContext *ctx, $format *dst,
 ;                             const $format *src, int size, int update_ctx)
 %if ARCH_X86_64 ; unix64 and win64
-cglobal resample_common_%1, 0, 15, 2, ctx, dst, src, phase_count, index, frac, \
+cglobal resample_common_%1, 0, 15, 2, "p", ctx, "p", dst, "p", src, "d", phase_count, "d", index, frac, \
                                       dst_incr_mod, size, min_filter_count_x4, \
                                       min_filter_len_x4, dst_incr_div, src_incr, \
                                       phase_mask, dst_end, filter_bank
@@ -82,7 +76,7 @@ cglobal resample_common_%1, 0, 15, 2, ctx, dst, src, phase_count, index, frac, \
     mov                       indexd, [ctxq+ResampleContext.index]
     mov                        fracd, [ctxq+ResampleContext.frac]
     mov                dst_incr_modd, [ctxq+ResampleContext.dst_incr_mod]
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
+    mov                 filter_bankp, [ctxq+ResampleContext.filter_bank]
     mov                    src_incrd, [ctxq+ResampleContext.src_incr]
     mov                   ctx_stackq, ctxq
     mov           min_filter_len_x4d, [ctxq+ResampleContext.filter_length]
@@ -111,8 +105,8 @@ cglobal resample_common_%1, 0, 15, 2, ctx, dst, src, phase_count, index, frac, \
     sub                         srcq, min_filter_len_x4q
     mov                   src_stackq, srcq
 %else ; x86-32
-cglobal resample_common_%1, 1, 7, 2, ctx, phase_count, dst, frac, \
-                                     index, min_filter_length_x4, filter_bank
+cglobal resample_common_%1, 1, 7, 2, "p", ctx, "p", phase_count, "p", dst, "d", frac, \
+                                     "d", index, min_filter_length_x4, filter_bank
 
     ; push temp variables to stack
 %define ctx_stackq            r0mp
@@ -133,7 +127,7 @@ cglobal resample_common_%1, 1, 7, 2, ctx, phase_count, dst, frac, \
     shl        min_filter_length_x4d, %3
     mov                        fracd, [ctxq+ResampleContext.frac]
     neg        min_filter_length_x4q
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
+    mov                 filter_bankp, [ctxq+ResampleContext.filter_bank]
     sub                         r2mp, min_filter_length_x4q
     sub                 filter_bankq, min_filter_length_x4q
     PUSH                              min_filter_length_x4q
@@ -273,14 +267,14 @@ cglobal resample_common_%1, 1, 7, 2, ctx, phase_count, dst, frac, \
 ;                             const float *src, int size, int update_ctx)
 %if ARCH_X86_64 ; unix64 and win64
 %if UNIX64
-cglobal resample_linear_%1, 0, 15, 5, ctx, dst, phase_mask, phase_count, index, frac, \
+cglobal resample_linear_%1, 0, 15, 5, "p", ctx, "p", dst, "p", phase_mask, "d", phase_count, "d", index, frac, \
                                       size, dst_incr_mod, min_filter_count_x4, \
                                       min_filter_len_x4, dst_incr_div, src_incr, \
                                       src, dst_end, filter_bank
 
-    mov                         srcq, r2mp
+    mov                         srcp, r2mp
 %else ; win64
-cglobal resample_linear_%1, 0, 15, 5, ctx, phase_mask, src, phase_count, index, frac, \
+cglobal resample_linear_%1, 0, 15, 5, "p", ctx, "p", phase_mask, "p", src, "d", phase_count, "d", index, frac, \
                                       size, dst_incr_mod, min_filter_count_x4, \
                                       min_filter_len_x4, dst_incr_div, src_incr, \
                                       dst, dst_end, filter_bank
@@ -307,7 +301,7 @@ cglobal resample_linear_%1, 0, 15, 5, ctx, phase_mask, src, phase_count, index,
     mov                       indexd, [ctxq+ResampleContext.index]
     mov                        fracd, [ctxq+ResampleContext.frac]
     mov                dst_incr_modd, [ctxq+ResampleContext.dst_incr_mod]
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
+    mov                 filter_bankp, [ctxq+ResampleContext.filter_bank]
     mov                    src_incrd, [ctxq+ResampleContext.src_incr]
     mov                   ctx_stackq, ctxq
     mov           min_filter_len_x4d, [ctxq+ResampleContext.filter_length]
@@ -343,7 +337,7 @@ cglobal resample_linear_%1, 0, 15, 5, ctx, phase_mask, src, phase_count, index,
     sub                         srcq, min_filter_len_x4q
     mov                   src_stackq, srcq
 %else ; x86-32
-cglobal resample_linear_%1, 1, 7, 5, ctx, min_filter_length_x4, filter2, \
+cglobal resample_linear_%1, 1, 7, 5, "p", ctx, min_filter_length_x4, filter2, \
                                      frac, index, dst, filter_bank
 
     ; push temp variables to stack
@@ -376,7 +370,7 @@ cglobal resample_linear_%1, 1, 7, 5, ctx, min_filter_length_x4, filter2, \
     shl        min_filter_length_x4d, %3
     mov                        fracd, [ctxq+ResampleContext.frac]
     neg        min_filter_length_x4q
-    mov                 filter_bankq, [ctxq+ResampleContext.filter_bank]
+    mov                 filter_bankp, [ctxq+ResampleContext.filter_bank]
     sub                         r2mp, min_filter_length_x4q
     sub                 filter_bankq, min_filter_length_x4q
     PUSH                              min_filter_length_x4q
-- 
2.21.0


From 3f2985b355fee1bf2c09a953ae76ac9a068e8e87 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:06:09 +0200
Subject: [PATCH 14/17] x32: libpostproc
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libpostproc/postprocess_template.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/libpostproc/postprocess_template.c b/libpostproc/postprocess_template.c
index f4a69f056e..3f13761de1 100644
--- a/libpostproc/postprocess_template.c
+++ b/libpostproc/postprocess_template.c
@@ -1184,10 +1184,10 @@ FIND_MIN_MAX((%0, %1, 8))
 #endif
         "movq %%mm6, %%mm0                      \n\t" // max
         "psubb %%mm7, %%mm6                     \n\t" // max - min
-        "push %%"FF_REG_a"                      \n\t"
+        "push %%"FF_REG_A"                      \n\t"
         "movd %%mm6, %%eax                      \n\t"
         "cmpb "MANGLE(deringThreshold)", %%al   \n\t"
-        "pop %%"FF_REG_a"                       \n\t"
+        "pop %%"FF_REG_A"                       \n\t"
         " jb 1f                                 \n\t"
         PAVGB(%%mm0, %%mm7)                           // a=(max + min)/2
         "punpcklbw %%mm7, %%mm7                 \n\t"
-- 
2.21.0


From 4d524718c69ba5246675737eda30ea6953958712 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:06:23 +0200
Subject: [PATCH 15/17] x32: libavresample
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavresample/x86/audio_convert.asm | 302 ++++++++++++++--------------
 libavresample/x86/audio_mix.asm     | 105 +++++-----
 libavresample/x86/dither.asm        |  28 +--
 3 files changed, 219 insertions(+), 216 deletions(-)

diff --git a/libavresample/x86/audio_convert.asm b/libavresample/x86/audio_convert.asm
index c6a5015282..03ce56f088 100644
--- a/libavresample/x86/audio_convert.asm
+++ b/libavresample/x86/audio_convert.asm
@@ -43,10 +43,10 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 INIT_XMM sse2
-cglobal conv_s16_to_s32, 3,3,3, dst, src, len
-    lea      lenq, [2*lend]
-    lea      dstq, [dstq+2*lenq]
-    add      srcq, lenq
+cglobal conv_s16_to_s32, 3, 3, 3, "p", dst, "p", src, "d", len
+    add      lend, lend
+    add      srcp, lenp
+    lea      dstp, [dstq+2*lenq]
     neg      lenq
 .loop:
     mova       m2, [srcq+lenq]
@@ -65,10 +65,10 @@ cglobal conv_s16_to_s32, 3,3,3, dst, src, len
 ;------------------------------------------------------------------------------
 
 %macro CONV_S16_TO_FLT 0
-cglobal conv_s16_to_flt, 3,3,3, dst, src, len
-    lea      lenq, [2*lend]
-    add      srcq, lenq
-    lea      dstq, [dstq + 2*lenq]
+cglobal conv_s16_to_flt, 3, 3, 3, "p", dst, "p", src, "d", len
+    add      lend, lend
+    add      srcp, lenp
+    lea      dstp, [dstq + 2*lenq]
     neg      lenq
     mova       m2, [pf_s16_inv_scale]
     ALIGN 16
@@ -96,10 +96,10 @@ CONV_S16_TO_FLT
 ;------------------------------------------------------------------------------
 
 %macro CONV_S32_TO_S16 0
-cglobal conv_s32_to_s16, 3,3,4, dst, src, len
-    lea     lenq, [2*lend]
-    lea     srcq, [srcq+2*lenq]
-    add     dstq, lenq
+cglobal conv_s32_to_s16, 3, 3, 4, "p", dst, "p", src, "d", len
+    add     lend, lend
+    lea     srcp, [srcq+2*lenq]
+    add     dstp, lenp
     neg     lenq
 .loop:
     mova      m0, [srcq+2*lenq         ]
@@ -134,10 +134,10 @@ CONV_S32_TO_S16
 ;------------------------------------------------------------------------------
 
 %macro CONV_S32_TO_FLT 0
-cglobal conv_s32_to_flt, 3,3,3, dst, src, len
-    lea     lenq, [4*lend]
-    add     srcq, lenq
-    add     dstq, lenq
+cglobal conv_s32_to_flt, 3, 3, 3, "p", dst, "p", src, "d", len
+    lea     lend, [4*lend]
+    add     srcp, lenp
+    add     dstp, lenp
     neg     lenq
     mova      m0, [pf_s32_inv_scale]
     ALIGN 16
@@ -165,10 +165,10 @@ CONV_S32_TO_FLT
 ;------------------------------------------------------------------------------
 
 INIT_XMM sse2
-cglobal conv_flt_to_s16, 3,3,5, dst, src, len
-    lea     lenq, [2*lend]
-    lea     srcq, [srcq+2*lenq]
-    add     dstq, lenq
+cglobal conv_flt_to_s16, 3, 3, 5, "p", dst, "p", src, "d", len
+    add     lend, lend
+    lea     srcp, [srcq+2*lenq]
+    add     dstp, lenp
     neg     lenq
     mova      m4, [pf_s16_scale]
 .loop:
@@ -197,10 +197,10 @@ cglobal conv_flt_to_s16, 3,3,5, dst, src, len
 ;------------------------------------------------------------------------------
 
 %macro CONV_FLT_TO_S32 0
-cglobal conv_flt_to_s32, 3,3,6, dst, src, len
-    lea     lenq, [lend*4]
-    add     srcq, lenq
-    add     dstq, lenq
+cglobal conv_flt_to_s32, 3, 3, 6, "p", dst, "p", src, "d", len
+    lea     lend, [lend*4]
+    add     srcp, lenp
+    add     dstp, lenp
     neg     lenq
     mova      m4, [pf_s32_scale]
     mova      m5, [pf_s32_clip]
@@ -239,13 +239,13 @@ CONV_FLT_TO_S32
 ;------------------------------------------------------------------------------
 
 %macro CONV_S16P_TO_S16_2CH 0
-cglobal conv_s16p_to_s16_2ch, 3,4,5, dst, src0, len, src1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q        ]
-    lea        lenq, [2*lend]
-    add       src0q, lenq
-    add       src1q, lenq
-    lea        dstq, [dstq+2*lenq]
+cglobal conv_s16p_to_s16_2ch, 3, 4, 5, "p", dst, "p", src0, "d", len, src1
+    mov       src1p, [src0q+ptrsize]
+    mov       src0p, [src0q        ]
+    add        lend, lend
+    add       src0p, lenp
+    add       src1p, lenp
+    lea        dstp, [dstq+2*lenq]
     neg        lenq
 .loop:
     mova         m0, [src0q+lenq       ]
@@ -286,17 +286,17 @@ CONV_S16P_TO_S16_2CH
 
 %macro CONV_S16P_TO_S16_6CH 0
 %if ARCH_X86_64
-cglobal conv_s16p_to_s16_6ch, 3,8,7, dst, src0, len, src1, src2, src3, src4, src5
+cglobal conv_s16p_to_s16_6ch, 3, 8, 7, "p", dst, "p", src0, "d", len, src1, src2, src3, src4, src5
 %else
-cglobal conv_s16p_to_s16_6ch, 2,7,7, dst, src0, src1, src2, src3, src4, src5
-%define lend dword r2m
+cglobal conv_s16p_to_s16_6ch, 2, 7, 7, "p", dst, "p", src0, "d*", src1, src2, src3, src4, src5
+%define lend r2md
 %endif
-    mov      src1q, [src0q+1*gprsize]
-    mov      src2q, [src0q+2*gprsize]
-    mov      src3q, [src0q+3*gprsize]
-    mov      src4q, [src0q+4*gprsize]
-    mov      src5q, [src0q+5*gprsize]
-    mov      src0q, [src0q]
+    mov      src1p, [src0q+1*ptrsize]
+    mov      src2p, [src0q+2*ptrsize]
+    mov      src3p, [src0q+3*ptrsize]
+    mov      src4p, [src0q+4*ptrsize]
+    mov      src5p, [src0q+5*ptrsize]
+    mov      src0p, [src0q]
     sub      src1q, src0q
     sub      src2q, src0q
     sub      src3q, src0q
@@ -399,13 +399,13 @@ CONV_S16P_TO_S16_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_S16P_TO_FLT_2CH 0
-cglobal conv_s16p_to_flt_2ch, 3,4,6, dst, src0, len, src1
-    lea       lenq, [2*lend]
-    mov      src1q, [src0q+gprsize]
-    mov      src0q, [src0q        ]
-    lea       dstq, [dstq+4*lenq]
-    add      src0q, lenq
-    add      src1q, lenq
+cglobal conv_s16p_to_flt_2ch, 3, 4, 6, "p", dst, "p", src0, "d", len, src1
+    add       lend, lend
+    mov      src1p, [src0q+ptrsize]
+    mov      src0p, [src0q        ]
+    lea       dstp, [dstq+4*lenq]
+    add      src0p, lenp
+    add      src1p, lenp
     neg       lenq
     mova        m5, [pf_s32_inv_scale]
 .loop:
@@ -449,17 +449,17 @@ CONV_S16P_TO_FLT_2CH
 
 %macro CONV_S16P_TO_FLT_6CH 0
 %if ARCH_X86_64
-cglobal conv_s16p_to_flt_6ch, 3,8,8, dst, src, len, src1, src2, src3, src4, src5
+cglobal conv_s16p_to_flt_6ch, 3, 8, 8, "p", dst, "p", src, "d", len, src1, src2, src3, src4, src5
 %else
-cglobal conv_s16p_to_flt_6ch, 2,7,8, dst, src, src1, src2, src3, src4, src5
-%define lend dword r2m
+cglobal conv_s16p_to_flt_6ch, 2, 7, 8, "p", dst, "p", src, src1, src2, src3, src4, src5
+%define lend r2md
 %endif
-    mov     src1q, [srcq+1*gprsize]
-    mov     src2q, [srcq+2*gprsize]
-    mov     src3q, [srcq+3*gprsize]
-    mov     src4q, [srcq+4*gprsize]
-    mov     src5q, [srcq+5*gprsize]
-    mov      srcq, [srcq]
+    mov     src1p, [srcq+1*ptrsize]
+    mov     src2p, [srcq+2*ptrsize]
+    mov     src3p, [srcq+3*ptrsize]
+    mov     src4p, [srcq+4*ptrsize]
+    mov     src5p, [srcq+5*ptrsize]
+    mov      srcp, [srcq]
     sub     src1q, srcq
     sub     src2q, srcq
     sub     src3q, srcq
@@ -552,13 +552,13 @@ CONV_S16P_TO_FLT_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_FLTP_TO_S16_2CH 0
-cglobal conv_fltp_to_s16_2ch, 3,4,3, dst, src0, len, src1
-    lea      lenq, [4*lend]
-    mov     src1q, [src0q+gprsize]
-    mov     src0q, [src0q        ]
-    add      dstq, lenq
-    add     src0q, lenq
-    add     src1q, lenq
+cglobal conv_fltp_to_s16_2ch, 3, 4, 3, "p", dst, "p", src0, "d", len, src1
+    lea      lend, [4*lend]
+    mov     src1p, [src0q+ptrsize]
+    mov     src0p, [src0q        ]
+    add      dstp, lenp
+    add     src0p, lenp
+    add     src1p, lenp
     neg      lenq
     mova       m2, [pf_s16_scale]
 %if cpuflag(ssse3)
@@ -595,17 +595,17 @@ CONV_FLTP_TO_S16_2CH
 
 %macro CONV_FLTP_TO_S16_6CH 0
 %if ARCH_X86_64
-cglobal conv_fltp_to_s16_6ch, 3,8,7, dst, src, len, src1, src2, src3, src4, src5
+cglobal conv_fltp_to_s16_6ch, 3, 8, 7, "p", dst, "p", src, "d", len, src1, src2, src3, src4, src5
 %else
-cglobal conv_fltp_to_s16_6ch, 2,7,7, dst, src, src1, src2, src3, src4, src5
-%define lend dword r2m
+cglobal conv_fltp_to_s16_6ch, 2, 7, 7, "p", dst, "p", src, src1, src2, src3, src4, src5
+%define lend r2md
 %endif
-    mov        src1q, [srcq+1*gprsize]
-    mov        src2q, [srcq+2*gprsize]
-    mov        src3q, [srcq+3*gprsize]
-    mov        src4q, [srcq+4*gprsize]
-    mov        src5q, [srcq+5*gprsize]
-    mov         srcq, [srcq]
+    mov        src1p, [srcq+1*ptrsize]
+    mov        src2p, [srcq+2*ptrsize]
+    mov        src3p, [srcq+3*ptrsize]
+    mov        src4p, [srcq+4*ptrsize]
+    mov        src5p, [srcq+5*ptrsize]
+    mov         srcp, [srcq]
     sub        src1q, srcq
     sub        src2q, srcq
     sub        src3q, srcq
@@ -708,13 +708,13 @@ CONV_FLTP_TO_S16_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_FLTP_TO_FLT_2CH 0
-cglobal conv_fltp_to_flt_2ch, 3,4,5, dst, src0, len, src1
-    mov  src1q, [src0q+gprsize]
-    mov  src0q, [src0q]
-    lea   lenq, [4*lend]
-    add  src0q, lenq
-    add  src1q, lenq
-    lea   dstq, [dstq+2*lenq]
+cglobal conv_fltp_to_flt_2ch, 3, 4, 5, "p", dst, "p", src0, "d", len, src1
+    mov  src1p, [src0q+ptrsize]
+    mov  src0p, [src0q]
+    lea   lend, [4*lend]
+    add  src0p, lenp
+    add  src1p, lenp
+    lea   dstp, [dstq+2*lenq]
     neg   lenq
 .loop:
     mova    m0, [src0q+lenq       ]
@@ -745,18 +745,18 @@ CONV_FLTP_TO_FLT_2CH
 ;-----------------------------------------------------------------------------
 
 %macro CONV_FLTP_TO_FLT_6CH 0
-cglobal conv_fltp_to_flt_6ch, 2,8,7, dst, src, src1, src2, src3, src4, src5, len
+cglobal conv_fltp_to_flt_6ch, 2, 8, 7, "p", dst, "p", src, src1, src2, src3, src4, src5, len
 %if ARCH_X86_64
     mov     lend, r2d
 %else
-    %define lend dword r2m
+    %define lend r2md
 %endif
-    mov    src1q, [srcq+1*gprsize]
-    mov    src2q, [srcq+2*gprsize]
-    mov    src3q, [srcq+3*gprsize]
-    mov    src4q, [srcq+4*gprsize]
-    mov    src5q, [srcq+5*gprsize]
-    mov     srcq, [srcq]
+    mov    src1p, [srcq+1*ptrsize]
+    mov    src2p, [srcq+2*ptrsize]
+    mov    src3p, [srcq+3*ptrsize]
+    mov    src4p, [srcq+4*ptrsize]
+    mov    src5p, [srcq+5*ptrsize]
+    mov     srcp, [srcq]
     sub    src1q, srcq
     sub    src2q, srcq
     sub    src3q, srcq
@@ -826,13 +826,13 @@ CONV_FLTP_TO_FLT_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_S16_TO_S16P_2CH 0
-cglobal conv_s16_to_s16p_2ch, 3,4,4, dst0, src, len, dst1
-    lea       lenq, [2*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    lea       srcq, [srcq+2*lenq]
-    add      dst0q, lenq
-    add      dst1q, lenq
+cglobal conv_s16_to_s16p_2ch, 3, 4, 4, "p", dst0, "p", src, "d", len, dst1
+    add       lend, lend
+    mov      dst1p, [dst0q+ptrsize]
+    mov      dst0p, [dst0q        ]
+    lea       srcp, [srcq+2*lenq]
+    add      dst0p, lenp
+    add      dst1p, lenp
     neg       lenq
 %if cpuflag(ssse3)
     mova        m3, [pb_deinterleave_words]
@@ -876,17 +876,17 @@ CONV_S16_TO_S16P_2CH
 
 %macro CONV_S16_TO_S16P_6CH 0
 %if ARCH_X86_64
-cglobal conv_s16_to_s16p_6ch, 3,8,5, dst, src, len, dst1, dst2, dst3, dst4, dst5
+cglobal conv_s16_to_s16p_6ch, 3, 8, 5, "p", dst, "p", src, "d", len, dst1, dst2, dst3, dst4, dst5
 %else
-cglobal conv_s16_to_s16p_6ch, 2,7,5, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
+cglobal conv_s16_to_s16p_6ch, 2, 7, 5, "p", dst, "p", src, dst1, dst2, dst3, dst4, dst5
+%define lend r2md
 %endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
+    mov     dst1p, [dstq+  ptrsize]
+    mov     dst2p, [dstq+2*ptrsize]
+    mov     dst3p, [dstq+3*ptrsize]
+    mov     dst4p, [dstq+4*ptrsize]
+    mov     dst5p, [dstq+5*ptrsize]
+    mov      dstp, [dstq          ]
     sub     dst1q, dstq
     sub     dst2q, dstq
     sub     dst3q, dstq
@@ -934,13 +934,13 @@ CONV_S16_TO_S16P_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_S16_TO_FLTP_2CH 0
-cglobal conv_s16_to_fltp_2ch, 3,4,5, dst0, src, len, dst1
-    lea       lenq, [4*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    add       srcq, lenq
-    add      dst0q, lenq
-    add      dst1q, lenq
+cglobal conv_s16_to_fltp_2ch, 3, 4, 5, "p", dst0, "p", src, "d", len, dst1
+    lea       lend, [4*lend]
+    mov      dst1p, [dst0q+ptrsize]
+    mov      dst0p, [dst0q        ]
+    add       srcp, lenp
+    add      dst0p, lenp
+    add      dst1p, lenp
     neg       lenq
     mova        m3, [pf_s32_inv_scale]
     mova        m4, [pw_zero_even]
@@ -973,17 +973,17 @@ CONV_S16_TO_FLTP_2CH
 
 %macro CONV_S16_TO_FLTP_6CH 0
 %if ARCH_X86_64
-cglobal conv_s16_to_fltp_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
+cglobal conv_s16_to_fltp_6ch, 3, 8, 7, "p", dst, "p", src, "d", len, dst1, dst2, dst3, dst4, dst5
 %else
-cglobal conv_s16_to_fltp_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
+cglobal conv_s16_to_fltp_6ch, 2, 7, 7, "p", dst, "p", src, dst1, dst2, dst3, dst4, dst5
+%define lend r2md
 %endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
+    mov     dst1p, [dstq+  ptrsize]
+    mov     dst2p, [dstq+2*ptrsize]
+    mov     dst3p, [dstq+3*ptrsize]
+    mov     dst4p, [dstq+4*ptrsize]
+    mov     dst5p, [dstq+5*ptrsize]
+    mov      dstp, [dstq          ]
     sub     dst1q, dstq
     sub     dst2q, dstq
     sub     dst3q, dstq
@@ -1053,13 +1053,13 @@ CONV_S16_TO_FLTP_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_FLT_TO_S16P_2CH 0
-cglobal conv_flt_to_s16p_2ch, 3,4,6, dst0, src, len, dst1
-    lea       lenq, [2*lend]
-    mov      dst1q, [dst0q+gprsize]
-    mov      dst0q, [dst0q        ]
-    lea       srcq, [srcq+4*lenq]
-    add      dst0q, lenq
-    add      dst1q, lenq
+cglobal conv_flt_to_s16p_2ch, 3, 4, 6, "p", dst0, "p", src, "d", len, dst1
+    add       lend, lend
+    mov      dst1p, [dst0q+ptrsize]
+    mov      dst0p, [dst0q        ]
+    lea       srcp, [srcq+4*lenq]
+    add      dst0p, lenp
+    add      dst1p, lenp
     neg       lenq
     mova        m5, [pf_s16_scale]
 .loop:
@@ -1100,17 +1100,17 @@ CONV_FLT_TO_S16P_2CH
 
 %macro CONV_FLT_TO_S16P_6CH 0
 %if ARCH_X86_64
-cglobal conv_flt_to_s16p_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
+cglobal conv_flt_to_s16p_6ch, 3, 8, 7, "p", dst, "p", src, "d", len, dst1, dst2, dst3, dst4, dst5
 %else
-cglobal conv_flt_to_s16p_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
+cglobal conv_flt_to_s16p_6ch, 2, 7, 7, "p", dst, "p", src, dst1, dst2, dst3, dst4, dst5
+%define lend r2md
 %endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
+    mov     dst1p, [dstq+  ptrsize]
+    mov     dst2p, [dstq+2*ptrsize]
+    mov     dst3p, [dstq+3*ptrsize]
+    mov     dst4p, [dstq+4*ptrsize]
+    mov     dst5p, [dstq+5*ptrsize]
+    mov      dstp, [dstq          ]
     sub     dst1q, dstq
     sub     dst2q, dstq
     sub     dst3q, dstq
@@ -1171,13 +1171,13 @@ CONV_FLT_TO_S16P_6CH
 ;------------------------------------------------------------------------------
 
 %macro CONV_FLT_TO_FLTP_2CH 0
-cglobal conv_flt_to_fltp_2ch, 3,4,3, dst0, src, len, dst1
-    lea    lenq, [4*lend]
-    mov   dst1q, [dst0q+gprsize]
-    mov   dst0q, [dst0q        ]
-    lea    srcq, [srcq+2*lenq]
-    add   dst0q, lenq
-    add   dst1q, lenq
+cglobal conv_flt_to_fltp_2ch, 3, 4, 3, "p", dst0, "p", src, "d", len, dst1
+    lea    lend, [4*lend]
+    mov   dst1p, [dst0q+ptrsize]
+    mov   dst0p, [dst0q        ]
+    lea    srcp, [srcq+2*lenq]
+    add   dst0p, lenp
+    add   dst1p, lenp
     neg    lenq
 .loop:
     mova     m0, [srcq+2*lenq       ]
@@ -1204,17 +1204,17 @@ CONV_FLT_TO_FLTP_2CH
 
 %macro CONV_FLT_TO_FLTP_6CH 0
 %if ARCH_X86_64
-cglobal conv_flt_to_fltp_6ch, 3,8,7, dst, src, len, dst1, dst2, dst3, dst4, dst5
+cglobal conv_flt_to_fltp_6ch, 3, 8, 7, "p", dst, "p", src, "d", len, dst1, dst2, dst3, dst4, dst5
 %else
-cglobal conv_flt_to_fltp_6ch, 2,7,7, dst, src, dst1, dst2, dst3, dst4, dst5
-%define lend dword r2m
+cglobal conv_flt_to_fltp_6ch, 2, 7, 7, "p", dst, "p", src, dst1, dst2, dst3, dst4, dst5
+%define lend r2md
 %endif
-    mov     dst1q, [dstq+  gprsize]
-    mov     dst2q, [dstq+2*gprsize]
-    mov     dst3q, [dstq+3*gprsize]
-    mov     dst4q, [dstq+4*gprsize]
-    mov     dst5q, [dstq+5*gprsize]
-    mov      dstq, [dstq          ]
+    mov     dst1p, [dstq+  ptrsize]
+    mov     dst2p, [dstq+2*ptrsize]
+    mov     dst3p, [dstq+3*ptrsize]
+    mov     dst4p, [dstq+4*ptrsize]
+    mov     dst5p, [dstq+5*ptrsize]
+    mov      dstp, [dstq          ]
     sub     dst1q, dstq
     sub     dst2q, dstq
     sub     dst3q, dstq
diff --git a/libavresample/x86/audio_mix.asm b/libavresample/x86/audio_mix.asm
index fe27d6a6c9..59dcde9404 100644
--- a/libavresample/x86/audio_mix.asm
+++ b/libavresample/x86/audio_mix.asm
@@ -30,11 +30,11 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 
 %macro MIX_2_TO_1_FLTP_FLT 0
-cglobal mix_2_to_1_fltp_flt, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq        ]
+cglobal mix_2_to_1_fltp_flt, 3, 4, 6, "p", src, "p", matrix, "d", len, src1
+    mov       src1p, [srcq+ptrsize]
+    mov        srcp, [srcq        ]
     sub       src1q, srcq
-    mov     matrixq, [matrixq  ]
+    mov     matrixp, [matrixq  ]
     VBROADCASTSS m4, [matrixq  ]
     VBROADCASTSS m5, [matrixq+4]
     ALIGN 16
@@ -47,7 +47,7 @@ cglobal mix_2_to_1_fltp_flt, 3,4,6, src, matrix, len, src1
     addps        m2, m2, m3
     mova  [srcq       ], m0
     mova  [srcq+mmsize], m2
-    add        srcq, mmsize*2
+    add        srcp, mmsize*2
     sub        lend, mmsize*2/4
     jg .loop
     REP_RET
@@ -66,11 +66,11 @@ MIX_2_TO_1_FLTP_FLT
 ;-----------------------------------------------------------------------------
 
 %macro MIX_2_TO_1_S16P_FLT 0
-cglobal mix_2_to_1_s16p_flt, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq]
+cglobal mix_2_to_1_s16p_flt, 3, 4, 6, "p", src, "p", matrix, "d", len, src1
+    mov       src1p, [srcq+ptrsize]
+    mov        srcp, [srcq]
     sub       src1q, srcq
-    mov     matrixq, [matrixq  ]
+    mov     matrixp, [matrixq  ]
     VBROADCASTSS m4, [matrixq  ]
     VBROADCASTSS m5, [matrixq+4]
     ALIGN 16
@@ -93,7 +93,7 @@ cglobal mix_2_to_1_s16p_flt, 3,4,6, src, matrix, len, src1
     cvtps2dq     m1, m1
     packssdw     m0, m1
     mova     [srcq], m0
-    add        srcq, mmsize
+    add        srcp, mmsize
     sub        lend, mmsize/2
     jg .loop
     REP_RET
@@ -110,11 +110,11 @@ MIX_2_TO_1_S16P_FLT
 ;-----------------------------------------------------------------------------
 
 INIT_XMM sse2
-cglobal mix_2_to_1_s16p_q8, 3,4,6, src, matrix, len, src1
-    mov       src1q, [srcq+gprsize]
-    mov        srcq, [srcq]
+cglobal mix_2_to_1_s16p_q8, 3, 4, 6, "p", src, "p", matrix, "d", len, src1
+    mov       src1p, [srcq+ptrsize]
+    mov        srcp, [srcq]
     sub       src1q, srcq
-    mov     matrixq, [matrixq]
+    mov     matrixp, [matrixq]
     movd         m4, [matrixq]
     movd         m5, [matrixq]
     SPLATW       m4, m4, 0
@@ -140,7 +140,7 @@ cglobal mix_2_to_1_s16p_q8, 3,4,6, src, matrix, len, src1
     psrad        m1, 8
     packssdw     m0, m1
     mova     [srcq], m0
-    add        srcq, mmsize
+    add        srcp, mmsize
     sub        lend, mmsize/2
     jg .loop
     REP_RET
@@ -151,12 +151,12 @@ cglobal mix_2_to_1_s16p_q8, 3,4,6, src, matrix, len, src1
 ;-----------------------------------------------------------------------------
 
 %macro MIX_1_TO_2_FLTP_FLT 0
-cglobal mix_1_to_2_fltp_flt, 3,5,4, src0, matrix0, len, src1, matrix1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q]
+cglobal mix_1_to_2_fltp_flt, 3, 5, 4, "p", src0, "p", matrix0, "d", len, src1, matrix1
+    mov       src1p, [src0q+ptrsize]
+    mov       src0p, [src0q]
     sub       src1q, src0q
-    mov    matrix1q, [matrix0q+gprsize]
-    mov    matrix0q, [matrix0q]
+    mov    matrix1p, [matrix0q+ptrsize]
+    mov    matrix0p, [matrix0q]
     VBROADCASTSS m2, [matrix0q]
     VBROADCASTSS m3, [matrix1q]
     ALIGN 16
@@ -166,7 +166,7 @@ cglobal mix_1_to_2_fltp_flt, 3,5,4, src0, matrix0, len, src1, matrix1
     mulps        m0, m0, m2
     mova  [src0q      ], m0
     mova  [src0q+src1q], m1
-    add       src0q, mmsize
+    add       src0p, mmsize
     sub        lend, mmsize/4
     jg .loop
     REP_RET
@@ -185,12 +185,12 @@ MIX_1_TO_2_FLTP_FLT
 ;-----------------------------------------------------------------------------
 
 %macro MIX_1_TO_2_S16P_FLT 0
-cglobal mix_1_to_2_s16p_flt, 3,5,6, src0, matrix0, len, src1, matrix1
-    mov       src1q, [src0q+gprsize]
-    mov       src0q, [src0q]
+cglobal mix_1_to_2_s16p_flt, 3, 5, 6, "p", src0, "p", matrix0, "d", len, src1, matrix1
+    mov       src1p, [src0q+ptrsize]
+    mov       src0p, [src0q]
     sub       src1q, src0q
-    mov    matrix1q, [matrix0q+gprsize]
-    mov    matrix0q, [matrix0q]
+    mov    matrix1p, [matrix0q+ptrsize]
+    mov    matrix0p, [matrix0q]
     VBROADCASTSS m4, [matrix0q]
     VBROADCASTSS m5, [matrix1q]
     ALIGN 16
@@ -211,7 +211,7 @@ cglobal mix_1_to_2_s16p_flt, 3,5,6, src0, matrix0, len, src1, matrix1
     packssdw     m1, m3
     mova  [src0q      ], m0
     mova  [src0q+src1q], m1
-    add       src0q, mmsize
+    add       src0p, mmsize
     sub        lend, mmsize/2
     jg .loop
     REP_RET
@@ -273,7 +273,7 @@ MIX_1_TO_2_S16P_FLT
 %assign needed_stack_size needed_stack_size - 16
 %endif
 
-cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, needed_stack_size, src0, src1, len, src2, src3, src4, src5, src6, src7
+cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, needed_stack_size, "p", src0, "p", src1, "d-", len, src2, src3, src4, src5, src6, src7
 
 ; define src pointers on stack if needed
 %if matrix_elements_stack > 0 && ARCH_X86_32 && in_channels >= 7
@@ -283,12 +283,14 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
 %endif
 
 ; load matrix pointers
+%define matrix0p r1p
+%define matrix1p r3p
 %define matrix0q r1q
 %define matrix1q r3q
 %if stereo
-    mov      matrix1q, [matrix0q+gprsize]
+    mov      matrix1p, [matrix0q+ptrsize]
 %endif
-    mov      matrix0q, [matrix0q]
+    mov      matrix0p, [matrix0q]
 
 ; define matrix coeff names
 %assign %%i 0
@@ -340,24 +342,21 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
 %endrep
 
 ; load channel pointers to registers as offsets from the first channel pointer
-%if ARCH_X86_64
-    movsxd       lenq, r2d
-%endif
     shl          lenq, 2-is_s16
 %assign %%i 1
 %rep (in_channels - 1)
     %if ARCH_X86_32 && in_channels >= 7 && %%i >= 5
-    mov         src5q, [src0q+%%i*gprsize]
-    add         src5q, lenq
-    mov         src %+ %%i %+ m, src5q
+    mov         src5p, [src0q+%%i*ptrsize]
+    add         src5p, lenp
+    mov         src %+ %%i %+ mp, src5p
     %else
-    mov         src %+ %%i %+ q, [src0q+%%i*gprsize]
-    add         src %+ %%i %+ q, lenq
+    mov         src %+ %%i %+ p, [src0q+%%i*ptrsize]
+    add         src %+ %%i %+ p, lenp
     %endif
     %assign %%i %%i+1
 %endrep
-    mov         src0q, [src0q]
-    add         src0q, lenq
+    mov         src0p, [src0q]
+    add         src0p, lenp
     neg          lenq
 .loop:
 ; for x86-32 with 7-8 channels we do not have enough gp registers for all src
@@ -378,15 +377,17 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
 %assign %%i 1
 %rep (in_channels - 1)
     %if copy_src_from_stack
-        %define src_ptr src5q
+        %define src_ptrp src5p
+        %define src_ptrq src5q
     %else
-        %define src_ptr src %+ %%i %+ q
+        %define src_ptrp src %+ %%i %+ p
+        %define src_ptrq src %+ %%i %+ q
     %endif
     %if stereo
     %if copy_src_from_stack
-    mov       src_ptr, src %+ %%i %+ m
+    mov       src_ptrp, src %+ %%i %+ mp
     %endif
-    mova           m4, [src_ptr+lenq]
+    mova           m4, [src_ptrq+lenq]
     S16_TO_S32_SX   4, 5
     cvtdq2ps       m4, m4
     cvtdq2ps       m5, m5
@@ -396,9 +397,9 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
     FMULADD_PS     m1, m5, mx_0_ %+ %%i, m1, m5
     %else
     %if copy_src_from_stack
-    mov       src_ptr, src %+ %%i %+ m
+    mov       src_ptrp, src %+ %%i %+ mp
     %endif
-    mova           m2, [src_ptr+lenq]
+    mova           m2, [src_ptrq+lenq]
     S16_TO_S32_SX   2, 3
     cvtdq2ps       m2, m2
     cvtdq2ps       m3, m3
@@ -433,14 +434,16 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
 %assign %%i 1
 %rep (in_channels - 1)
     %if copy_src_from_stack
-        %define src_ptr src5q
-        mov   src_ptr, src %+ %%i %+ m
+        %define src_ptrp src5p
+        %define src_ptrq src5q
+        mov   src_ptrp, src %+ %%i %+ mp
     %else
-        %define src_ptr src %+ %%i %+ q
+        %define src_ptrp src %+ %%i %+ p
+        %define src_ptrq src %+ %%i %+ q
     %endif
     ; avoid extra load for mono if matrix is in a mm register
     %if stereo || mx_stack_0_ %+ %%i
-    mova           m2, [src_ptr+lenq]
+    mova           m2, [src_ptrq+lenq]
     %endif
     %if stereo
     FMULADD_PS     m1, m2, mx_1_ %+ %%i, m1, m3
@@ -448,7 +451,7 @@ cglobal mix_%1_to_%2_%3_flt, 3,in_channels+2,needed_mmregs+matrix_elements_mm, n
     %if stereo || mx_stack_0_ %+ %%i
     FMULADD_PS     m0, m2, mx_0_ %+ %%i, m0, m2
     %else
-    FMULADD_PS     m0, mx_0_ %+ %%i, [src_ptr+lenq], m0, m1
+    FMULADD_PS     m0, mx_0_ %+ %%i, [src_ptrq+lenq], m0, m1
     %endif
     %assign %%i %%i+1
 %endrep
diff --git a/libavresample/x86/dither.asm b/libavresample/x86/dither.asm
index d677c7179a..b505a07c98 100644
--- a/libavresample/x86/dither.asm
+++ b/libavresample/x86/dither.asm
@@ -35,11 +35,11 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 INIT_XMM sse2
-cglobal quantize, 4,4,3, dst, src, dither, len
-    lea         lenq, [2*lend]
-    add         dstq, lenq
-    lea         srcq, [srcq+2*lenq]
-    lea      ditherq, [ditherq+2*lenq]
+cglobal quantize, 4, 4, 3, "p", dst, "p", src, "d", dither, len
+    add         lend, lend
+    add         dstp, lenp
+    lea         srcp, [srcq+2*lenq]
+    lea      ditherp, [ditherq+2*lenq]
     neg         lenq
     mova          m2, [pf_s16_scale]
 .loop:
@@ -60,10 +60,10 @@ cglobal quantize, 4,4,3, dst, src, dither, len
 ;------------------------------------------------------------------------------
 
 %macro DITHER_INT_TO_FLOAT_RECTANGULAR 0
-cglobal dither_int_to_float_rectangular, 3,3,3, dst, src, len
-    lea         lenq, [4*lend]
-    add         srcq, lenq
-    add         dstq, lenq
+cglobal dither_int_to_float_rectangular, 3, 3, 3, "p", dst, "p", src, "d", len
+    lea         lend, [4*lend]
+    add         srcp, lenp
+    add         dstp, lenp
     neg         lenq
     mova          m0, [pf_dither_scale]
 .loop:
@@ -88,11 +88,11 @@ DITHER_INT_TO_FLOAT_RECTANGULAR
 ;------------------------------------------------------------------------------
 
 %macro DITHER_INT_TO_FLOAT_TRIANGULAR 0
-cglobal dither_int_to_float_triangular, 3,4,5, dst, src0, len, src1
-    lea         lenq, [4*lend]
-    lea        src1q, [src0q+2*lenq]
-    add        src0q, lenq
-    add         dstq, lenq
+cglobal dither_int_to_float_triangular, 3, 4, 5, "p", dst, "p", src0, "d", len, src1
+    lea         lend, [4*lend]
+    lea        src1p, [src0q+2*lenq]
+    add        src0p, lenp
+    add         dstp, lenp
     neg         lenq
     mova          m0, [pf_dither_scale]
 .loop:
-- 
2.21.0


From 627f96fa7a5483fbd94aa98c25e86df11f3c7372 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:06:38 +0200
Subject: [PATCH 16/17] x32: libavfilter
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavfilter/x86/af_afir.asm        |   8 +-
 libavfilter/x86/af_volume.asm      |  26 ++---
 libavfilter/x86/avf_showcqt.asm    |  18 ++--
 libavfilter/x86/colorspacedsp.asm  | 160 ++++++++++++++---------------
 libavfilter/x86/vf_blend.asm       |  24 ++---
 libavfilter/x86/vf_bwdif.asm       |  40 ++++----
 libavfilter/x86/vf_framerate.asm   |  24 ++---
 libavfilter/x86/vf_fspp.asm        |  18 ++--
 libavfilter/x86/vf_gradfun.asm     |  42 ++++----
 libavfilter/x86/vf_hflip.asm       |   2 +-
 libavfilter/x86/vf_hqdn3d.asm      |  16 +--
 libavfilter/x86/vf_idet.asm        |   6 +-
 libavfilter/x86/vf_interlace.asm   |   8 +-
 libavfilter/x86/vf_limiter.asm     |   5 +-
 libavfilter/x86/vf_maskedmerge.asm |   4 +-
 libavfilter/x86/vf_overlay.asm     |  11 +-
 libavfilter/x86/vf_pp7.asm         |   2 +-
 libavfilter/x86/vf_psnr.asm        |   6 +-
 libavfilter/x86/vf_pullup.asm      |   6 +-
 libavfilter/x86/vf_removegrain.asm |  42 ++++----
 libavfilter/x86/vf_ssim.asm        |   6 +-
 libavfilter/x86/vf_stereo3d.asm    |  20 ++--
 libavfilter/x86/vf_threshold.asm   |   8 +-
 libavfilter/x86/vf_w3fdif.asm      |  56 +++++-----
 libavfilter/x86/vf_yadif.asm       |   8 +-
 libavfilter/x86/yadif-10.asm       |   8 +-
 libavfilter/x86/yadif-16.asm       |   8 +-
 27 files changed, 290 insertions(+), 292 deletions(-)

diff --git a/libavfilter/x86/af_afir.asm b/libavfilter/x86/af_afir.asm
index 849d85e70f..1a9397663c 100644
--- a/libavfilter/x86/af_afir.asm
+++ b/libavfilter/x86/af_afir.asm
@@ -28,12 +28,12 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 INIT_XMM sse3
-cglobal fcmul_add, 4,4,6, sum, t, c, len
+cglobal fcmul_add, 4, 4, 6, "p", sum, "p", t, "p", c, "d", len
     shl       lend, 3
     add       lend, mmsize*2
-    add         tq, lenq
-    add         cq, lenq
-    add       sumq, lenq
+    add         tp, lenp
+    add         cp, lenp
+    add       sump, lenp
     neg       lenq
 ALIGN 16
 .loop:
diff --git a/libavfilter/x86/af_volume.asm b/libavfilter/x86/af_volume.asm
index 723ab1f8fb..44c0e79507 100644
--- a/libavfilter/x86/af_volume.asm
+++ b/libavfilter/x86/af_volume.asm
@@ -37,12 +37,12 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 INIT_XMM sse2
-cglobal scale_samples_s16, 4,4,4, dst, src, len, volume
-    movd        m0, volumem
+cglobal scale_samples_s16, 4, 4, 4, "p", dst, "p", src, "d", len, "d*", volume
+    movd        m0, volumemd
     pshuflw     m0, m0, 0
     punpcklwd   m0, [pw_1]
     mova        m1, [pw_128]
-    lea       lenq, [lend*2-mmsize]
+    lea       lend, [lend*2-mmsize]
 .loop:
     ; dst[i] = av_clip_int16((src[i] * volume + 128) >> 8);
     mova        m2, [srcq+lenq]
@@ -54,7 +54,7 @@ cglobal scale_samples_s16, 4,4,4, dst, src, len, volume
     psrad       m2, 8
     packssdw    m3, m2
     mova  [dstq+lenq], m3
-    sub       lenq, mmsize
+    sub       lend, mmsize
     jge .loop
     REP_RET
 
@@ -64,17 +64,17 @@ cglobal scale_samples_s16, 4,4,4, dst, src, len, volume
 ;------------------------------------------------------------------------------
 
 %macro SCALE_SAMPLES_S32 0
-cglobal scale_samples_s32, 4,4,4, dst, src, len, volume
+cglobal scale_samples_s32, 4, 4, 4, "p", dst, "p", src, "d", len, "d*", volume
 %if ARCH_X86_32 && cpuflag(avx)
-    vbroadcastss   xmm2, volumem
+    vbroadcastss   xmm2, volumemd
 %else
-    movd           xmm2, volumed
+    movd           xmm2, volumemd
     pshufd         xmm2, xmm2, 0
 %endif
     CVTDQ2PD         m2, xmm2
     mulpd            m2, m2, [pd_1_256]
     mova             m3, [pd_int32_max]
-    lea            lenq, [lend*4-mmsize]
+    lea            lend, [lend*4-mmsize]
 .loop:
     CVTDQ2PD         m0, [srcq+lenq         ]
     CVTDQ2PD         m1, [srcq+lenq+mmsize/2]
@@ -91,7 +91,7 @@ cglobal scale_samples_s32, 4,4,4, dst, src, len, volume
     movq    [dstq+lenq         ], xmm0
     movq    [dstq+lenq+mmsize/2], xmm1
 %endif
-    sub            lenq, mmsize
+    sub            lend, mmsize
     jge .loop
     REP_RET
 %endmacro
@@ -110,12 +110,12 @@ SCALE_SAMPLES_S32
 ;       [-INT_MAX, INT_MAX] instead of [INT_MIN, INT_MAX]
 
 INIT_XMM ssse3, atom
-cglobal scale_samples_s32, 4,4,8, dst, src, len, volume
-    movd        m4, volumem
+cglobal scale_samples_s32, 4, 4, 8, "p", dst, "p", src, "d", len, "d*", volume
+    movd        m4, volumemd
     pshufd      m4, m4, 0
     mova        m5, [pq_128]
     pxor        m6, m6
-    lea       lenq, [lend*4-mmsize]
+    lea       lend, [lend*4-mmsize]
 .loop:
     ; src[i] = av_clipl_int32((src[i] * volume + 128) >> 8);
     mova        m7, [srcq+lenq]
@@ -135,6 +135,6 @@ cglobal scale_samples_s32, 4,4,8, dst, src, len, volume
     psrld       m0, 1
     psignd      m0, m7
     mova  [dstq+lenq], m0
-    sub       lenq, mmsize
+    sub       lend, mmsize
     jge .loop
     REP_RET
diff --git a/libavfilter/x86/avf_showcqt.asm b/libavfilter/x86/avf_showcqt.asm
index 63e58408cd..a6f3935c2e 100644
--- a/libavfilter/x86/avf_showcqt.asm
+++ b/libavfilter/x86/avf_showcqt.asm
@@ -22,14 +22,8 @@
 
 %include "libavutil/x86/x86util.asm"
 
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
 struc Coeffs
-    .val:   pointer 1
+    .val:   resp 1
     .start: resd 1
     .len:   resd 1
     .sizeof:
@@ -77,7 +71,7 @@ endstruc
 %macro DECLARE_CQT_CALC 0
 ; ff_showcqt_cqt_calc_*(dst, src, coeffs, len, fft_len)
 %if ARCH_X86_64
-cglobal showcqt_cqt_calc, 5, 10, 12, dst, src, coeffs, len, fft_len, x, coeffs_val, coeffs_val2, i, coeffs_len
+cglobal showcqt_cqt_calc, 5, 10, 12, "p", dst, "p", src, "p", coeffs, "d", len, "d", fft_len, x, coeffs_val, coeffs_val2, i, coeffs_len
     align   16
     .loop_k:
         mov     xd, [coeffsq + Coeffs.len]
@@ -95,8 +89,8 @@ cglobal showcqt_cqt_calc, 5, 10, 12, dst, src, coeffs, len, fft_len, x, coeffs_v
         xor     xd, xd
         test    coeffs_lend, coeffs_lend
         jz      .check_loop_b
-        mov     coeffs_valq, [coeffsq + Coeffs.val]
-        mov     coeffs_val2q, [coeffsq + Coeffs.val + Coeffs.sizeof]
+        mov     coeffs_valp, [coeffsq + Coeffs.val]
+        mov     coeffs_val2p, [coeffsq + Coeffs.val + Coeffs.sizeof]
         align   16
         .loop_ab:
             movaps  m7, [coeffs_valq + 4 * xq]
@@ -141,7 +135,7 @@ cglobal showcqt_cqt_calc, 5, 10, 12, dst, src, coeffs, len, fft_len, x, coeffs_v
             jb      .loop_a
         jmp     .loop_end
 %else
-cglobal showcqt_cqt_calc, 4, 7, 8, dst, src, coeffs, len, x, coeffs_val, i
+cglobal showcqt_cqt_calc, 4, 7, 8, "p", dst, "p", src, "p", coeffs, "d", len, x, coeffs_val, i
 %define fft_lend r4m
     align   16
     .loop_k:
@@ -152,7 +146,7 @@ cglobal showcqt_cqt_calc, 4, 7, 8, dst, src, coeffs, len, x, coeffs_val, i
         movaps  m3, m0
         test    xd, xd
         jz      .store
-        mov     coeffs_valq, [coeffsq + Coeffs.val]
+        mov     coeffs_valp, [coeffsq + Coeffs.val]
         xor     xd, xd
         align   16
         .loop_x:
diff --git a/libavfilter/x86/colorspacedsp.asm b/libavfilter/x86/colorspacedsp.asm
index 67d851abf4..1bf9a7c4fd 100644
--- a/libavfilter/x86/colorspacedsp.asm
+++ b/libavfilter/x86/colorspacedsp.asm
@@ -85,7 +85,7 @@ SECTION .text
 %assign %%ypmul (1 << %%ypsh)
 
 cglobal yuv2yuv_ %+ %%ss %+ p%1to%2, 8, 14, 16, 0 - (4 * mmsize), \
-                                     yo, yos, yi, yis, w, h, c, yoff, ui, vi, uo, vo
+                                     "p", yo, "p", yos, "p", yi, "p", yis, "d", w, "d", h, "p", c, "p", yoff, ui, vi, uo, vo
 %if %3 == 1
     inc             wd
     sar             wd, 1
@@ -124,27 +124,27 @@ cglobal yuv2yuv_ %+ %%ss %+ p%1to%2, 8, 14, 16, 0 - (4 * mmsize), \
     mova [rsp+1*mmsize], m2
     mova [rsp+2*mmsize], m4
 
-    DEFINE_ARGS yo, yos, yi, yis, ui, vi, uo, vo, uis, vis, uos, vos, x, tmp
-
-    mov            uiq, [yiq+gprsize*1]
-    mov            viq, [yiq+gprsize*2]
-    mov            yiq, [yiq+gprsize*0]
-    mov            uoq, [yoq+gprsize*1]
-    mov            voq, [yoq+gprsize*2]
-    mov            yoq, [yoq+gprsize*0]
-    mov           uisq, [yisq+gprsize*1]
-    mov           visq, [yisq+gprsize*2]
-    mov           yisq, [yisq+gprsize*0]
-    mov           uosq, [yosq+gprsize*1]
-    mov           vosq, [yosq+gprsize*2]
-    mov           yosq, [yosq+gprsize*0]
+    DEFINE_ARGS "p", yo, "p", yos, "p", yi, "p", yis, ui, vi, uo, vo, uis, vis, uos, vos, x, tmp
+
+    mov            uip, [yiq+ptrsize*1]
+    mov            vip, [yiq+ptrsize*2]
+    mov            yip, [yiq+ptrsize*0]
+    mov            uop, [yoq+ptrsize*1]
+    mov            vop, [yoq+ptrsize*2]
+    mov            yop, [yoq+ptrsize*0]
+    mov           uisp, [yisq+ptrsize*1]
+    mov           visp, [yisq+ptrsize*2]
+    mov           yisp, [yisq+ptrsize*0]
+    mov           uosp, [yosq+ptrsize*1]
+    mov           vosp, [yosq+ptrsize*2]
+    mov           yosp, [yosq+ptrsize*0]
 
 .loop_v:
-    xor             xq, xq
+    xor             xd, xd
 
 .loop_h:
 %if %4 == 1
-    lea           tmpq, [yiq+yisq]
+    lea           tmpp, [yiq+yisq]
 %endif ; %4 == 1
 %if %1 == 8
     movu            m0, [yiq+xq*(1<<%3)]        ; y00/01
@@ -301,7 +301,7 @@ cglobal yuv2yuv_ %+ %%ss %+ p%1to%2, 8, 14, 16, 0 - (4 * mmsize), \
     packssdw        m2, m6
     packssdw        m3, m7
 
-    lea           tmpq, [yoq+yosq]
+    lea           tmpp, [yoq+yosq]
 %if %2 == 8
     packuswb        m2, m3
     movu   [tmpq+xq*2], m2
@@ -342,21 +342,21 @@ cglobal yuv2yuv_ %+ %%ss %+ p%1to%2, 8, 14, 16, 0 - (4 * mmsize), \
     movu [yoq+xq*(2<<%3)+mmsize], m1
 %endif ; %2 ==/!= 8
 
-    add             xq, mmsize >> %3
+    add             xp, mmsize >> %3
     cmp             xd, dword [rsp+3*mmsize+0]
     jl .loop_h
 
 %if %4 == 1
-    lea            yiq, [yiq+yisq*2]
-    lea            yoq, [yoq+yosq*2]
+    lea            yip, [yiq+yisq*2]
+    lea            yop, [yoq+yosq*2]
 %else ; %4 != 1
-    add            yiq, yisq
-    add            yoq, yosq
+    add            yip, yisp
+    add            yop, yosp
 %endif ; %4 ==/!= 1
-    add            uiq, uisq
-    add            viq, visq
-    add            uoq, uosq
-    add            voq, vosq
+    add            uip, uisp
+    add            vip, visp
+    add            uop, uosp
+    add            vop, vosp
     dec dword [rsp+3*mmsize+4]
     jg .loop_v
 
@@ -397,7 +397,7 @@ YUV2YUV_FNS 1, 1
 %endif ; %2/%3
 
 cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
-                                rgb, rgbs, yuv, yuvs, ww, h, c, yoff
+                                "p", rgb, "p", rgbs, "p", yuv, "p", yuvs, "d", ww, "d", h, "p", c, "p", yoff
 %if %2 == 1
     inc            wwd
     sar            wwd, 1
@@ -423,24 +423,24 @@ cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
     mova [rsp+3*mmsize], m14
     pxor           m14, m14
 
-    DEFINE_ARGS r, rgbs, y, ys, ww, h, g, b, u, v, us, vs, x, tmp
+    DEFINE_ARGS "p", r, "p", rgbs, "p", y, "p", ys, "d", ww, "d", h, g, b, u, v, us, vs, x, tmp
 
-    mov             gq, [rq+1*gprsize]
-    mov             bq, [rq+2*gprsize]
-    mov             rq, [rq+0*gprsize]
-    mov             uq, [yq+1*gprsize]
-    mov             vq, [yq+2*gprsize]
-    mov             yq, [yq+0*gprsize]
-    mov            usq, [ysq+1*gprsize]
-    mov            vsq, [ysq+2*gprsize]
-    mov            ysq, [ysq+0*gprsize]
+    mov             gp, [rq+1*ptrsize]
+    mov             bp, [rq+2*ptrsize]
+    mov             rp, [rq+0*ptrsize]
+    mov             up, [yq+1*ptrsize]
+    mov             vp, [yq+2*ptrsize]
+    mov             yp, [yq+0*ptrsize]
+    mov            usp, [ysq+1*ptrsize]
+    mov            vsp, [ysq+2*ptrsize]
+    mov            ysp, [ysq+0*ptrsize]
 
 .loop_v:
-    xor             xq, xq
+    xor             xd, xd
 
 .loop_h:
 %if %3 == 1
-    lea           tmpq, [yq+ysq]
+    lea           tmpp, [yq+ysq]
 %endif ; %3 == 1
 %if %1 == 8
     movu            m0, [yq+xq*(1<<%2)]
@@ -553,7 +553,7 @@ cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
     psrad          m12, %%sh
     psrad          m13, %%sh
 %if %3 == 1
-    lea           tmpq, [rq+rgbsq*2]
+    lea           tmpp, [rq+rgbsq*2]
     packssdw        m2, m3
     packssdw        m8, m9
     mova [tmpq+xq*4], m2
@@ -598,7 +598,7 @@ cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
     psrad          m12, %%sh
     psrad          m13, %%sh
 %if %3 == 1
-    lea           tmpq, [gq+rgbsq*2]
+    lea           tmpp, [gq+rgbsq*2]
     packssdw        m2, m3
     packssdw        m8, m9
     mova [tmpq+xq*4], m2
@@ -647,7 +647,7 @@ cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
     movu   [bq+xq*(2 << %2)], m0
     movu   [bq+xq*(2 << %2)+mmsize], m1
 %if %3 == 1
-    lea           tmpq, [bq+rgbsq*2]
+    lea           tmpp, [bq+rgbsq*2]
     packssdw        m4, m2
     packssdw        m5, m3
     movu [tmpq+xq*4], m4
@@ -658,16 +658,16 @@ cglobal yuv2rgb_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 8 * mmsize, \
     cmp             xd, wwd
     jl .loop_h
 
-    lea             rq, [rq+rgbsq*(2 << %3)]
-    lea             gq, [gq+rgbsq*(2 << %3)]
-    lea             bq, [bq+rgbsq*(2 << %3)]
+    lea             rp, [rq+rgbsq*(2 << %3)]
+    lea             gp, [gq+rgbsq*(2 << %3)]
+    lea             bp, [bq+rgbsq*(2 << %3)]
 %if %3 == 1
-    lea             yq, [yq+ysq*2]
+    lea             yp, [yq+ysq*2]
 %else ; %3 != 0
-    add             yq, ysq
+    add             yp, ysp
 %endif ; %3 ==/!= 1
-    add             uq, usq
-    add             vq, vsq
+    add             up, usp
+    add             vp, vsp
     dec             hd
     jg .loop_v
 
@@ -701,7 +701,7 @@ YUV2RGB_FNS 1, 1
 %endif ; %2/%3
 
 cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
-                                 yuv, yuvs, rgb, rgbs, ww, h, c, off
+                                 "p", yuv, "p", yuvs, "p", rgb, "p", rgbs, "d", ww, "d", h, "p", c, "p", off
 %if %2 == 1
     inc            wwd
     sar            wwd, 1
@@ -740,16 +740,16 @@ cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
     mova [rsp+5*mmsize], m7                 ; cbv, uvoff + rnd
 
 
-    DEFINE_ARGS y, ys, r, rgbs, ww, h, u, v, us, vs, g, b, tmp, x
-    mov             gq, [rq+gprsize*1]
-    mov             bq, [rq+gprsize*2]
-    mov             rq, [rq+gprsize*0]
-    mov             uq, [yq+gprsize*1]
-    mov             vq, [yq+gprsize*2]
-    mov             yq, [yq+gprsize*0]
-    mov            usq, [ysq+gprsize*1]
-    mov            vsq, [ysq+gprsize*2]
-    mov            ysq, [ysq+gprsize*0]
+    DEFINE_ARGS "p", y, "p", ys, "p", r, "p", rgbs, "d", ww, "d", h, u, v, us, vs, g, b, tmp, x
+    mov             gp, [rq+ptrsize*1]
+    mov             bp, [rq+ptrsize*2]
+    mov             rp, [rq+ptrsize*0]
+    mov             up, [yq+ptrsize*1]
+    mov             vp, [yq+ptrsize*2]
+    mov             yp, [yq+ptrsize*0]
+    mov            usp, [ysq+ptrsize*1]
+    mov            vsp, [ysq+ptrsize*2]
+    mov            ysp, [ysq+ptrsize*0]
 
     pxor           m15, m15
 .loop_v:
@@ -812,7 +812,7 @@ cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
 
 %if %3 == 1
     ; bottom line y, r/g portion only
-    lea           tmpq, [rgbsq+xq*2]
+    lea           tmpp, [rgbsq+xq*2]
     mova            m6, [rq+tmpq*2]
     mova            m9, [rq+tmpq*2+mmsize]
     mova            m7, [gq+tmpq*2]
@@ -859,7 +859,7 @@ cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
     psrad          m15, %%sh
     packssdw       m12, m13
     packssdw       m14, m15
-    lea           tmpq, [yq+ysq]
+    lea           tmpp, [yq+ysq]
 %if %1 == 8
     packuswb       m12, m14
     movu   [tmpq+xq*2], m12
@@ -983,20 +983,20 @@ cglobal rgb2yuv_ %+ %%ss %+ p%1, 8, 14, 16, 0 - 6 * mmsize, \
 %endif
 %endif ; %2 ==/!= 1
 
-    add             xq, mmsize >> %2
+    add             xd, mmsize >> %2
     cmp             xd, wwd
     jl .loop_h
 
 %if %3 == 0
-    add             yq, ysq
+    add             yp, ysp
 %else ; %3 != 0
-    lea             yq, [yq+ysq*2]
+    lea             yp, [yq+ysq*2]
 %endif ; %3 ==/!= 0
-    add             uq, usq
-    add             vq, vsq
-    lea             rq, [rq+rgbsq*(2<<%3)]
-    lea             gq, [gq+rgbsq*(2<<%3)]
-    lea             bq, [bq+rgbsq*(2<<%3)]
+    add             up, usp
+    add             vp, vsp
+    lea             rp, [rq+rgbsq*(2<<%3)]
+    lea             gp, [gq+rgbsq*(2<<%3)]
+    lea             bp, [bq+rgbsq*(2<<%3)]
     dec             hd
     jg .loop_v
 
@@ -1017,7 +1017,7 @@ RGB2YUV_FNS 1, 1
 ; void ff_multiply3x3_sse2(int16_t *data[3], ptrdiff_t stride,
 ;                          int w, int h, const int16_t coeff[3][3][8])
 INIT_XMM sse2
-cglobal multiply3x3, 5, 7, 16, data, stride, ww, h, c
+cglobal multiply3x3, 5, 7, 16, "p", data, "p", stride, "d", ww, "d", h, "p", c
     movh            m0, [cq+  0]
     movh            m1, [cq+ 32]
     movh            m2, [cq+ 48]
@@ -1031,11 +1031,11 @@ cglobal multiply3x3, 5, 7, 16, data, stride, ww, h, c
     punpcklwd       m4, [cq+112]
     punpcklwd       m5, [pw_8192]
 
-    DEFINE_ARGS data0, stride, ww, h, data1, data2, x
-    shl        strideq, 1
-    mov         data1q, [data0q+gprsize*1]
-    mov         data2q, [data0q+gprsize*2]
-    mov         data0q, [data0q+gprsize*0]
+    DEFINE_ARGS "p", data0, "p", stride, "d", ww, "d", h, data1, data2, x
+    shl        stridep, 1
+    mov         data1p, [data0q+ptrsize*1]
+    mov         data2p, [data0q+ptrsize*2]
+    mov         data0p, [data0q+ptrsize*0]
 
 .loop_v:
     xor             xd, xd
@@ -1087,9 +1087,9 @@ cglobal multiply3x3, 5, 7, 16, data, stride, ww, h, c
     cmp             xd, wwd
     jl .loop_h
 
-    add         data0q, strideq
-    add         data1q, strideq
-    add         data2q, strideq
+    add         data0p, stridep
+    add         data1p, stridep
+    add         data2p, stridep
     dec             hd
     jg .loop_v
 
diff --git a/libavfilter/x86/vf_blend.asm b/libavfilter/x86/vf_blend.asm
index 251bbb5a12..1269d5c663 100644
--- a/libavfilter/x86/vf_blend.asm
+++ b/libavfilter/x86/vf_blend.asm
@@ -40,27 +40,27 @@ SECTION .text
 
 %macro BLEND_INIT 2-3
 %if ARCH_X86_64
-cglobal blend_%1, 6, 9, %2, top, top_linesize, bottom, bottom_linesize, dst, dst_linesize, width, end, x
-    mov    widthd, dword widthm
+cglobal blend_%1, 7, 9, %2, "p", top, "p", top_linesize, "p", bottom, "p", bottom_linesize, "p", dst, "p", dst_linesize, "p", width, end, x
     %if %0 == 3; is 16 bit
-        add    widthq, widthq ; doesn't compile on x86_32
+        add    widthp, widthp ; doesn't compile on x86_32
     %endif
 %else
-cglobal blend_%1, 5, 7, %2, top, top_linesize, bottom, bottom_linesize, dst, end, x
-%define dst_linesizeq r5mp
+cglobal blend_%1, 5, 7, %2, "p", top, "p", top_linesize, "p", bottom, "p", bottom_linesize, "p", dst, end, x
+%define dst_linesizep r5mp
+%define widthp r6mp
 %define widthq r6mp
 %endif
-    mov      endd, dword r7m
-    add      topq, widthq
-    add   bottomq, widthq
-    add      dstq, widthq
+    mov      endd, r7md
+    add      topp, widthp
+    add   bottomp, widthp
+    add      dstp, widthp
     neg    widthq
 %endmacro
 
 %macro BLEND_END 0
-    add          topq, top_linesizeq
-    add       bottomq, bottom_linesizeq
-    add          dstq, dst_linesizeq
+    add          topp, top_linesizep
+    add       bottomp, bottom_linesizep
+    add          dstp, dst_linesizep
     sub          endd, 1
     jg .nextrow
 REP_RET
diff --git a/libavfilter/x86/vf_bwdif.asm b/libavfilter/x86/vf_bwdif.asm
index 147b7c6ac6..28ba3e0824 100644
--- a/libavfilter/x86/vf_bwdif.asm
+++ b/libavfilter/x86/vf_bwdif.asm
@@ -192,11 +192,11 @@ SECTION .text
     pxor         m7, m7
     DISP%4
 
-    add        dstq, STEP
-    add       prevq, STEP
-    add        curq, STEP
-    add       nextq, STEP
-    sub    DWORD wm, mmsize/2
+    add        dstp, STEP
+    add       prevp, STEP
+    add        curp, STEP
+    add       nextp, STEP
+    sub         wmd, mmsize/2
     jg .loop%1
 %endmacro
 
@@ -228,31 +228,29 @@ SECTION .text
 
 %macro BWDIF 0
 %if ARCH_X86_64
-cglobal bwdif_filter_line, 4, 9, 12, 0, dst, prev, cur, next, w, prefs, \
-                                        mrefs, prefs2, mrefs2, prefs3, mrefs3, \
-                                        prefs4, mrefs4, parity, clip_max
+cglobal bwdif_filter_line, 4, 9, 12, 0, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                        "d", mrefs, "d", prefs2, "d", mrefs2, "d", prefs3, "d", mrefs3, \
+                                        "d", prefs4, "d", mrefs4, "d", parity, "d", clip_max
 %else
-cglobal bwdif_filter_line, 4, 6, 8, 64, dst, prev, cur, next, w, prefs, \
-                                        mrefs, prefs2, mrefs2, prefs3, mrefs3, \
-                                        prefs4, mrefs4, parity, clip_max
+cglobal bwdif_filter_line, 4, 6, 8, 64, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                        "d", mrefs, "d", prefs2, "d", mrefs2, "d", prefs3, "d", mrefs3, \
+                                        "d", prefs4, "d", mrefs4, "d", parity, "d", clip_max
 %endif
     %define STEP mmsize/2
     PROC 8, 1
 
 %if ARCH_X86_64
-cglobal bwdif_filter_line_12bit, 4, 9, 13, 0, dst, prev, cur, next, w, \
-                                              prefs, mrefs, prefs2, mrefs2, \
-                                              prefs3, mrefs3, prefs4, \
-                                              mrefs4, parity, clip_max
-    movd        m12, DWORD clip_maxm
+cglobal bwdif_filter_line_12bit, 4, 9, 13, 0, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                              "d", mrefs, "d", prefs2, "d", mrefs2, "d", prefs3, "d", mrefs3, \
+                                              "d", prefs4, "d", mrefs4, "d", parity, "d", clip_max
+    movd        m12, clip_maxmd
     SPLATW      m12, m12, 0
 %else
-cglobal bwdif_filter_line_12bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, prefs2, mrefs2, \
-                                              prefs3, mrefs3, prefs4, \
-                                              mrefs4, parity, clip_max
+cglobal bwdif_filter_line_12bit, 4, 6, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                              "d", mrefs, "d", prefs2, "d", mrefs2, "d", prefs3, "d", mrefs3, \
+                                              "d", prefs4, "d", mrefs4, "d", parity, "d", clip_max
     %define m12 [rsp+64]
-    movd         m0, DWORD clip_maxm
+    movd         m0, clip_maxmd
     SPLATW       m0, m0, 0
     mova        m12, m0
 %endif
diff --git a/libavfilter/x86/vf_framerate.asm b/libavfilter/x86/vf_framerate.asm
index 7a30c870bd..31afe7ab8d 100644
--- a/libavfilter/x86/vf_framerate.asm
+++ b/libavfilter/x86/vf_framerate.asm
@@ -43,17 +43,17 @@ SECTION .text
 
 %macro BLEND_INIT 0-1
 %if ARCH_X86_64
-cglobal blend_frames%1, 6, 9, 5, src1, src1_linesize, src2, src2_linesize, dst, dst_linesize, width, end, x
-    mov    widthd, dword widthm
+cglobal blend_frames%1, 7, 9, 5, "p", src1, "p", src1_linesize, "p", src2, "p", src2_linesize, "p", dst, "p", dst_linesize, "p", width, end, x
 %else
-cglobal blend_frames%1, 5, 7, 5, src1, src1_linesize, src2, src2_linesize, dst, end, x
-%define dst_linesizeq r5mp
-%define widthq r6mp
+cglobal blend_frames%1, 5, 7, 5, "p", src1, "p", src1_linesize, "p", src2, "p", src2_linesize, "p", dst, end, x
+%define dst_linesizep r5mp
+%define widthp r6mp
+%define widthq r6mq
 %endif
-    mov      endd, dword r7m
-    add     src1q, widthq
-    add     src2q, widthq
-    add      dstq, widthq
+    mov      endd, r7md
+    add     src1p, widthp
+    add     src2p, widthp
+    add      dstp, widthp
     neg    widthq
 %endmacro
 
@@ -79,9 +79,9 @@ cglobal blend_frames%1, 5, 7, 5, src1, src1_linesize, src2, src2_linesize, dst,
         movu   [dstq + xq], m0
         add             xq, mmsize
     jl .loop
-    add     src1q, src1_linesizeq
-    add     src2q, src2_linesizeq
-    add      dstq, dst_linesizeq
+    add     src1p, src1_linesizep
+    add     src2p, src2_linesizep
+    add      dstp, dst_linesizep
     sub      endd, 1
     jg .nextrow
 REP_RET
diff --git a/libavfilter/x86/vf_fspp.asm b/libavfilter/x86/vf_fspp.asm
index c7f8f64f1b..0405863de4 100644
--- a/libavfilter/x86/vf_fspp.asm
+++ b/libavfilter/x86/vf_fspp.asm
@@ -52,9 +52,9 @@ INIT_MMX mmx
 ;                        ptrdiff_t dst_stride, ptrdiff_t src_stride,
 ;                        ptrdiff_t width, ptrdiff_t height, ptrdiff_t log2_scale)
 %if ARCH_X86_64
-cglobal store_slice, 7, 9, 0, dst, src, dst_stride, src_stride, width, dither_height, dither, tmp, tmp2
+cglobal store_slice, 7, 9, 0, "p", dst, "p", src, "p-", dst_stride, "p-", src_stride, "p-", width, "p-", dither_height, "p-", dither, tmp, tmp2
 %else
-cglobal store_slice, 2, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
+cglobal store_slice, 2, 7, 0, "p", dst, "p", src, width, dither_height, dither, tmp, tmp2
 %define dst_strideq r2m
 %define src_strideq r3m
     mov       widthq, r4m
@@ -117,13 +117,11 @@ cglobal store_slice, 2, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
 ;                         ptrdiff_t dst_stride, ptrdiff_t src_stride,
 ;                         ptrdiff_t width, ptrdiff_t height, ptrdiff_t log2_scale)
 %if ARCH_X86_64
-cglobal store_slice2, 7, 9, 0, dst, src, dst_stride, src_stride, width, dither_height, dither, tmp, tmp2
+cglobal store_slice2, 7, 9, 0, "p", dst, "p", src, "p-", dst_stride, "p-", src_stride, "p-", width, "p-", dither_height, "p-", dither, tmp, tmp2
 %else
-cglobal store_slice2, 0, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
+cglobal store_slice2, 2, 7, 0, "p", dst, "p", src, width, dither_height, dither, tmp, tmp2
 %define dst_strideq r2m
 %define src_strideq r3m
-    mov       dstq, dstm
-    mov       srcq, srcm
     mov       widthq, r4m
     mov       dither_heightq, r5m
     mov       ditherq, r6m ; log2_scale
@@ -181,7 +179,7 @@ cglobal store_slice2, 0, 7, 0, dst, src, width, dither_height, dither, tmp, tmp2
     RET
 
 ;void ff_mul_thrmat_mmx(int16_t *thr_adr_noq, int16_t *thr_adr, int q);
-cglobal mul_thrmat, 3, 3, 0, thrn, thr, q
+cglobal mul_thrmat, 3, 3, 0, "p", thrn, "p", thr, "d", q
     movd      m7, qd
     movq      m0, [thrnq]
     punpcklwd m7, m7
@@ -461,7 +459,7 @@ cglobal mul_thrmat, 3, 3, 0, thrn, thr, q
 %endmacro
 
 ;void ff_column_fidct_mmx(int16_t *thr_adr, int16_t *data, int16_t *output, int cnt);
-cglobal column_fidct, 4, 5, 0, 32, thr, src, out, cnt, tmp
+cglobal column_fidct, 4, 5, 0, 32, "p", thr, "p", src, "p", out, "d", cnt, tmp
 .fdct1:
     COLUMN_FDCT .idct1
     jmp .fdct2
@@ -482,7 +480,7 @@ cglobal column_fidct, 4, 5, 0, 32, thr, src, out, cnt, tmp
     RET
 
 ;void ff_row_idct_mmx(int16_t *workspace, int16_t *output_adr, ptrdiff_t output_stride, int cnt);
-cglobal row_idct, 4, 5, 0, 16, src, dst, stride, cnt, stride3
+cglobal row_idct, 4, 5, 0, 16, "p", src, "p", dst, "p-", stride, "d", cnt, stride3
     add       strideq, strideq
     lea       stride3q, [strideq+strideq*2]
 .loop:
@@ -612,7 +610,7 @@ cglobal row_idct, 4, 5, 0, 16, src, dst, stride, cnt, stride3
     RET
 
 ;void ff_row_fdct_mmx(int16_t *data, const uint8_t *pixels, ptrdiff_t line_size, int cnt);
-cglobal row_fdct, 4, 5, 0, 16, src, pix, stride, cnt, stride3
+cglobal row_fdct, 4, 5, 0, 16, "p", src, "p", pix, "p-", stride, "d", cnt, stride3
     lea       stride3q, [strideq+strideq*2]
 .loop:
     movd      m0, [pixq]
diff --git a/libavfilter/x86/vf_gradfun.asm b/libavfilter/x86/vf_gradfun.asm
index 3581f89fe8..27ff951750 100644
--- a/libavfilter/x86/vf_gradfun.asm
+++ b/libavfilter/x86/vf_gradfun.asm
@@ -28,8 +28,8 @@ pw_ff: times 8 dw 0xFF
 SECTION .text
 
 %macro FILTER_LINE 1
-    movh       m0, [r2+r0]
-    movh       m1, [r3+r0]
+    movh       m0, [srcq+xq]
+    movh       m1, [dcq+xq]
     punpcklbw  m0, m7
     punpcklwd  m1, m1
     psllw      m0, 7
@@ -45,47 +45,47 @@ SECTION .text
     paddw      m0, m1
     psraw      m0, 7
     packuswb   m0, m0
-    movh  [r1+r0], m0
+    movh  [dstq+xq], m0
 %endmacro
 
 INIT_MMX mmxext
-cglobal gradfun_filter_line, 6, 6
-    movh      m5, r4d
+cglobal gradfun_filter_line, 6, 6, "p-", x, "p", dst, "p", src, "p", dc, "d", thresh, "p", dithers
+    movh      m5, threshd
     pxor      m7, m7
     pshufw    m5, m5,0
     mova      m6, [pw_7f]
-    mova      m3, [r5]
-    mova      m4, [r5+8]
+    mova      m3, [dithersq]
+    mova      m4, [dithersq+8]
 .loop:
     FILTER_LINE m3
-    add       r0, 4
+    add       xq, 4
     jge .end
     FILTER_LINE m4
-    add       r0, 4
+    add       xq, 4
     jl .loop
 .end:
     REP_RET
 
 INIT_XMM ssse3
-cglobal gradfun_filter_line, 6, 6, 8
-    movd       m5, r4d
+cglobal gradfun_filter_line, 6, 6, 8, "p-", x, "p", dst, "p", src, "p", dc, "d", thresh, "p", dithers
+    movd       m5, threshd
     pxor       m7, m7
     pshuflw    m5, m5, 0
     mova       m6, [pw_7f]
     punpcklqdq m5, m5
-    mova       m4, [r5]
+    mova       m4, [dithersq]
 .loop:
     FILTER_LINE m4
-    add        r0, 8
+    add        xq, 8
     jl .loop
     REP_RET
 
 %macro BLUR_LINE 1
-cglobal gradfun_blur_line_%1, 6, 6, 8
+cglobal gradfun_blur_line_%1, 6, 6, 8, "p-", x, "p", buf, "p", buf1, "p", dc, "p", src1, "p", src2
     mova        m7, [pw_ff]
 .loop:
-    %1          m0, [r4+r0]
-    %1          m1, [r5+r0]
+    %1          m0, [src1q+xq]
+    %1          m1, [src2q+xq]
     mova        m2, m0
     mova        m3, m1
     psrlw       m0, 8
@@ -95,12 +95,12 @@ cglobal gradfun_blur_line_%1, 6, 6, 8
     paddw       m0, m1
     paddw       m2, m3
     paddw       m0, m2
-    paddw       m0, [r2+r0]
-    mova        m1, [r1+r0]
-    mova   [r1+r0], m0
+    paddw       m0, [buf1q+xq]
+    mova        m1, [bufq+xq]
+    mova   [bufq+xq], m0
     psubw       m0, m1
-    mova   [r3+r0], m0
-    add         r0, 16
+    mova   [dcq+xq], m0
+    add         xq, 16
     jl .loop
     REP_RET
 %endmacro
diff --git a/libavfilter/x86/vf_hflip.asm b/libavfilter/x86/vf_hflip.asm
index 285618954f..7c530cf8f2 100644
--- a/libavfilter/x86/vf_hflip.asm
+++ b/libavfilter/x86/vf_hflip.asm
@@ -31,7 +31,7 @@ SECTION .text
 
 ;%1 byte or short, %2 b or w, %3 size in byte (1 for byte, 2 for short)
 %macro HFLIP 3
-cglobal hflip_%1, 3, 5, 3, src, dst, w, r, x
+cglobal hflip_%1, 3, 5, 3, "p", src, "p", dst, "d", w, r, x
     VBROADCASTI128    m0, [pb_flip_%1]
     xor               xq, xq
 %if %3 == 1
diff --git a/libavfilter/x86/vf_hqdn3d.asm b/libavfilter/x86/vf_hqdn3d.asm
index e3b1bdca53..3a905bdbda 100644
--- a/libavfilter/x86/vf_hqdn3d.asm
+++ b/libavfilter/x86/vf_hqdn3d.asm
@@ -45,9 +45,9 @@ SECTION .text
 
 %macro HQDN3D_ROW 1 ; bitdepth
 %if ARCH_X86_64
-cglobal hqdn3d_row_%1_x86, 7,10,0, src, dst, lineant, frameant, width, spatial, temporal, pixelant, t0, t1
+cglobal hqdn3d_row_%1_x86, 7, 10, 0, "p", src, "p", dst, "p", lineant, "p", frameant, "p-", width, "p", spatial, "p", temporal, pixelant, t0, t1
 %else
-cglobal hqdn3d_row_%1_x86, 7,7,0, src, dst, lineant, frameant, width, spatial, temporal
+cglobal hqdn3d_row_%1_x86, 7, 7, 0, "p", src, "p", dst, "p", lineant, "p", frameant, "p-", width, "p", spatial, "p", temporal
 %endif
     %assign bytedepth (%1+7)>>3
     %assign lut_bits 4+4*(%1/16)
@@ -67,25 +67,29 @@ cglobal hqdn3d_row_%1_x86, 7,7,0, src, dst, lineant, frameant, width, spatial, t
     %define frameantq r0
     %define lineantq  r0
     %define pixelantq r1
+    %define dstp r0p
+    %define frameantp r0p
+    %define lineantp  r0p
+    %define pixelantp r1p
     %define pixelantd r1d
     DECLARE_REG_TMP 2,3
 %endif
     LOAD   pixelantd, xq, %1
 ALIGN 16
 .loop:
-    movifnidn srcq, srcmp
+    movifnidn srcp, srcmp
     LOAD      t0d, xq+1, %1 ; skip on the last iteration to avoid overread
 .loop2:
-    movifnidn lineantq, lineantmp
+    movifnidn lineantp, lineantmp
     movzx     t1d, word [lineantq+xq*2]
     LOWPASS   t1, pixelant, spatial
     mov       [lineantq+xq*2], t1w
     LOWPASS   pixelant, t0, spatial
-    movifnidn frameantq, frameantmp
+    movifnidn frameantp, frameantmp
     movzx     t0d, word [frameantq+xq*2]
     LOWPASS   t0, t1, temporal
     mov       [frameantq+xq*2], t0w
-    movifnidn dstq, dstmp
+    movifnidn dstp, dstmp
 %if %1 != 16
     shr    t0d, 16-%1 ; could eliminate this by storing from t0h, but only with some contraints on register allocation
 %endif
diff --git a/libavfilter/x86/vf_idet.asm b/libavfilter/x86/vf_idet.asm
index 9596abd7e2..78181749d9 100644
--- a/libavfilter/x86/vf_idet.asm
+++ b/libavfilter/x86/vf_idet.asm
@@ -28,7 +28,7 @@ SECTION .text
 ; Implementation that does 8-bytes at a time using single-word operations.
 %macro IDET_FILTER_LINE 1
 INIT_MMX %1
-cglobal idet_filter_line, 4, 5, 0, a, b, c, width, index
+cglobal idet_filter_line, 4, 5, 0, "p", a, "p", b, "p", c, "d", width, index
     xor       indexq, indexq
 %define   m_zero m2
 %define   m_sum  m5
@@ -94,7 +94,7 @@ IDET_FILTER_LINE mmx
 %endmacro
 
 %macro IDET_FILTER_LINE_16BIT 1   ; %1=increment (4 or 8 words)
-cglobal idet_filter_line_16bit, 4, 5, 8, a, b, c, width, index
+cglobal idet_filter_line_16bit, 4, 5, 8, "p", a, "p", b, "p", c, "d", width, index
     xor       indexq, indexq
 %define m_zero m1
 %define m_sum  m0
@@ -137,7 +137,7 @@ IDET_FILTER_LINE_16BIT 4
 ; SSE2 8-bit implementation that does 16-bytes at a time:
 
 INIT_XMM sse2
-cglobal idet_filter_line, 4, 6, 7, a, b, c, width, index, total
+cglobal idet_filter_line, 4, 6, 7, "p", a, "p", b, "p", c, "d", width, index, total
     xor       indexq, indexq
     pxor      m0, m0
     pxor      m1, m1
diff --git a/libavfilter/x86/vf_interlace.asm b/libavfilter/x86/vf_interlace.asm
index a6c65b805d..37c09e5fed 100644
--- a/libavfilter/x86/vf_interlace.asm
+++ b/libavfilter/x86/vf_interlace.asm
@@ -77,16 +77,16 @@ SECTION .text
 %endmacro
 
 %macro LOWPASS_LINE 0
-cglobal lowpass_line, 5, 5, 7, dst, h, src, mref, pref
+cglobal lowpass_line, 5, 5, 7, "p", dst, "p-", h, "p", src, "p-", mref, "p-", pref
     LOWPASS b
 
-cglobal lowpass_line_16, 5, 5, 7, dst, h, src, mref, pref
+cglobal lowpass_line_16, 5, 5, 7, "p", dst, "p-", h, "p", src, "p-", mref, "p-", pref
     shl hq, 1
     LOWPASS w
 %endmacro
 
 %macro LOWPASS_LINE_COMPLEX 0
-cglobal lowpass_line_complex, 5, 5, 8, dst, h, src, mref, pref
+cglobal lowpass_line_complex, 5, 5, 8, "p", dst, "p-", h, "p", src, "p-", mref, "p-", pref
     pxor m7, m7
 .loop:
     movu m0, [srcq+mrefq]
@@ -148,7 +148,7 @@ cglobal lowpass_line_complex, 5, 5, 8, dst, h, src, mref, pref
     jg .loop
 REP_RET
 
-cglobal lowpass_line_complex_12, 5, 5, 8, 16, dst, h, src, mref, pref, clip_max
+cglobal lowpass_line_complex_12, 5, 5, 8, 16, "p", dst, "p-", h, "p", src, "p-", mref, "p-", pref, "d", clip_max
     movd m7, DWORD clip_maxm
     SPLATW m7, m7, 0
     movu [rsp], m7
diff --git a/libavfilter/x86/vf_limiter.asm b/libavfilter/x86/vf_limiter.asm
index c5b9b0a64d..c9d842fa27 100644
--- a/libavfilter/x86/vf_limiter.asm
+++ b/libavfilter/x86/vf_limiter.asm
@@ -24,8 +24,7 @@ SECTION .text
 
 INIT_XMM sse2
 
-cglobal limiter_8bit, 6, 7, 3, src, dst, slinesize, dlinesize, w, h, x
-    movsxdifnidn wq, wd
+cglobal limiter_8bit, 6, 7, 3, "p", src, "p", dst, "p-", slinesize, "p-", dlinesize, "d-", w, "d", h, x
     add        srcq, wq
     add        dstq, wq
     neg          wq
@@ -53,7 +52,7 @@ cglobal limiter_8bit, 6, 7, 3, src, dst, slinesize, dlinesize, w, h, x
 
 INIT_XMM sse4
 
-cglobal limiter_16bit, 6, 7, 3, src, dst, slinesize, dlinesize, w, h, x
+cglobal limiter_16bit, 6, 7, 3, "p", src, "p", dst, "p-", slinesize, "p-", dlinesize, "d", w, "d", h, x
     shl          wd, 1
     add        srcq, wq
     add        dstq, wq
diff --git a/libavfilter/x86/vf_maskedmerge.asm b/libavfilter/x86/vf_maskedmerge.asm
index 7e61935b97..12d533aeda 100644
--- a/libavfilter/x86/vf_maskedmerge.asm
+++ b/libavfilter/x86/vf_maskedmerge.asm
@@ -31,11 +31,11 @@ SECTION .text
 
 INIT_XMM sse2
 %if ARCH_X86_64
-cglobal maskedmerge8, 8, 11, 7, bsrc, osrc, msrc, dst, blinesize, olinesize, mlinesize, dlinesize, w, h, x
+cglobal maskedmerge8, 8, 11, 7, "p", bsrc, "p", osrc, "p", msrc, "p", dst, "p-", blinesize, "p-", olinesize, "p-", mlinesize, "p-", dlinesize, "d", w, "d", h, "d", x
     mov         wd, dword wm
     mov         hd, dword hm
 %else
-cglobal maskedmerge8, 5, 7, 7, bsrc, osrc, msrc, dst, blinesize, w, x
+cglobal maskedmerge8, 5, 7, 7, "p", bsrc, "p", osrc, "p", msrc, "p", dst, "p-", blinesize, w, x
     mov         wd, r8m
 %define olinesizeq r5mp
 %define mlinesizeq r6mp
diff --git a/libavfilter/x86/vf_overlay.asm b/libavfilter/x86/vf_overlay.asm
index 14ec60ca34..4451e66310 100644
--- a/libavfilter/x86/vf_overlay.asm
+++ b/libavfilter/x86/vf_overlay.asm
@@ -33,9 +33,8 @@ pw_257:   times  8 dw 257
 SECTION .text
 
 INIT_XMM sse4
-cglobal overlay_row_44, 5, 7, 6, 0, d, da, s, a, w, r, x
+cglobal overlay_row_44, 5, 7, 6, 0, "p", d, "p", da, "p", s, "p", a, "d-", w, r, x
     xor          xq, xq
-    movsxdifnidn wq, wd
     mov          rq, wq
     and          rq, mmsize/2 - 1
     cmp          wq, mmsize/2
@@ -65,9 +64,8 @@ cglobal overlay_row_44, 5, 7, 6, 0, d, da, s, a, w, r, x
     RET
 
 INIT_XMM sse4
-cglobal overlay_row_22, 5, 7, 6, 0, d, da, s, a, w, r, x
+cglobal overlay_row_22, 5, 7, 6, 0, "p", d, "p", da, "p", s, "p", a, "d-", w, r, x
     xor          xq, xq
-    movsxdifnidn wq, wd
     sub          wq, 1
     mov          rq, wq
     and          rq, mmsize/2 - 1
@@ -103,11 +101,10 @@ cglobal overlay_row_22, 5, 7, 6, 0, d, da, s, a, w, r, x
     RET
 
 INIT_XMM sse4
-cglobal overlay_row_20, 6, 7, 7, 0, d, da, s, a, w, r, x
+cglobal overlay_row_20, 6, 7, 7, 0, "p", d, "p", da, "p", s, "p", a, "d-", w, r, x
     mov         daq, aq
-    add         daq, rmp
+    add         daq, rmq
     xor          xq, xq
-    movsxdifnidn wq, wd
     sub          wq, 1
     mov          rq, wq
     and          rq, mmsize/2 - 1
diff --git a/libavfilter/x86/vf_pp7.asm b/libavfilter/x86/vf_pp7.asm
index 7b3e5cf5e3..e6a026e62c 100644
--- a/libavfilter/x86/vf_pp7.asm
+++ b/libavfilter/x86/vf_pp7.asm
@@ -27,7 +27,7 @@ SECTION .text
 INIT_MMX mmx
 
 ;void ff_pp7_dctB_mmx(int16_t *dst, int16_t *src)
-cglobal pp7_dctB, 2, 2, 0, dst, src
+cglobal pp7_dctB, 2, 2, 0, "p", dst, "p", src
     movq   m0, [srcq]
     movq   m1, [srcq+mmsize*1]
     paddw  m0, [srcq+mmsize*6]
diff --git a/libavfilter/x86/vf_psnr.asm b/libavfilter/x86/vf_psnr.asm
index 11eb81a225..727a55b45e 100644
--- a/libavfilter/x86/vf_psnr.asm
+++ b/libavfilter/x86/vf_psnr.asm
@@ -28,15 +28,15 @@ SECTION .text
 INIT_XMM sse2
 %if ARCH_X86_32
 %if %1 == 8
-cglobal sse_line_%1 %+ bit, 0, 6, 8, res, buf, w, px1, px2, ref
+cglobal sse_line_%1 %+ bit, 0, 6, 8, "p", res, "p", buf, "d", w, px1, px2, ref
 %else
-cglobal sse_line_%1 %+ bit, 0, 7, 8, res, buf, reshigh, w, px1, px2, ref
+cglobal sse_line_%1 %+ bit, 0, 7, 8, "p", res, "p", buf, "d", reshigh, w, px1, px2, ref
 %endif
     mov       bufq, r0mp
     mov       refq, r1mp
     mov         wd, r2m
 %else
-cglobal sse_line_%1 %+ bit, 3, 5, 8, buf, ref, w, px1, px2
+cglobal sse_line_%1 %+ bit, 3, 5, 8, "p", buf, "p", ref, "d", w, px1, px2
 %endif
     pxor        m6, m6
     pxor        m7, m7
diff --git a/libavfilter/x86/vf_pullup.asm b/libavfilter/x86/vf_pullup.asm
index 26c2a27d37..cc59af83f5 100644
--- a/libavfilter/x86/vf_pullup.asm
+++ b/libavfilter/x86/vf_pullup.asm
@@ -23,7 +23,7 @@
 SECTION .text
 
 INIT_MMX mmx
-cglobal pullup_filter_diff, 3, 5, 8, first, second, size
+cglobal pullup_filter_diff, 3, 5, 8, "p", first, "p", second, "p-", size
     mov        r3, 4
     pxor       m4, m4
     pxor       m7, m7
@@ -61,7 +61,7 @@ cglobal pullup_filter_diff, 3, 5, 8, first, second, size
     RET
 
 INIT_MMX mmx
-cglobal pullup_filter_comb, 3, 5, 8, first, second, size
+cglobal pullup_filter_comb, 3, 5, 8, "p", first, "p", second, "p-", size
     mov        r3, 4
     pxor       m6, m6
     pxor       m7, m7
@@ -140,7 +140,7 @@ cglobal pullup_filter_comb, 3, 5, 8, first, second, size
     RET
 
 INIT_MMX mmx
-cglobal pullup_filter_var, 3, 5, 8, first, second, size
+cglobal pullup_filter_var, 3, 5, 8, "p", first, "p", second, "p-", size
     mov        r3, 3
     pxor       m4, m4
     pxor       m7, m7
diff --git a/libavfilter/x86/vf_removegrain.asm b/libavfilter/x86/vf_removegrain.asm
index d049bf257d..5a8cea9683 100644
--- a/libavfilter/x86/vf_removegrain.asm
+++ b/libavfilter/x86/vf_removegrain.asm
@@ -158,7 +158,7 @@ SECTION .text
 ; Functions
 
 INIT_XMM sse2
-cglobal rg_fl_mode_1, 4, 5, 3, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_1, 4, 5, 3, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -208,7 +208,7 @@ cglobal rg_fl_mode_1, 4, 5, 3, 0, dst, src, stride, pixels
 RET
 
 %if ARCH_X86_64
-cglobal rg_fl_mode_2, 4, 5, 10, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_2, 4, 5, 10, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -227,7 +227,7 @@ cglobal rg_fl_mode_2, 4, 5, 10, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_3, 4, 5, 10, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_3, 4, 5, 10, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -246,7 +246,7 @@ cglobal rg_fl_mode_3, 4, 5, 10, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_4, 4, 5, 10, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_4, 4, 5, 10, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -265,7 +265,7 @@ cglobal rg_fl_mode_4, 4, 5, 10, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_5, 4, 5, 13, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_5, 4, 5, 13, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -315,7 +315,7 @@ cglobal rg_fl_mode_5, 4, 5, 13, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_6, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_6, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -383,7 +383,7 @@ cglobal rg_fl_mode_6, 4, 5, 16, 0, dst, src, stride, pixels
 RET
 
 ; This is just copy-pasted straight from mode 6 with the left shifts removed.
-cglobal rg_fl_mode_7, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_7, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -444,7 +444,7 @@ cglobal rg_fl_mode_7, 4, 5, 16, 0, dst, src, stride, pixels
 RET
 
 ; This is just copy-pasted straight from mode 6 with a few changes.
-cglobal rg_fl_mode_8, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_8, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -508,7 +508,7 @@ cglobal rg_fl_mode_8, 4, 5, 16, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_9, 4, 5, 13, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_9, 4, 5, 13, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -552,7 +552,7 @@ cglobal rg_fl_mode_9, 4, 5, 13, 0, dst, src, stride, pixels
 RET
 %endif
 
-cglobal rg_fl_mode_10, 4, 5, 8, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_10, 4, 5, 8, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -621,7 +621,7 @@ cglobal rg_fl_mode_10, 4, 5, 8, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_11_12, 4, 5, 7, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_11_12, 4, 5, 7, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -663,7 +663,7 @@ cglobal rg_fl_mode_11_12, 4, 5, 7, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_13_14, 4, 5, 8, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_13_14, 4, 5, 8, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -702,7 +702,7 @@ cglobal rg_fl_mode_13_14, 4, 5, 8, 0, dst, src, stride, pixels
 RET
 
 %if ARCH_X86_64
-cglobal rg_fl_mode_15_16, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_15_16, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -755,7 +755,7 @@ cglobal rg_fl_mode_15_16, 4, 5, 16, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_17, 4, 5, 9, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_17, 4, 5, 9, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -787,7 +787,7 @@ cglobal rg_fl_mode_17, 4, 5, 9, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_18, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_18, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -874,7 +874,7 @@ cglobal rg_fl_mode_18, 4, 5, 16, 0, dst, src, stride, pixels
 RET
 %endif
 
-cglobal rg_fl_mode_19, 4, 5, 7, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_19, 4, 5, 7, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -914,7 +914,7 @@ cglobal rg_fl_mode_19, 4, 5, 7, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_20, 4, 5, 7, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_20, 4, 5, 7, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -957,7 +957,7 @@ cglobal rg_fl_mode_20, 4, 5, 7, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_21, 4, 5, 8, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_21, 4, 5, 8, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -1036,7 +1036,7 @@ cglobal rg_fl_mode_21, 4, 5, 8, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_22, 4, 5, 8, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_22, 4, 5, 8, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -1078,7 +1078,7 @@ cglobal rg_fl_mode_22, 4, 5, 8, 0, dst, src, stride, pixels
 RET
 
 %if ARCH_X86_64
-cglobal rg_fl_mode_23, 4, 5, 16, 0, dst, src, stride, pixels
+cglobal rg_fl_mode_23, 4, 5, 16, 0, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
@@ -1139,7 +1139,7 @@ cglobal rg_fl_mode_23, 4, 5, 16, 0, dst, src, stride, pixels
     jg .loop
 RET
 
-cglobal rg_fl_mode_24, 4, 5, 16, mmsize, dst, src, stride, pixels
+cglobal rg_fl_mode_24, 4, 5, 16, mmsize, "p", dst, "p", src, "p-", stride, "d", pixels
     mov r4q, strideq
     neg r4q
     %define stride_p strideq
diff --git a/libavfilter/x86/vf_ssim.asm b/libavfilter/x86/vf_ssim.asm
index 3293e66701..de5eee7ffd 100644
--- a/libavfilter/x86/vf_ssim.asm
+++ b/libavfilter/x86/vf_ssim.asm
@@ -32,9 +32,9 @@ SECTION .text
 
 %macro SSIM_4X4_LINE 1
 %if ARCH_X86_64
-cglobal ssim_4x4_line, 6, 8, %1, buf, buf_stride, ref, ref_stride, sums, w, buf_stride3, ref_stride3
+cglobal ssim_4x4_line, 6, 8, %1, "p", buf, "p-", buf_stride, "p", ref, "p-", ref_stride, "p", sums, "d", w, buf_stride3, ref_stride3
 %else
-cglobal ssim_4x4_line, 5, 7, %1, buf, buf_stride, ref, ref_stride, sums, buf_stride3, ref_stride3
+cglobal ssim_4x4_line, 5, 7, %1, "p", buf, "p-", buf_stride, "p", ref, "p-", ref_stride, "p", sums, buf_stride3, ref_stride3
 %define wd r5mp
 %endif
     lea     ref_stride3q, [ref_strideq*3]
@@ -169,7 +169,7 @@ SSIM_4X4_LINE 8
 %endif
 
 INIT_XMM sse4
-cglobal ssim_end_line, 3, 3, 6, sum0, sum1, w
+cglobal ssim_end_line, 3, 3, 6, "p", sum0, "p", sum1, "d", w
     pxor              m0, m0
 .loop:
     mova              m1, [sum0q+mmsize*0]
diff --git a/libavfilter/x86/vf_stereo3d.asm b/libavfilter/x86/vf_stereo3d.asm
index a057e495f1..157e3b5be9 100644
--- a/libavfilter/x86/vf_stereo3d.asm
+++ b/libavfilter/x86/vf_stereo3d.asm
@@ -36,18 +36,26 @@ SECTION .text
 
 INIT_XMM sse4
 %if ARCH_X86_64
-cglobal anaglyph, 6, 10, 14, 2*6*mmsize, dst, lsrc, rsrc, dst_linesize, l_linesize, r_linesize, width, height, o, cnt
+cglobal anaglyph, 6, 10, 14, 2*6*mmsize, "p", dst, "p", lsrc, "p", rsrc, \
+                                         "p-", dst_linesize, "p-", l_linesize, "p-", r_linesize, \
+                                         width, height, o, cnt
+%define ana_matrix_rp r6p
+%define ana_matrix_gp r7p
+%define ana_matrix_bp r8p
 %define ana_matrix_rq r6q
 %define ana_matrix_gq r7q
 %define ana_matrix_bq r8q
 
 %else ; ARCH_X86_32
 %if HAVE_ALIGNED_STACK
-cglobal anaglyph, 3, 7, 8, 2*9*mmsize, dst, lsrc, rsrc, dst_linesize, l_linesize, o, cnt
+cglobal anaglyph, 3, 7, 8, 2*9*mmsize, "p", dst, "p", lsrc, "p", rsrc, "p-", dst_linesize, "p-", l_linesize, o, cnt
 %else
-cglobal anaglyph, 3, 6, 8, 2*9*mmsize, dst, lsrc, rsrc, dst_linesize, o, cnt
+cglobal anaglyph, 3, 6, 8, 2*9*mmsize, "p", dst, "p", lsrc, "p", rsrc, "p-", dst_linesize, o, cnt
 %define l_linesizeq r4mp
 %endif ; HAVE_ALIGNED_STACK
+%define ana_matrix_rp r3p
+%define ana_matrix_gp r4p
+%define ana_matrix_bp r5p
 %define ana_matrix_rq r3q
 %define ana_matrix_gq r4q
 %define ana_matrix_bq r5q
@@ -62,9 +70,9 @@ cglobal anaglyph, 3, 6, 8, 2*9*mmsize, dst, lsrc, rsrc, dst_linesize, o, cnt
 %define m13 [rsp+mmsize*17]
 %endif ; ARCH
 
-    mov        ana_matrix_rq, r8m
-    mov        ana_matrix_gq, r9m
-    mov        ana_matrix_bq, r10m
+    mov        ana_matrix_rp, r8mp
+    mov        ana_matrix_gp, r9mp
+    mov        ana_matrix_bp, r10mp
     movu                  m3, [ana_matrix_rq+ 0]
     movq                  m5, [ana_matrix_rq+16]
     pshufd                m0, m3, q0000
diff --git a/libavfilter/x86/vf_threshold.asm b/libavfilter/x86/vf_threshold.asm
index 098069b083..cb7253a287 100644
--- a/libavfilter/x86/vf_threshold.asm
+++ b/libavfilter/x86/vf_threshold.asm
@@ -32,11 +32,11 @@ SECTION .text
 ;%1 depth (8 or 16) ; %2 b or w ; %3 constant
 %macro THRESHOLD 3
 %if ARCH_X86_64
-cglobal threshold%1, 10, 13, 5, in, threshold, min, max, out, ilinesize, tlinesize, flinesize, slinesize, olinesize, w, h, x
-    mov             wd, dword wm
-    mov             hd, dword hm
+cglobal threshold%1, 12, 13, 5, "p", in, "p", threshold, "p", min, "p", max, "p", out, \
+                                "p-", ilinesize, "p-", tlinesize, "p-", flinesize, "p-", slinesize, "p-", olinesize, \
+                                "d", w, "d", h, x
 %else
-cglobal threshold%1, 5, 7, 5, in, threshold, min, max, out, w, x
+cglobal threshold%1, 5, 7, 5, "p", in, "p", threshold, "p", min, "p", max, "p", out, w, x
     mov             wd, r10m
 %define     ilinesizeq  r5mp
 %define     tlinesizeq  r6mp
diff --git a/libavfilter/x86/vf_w3fdif.asm b/libavfilter/x86/vf_w3fdif.asm
index 52628c38d7..3a6f662f1f 100644
--- a/libavfilter/x86/vf_w3fdif.asm
+++ b/libavfilter/x86/vf_w3fdif.asm
@@ -25,7 +25,7 @@
 SECTION .text
 
 INIT_XMM sse2
-cglobal w3fdif_scale, 3, 3, 2, 0, out_pixel, work_pixel, linesize
+cglobal w3fdif_scale, 3, 3, 2, 0, "p", out_pixel, "p", work_pixel, "d", linesize
 .loop:
     mova                         m0, [work_pixelq]
     mova                         m1, [work_pixelq+mmsize]
@@ -40,15 +40,15 @@ cglobal w3fdif_scale, 3, 3, 2, 0, out_pixel, work_pixel, linesize
     jg .loop
 REP_RET
 
-cglobal w3fdif_simple_low, 4, 5, 6, 0, work_line, in_lines_cur0, coef, linesize, offset
+cglobal w3fdif_simple_low, 4, 5, 6, 0, "p", work_line, "p", in_lines_cur0, "p", coef, "d", linesize, offset
     movd                  m1, [coefq]
     DEFINE_ARGS    work_line, in_lines_cur0, in_lines_cur1, linesize, offset
     SPLATW                m0, m1, 0
     SPLATW                m1, m1, 1
     pxor                  m4, m4
     mov              offsetq, 0
-    mov       in_lines_cur1q, [in_lines_cur0q + gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
+    mov       in_lines_cur1p, [in_lines_cur0q + ptrsize]
+    mov       in_lines_cur0p, [in_lines_cur0q]
 
 .loop:
     movh                                   m2, [in_lines_cur0q+offsetq]
@@ -65,17 +65,17 @@ cglobal w3fdif_simple_low, 4, 5, 6, 0, work_line, in_lines_cur0, coef, linesize,
     jg .loop
 REP_RET
 
-cglobal w3fdif_complex_low, 4, 7, 8, 0, work_line, in_lines_cur0, coef, linesize
+cglobal w3fdif_complex_low, 4, 7, 8, 0, "p", work_line, "p", in_lines_cur0, "p", coef, "d", linesize
     movq                  m0, [coefq]
     DEFINE_ARGS    work_line, in_lines_cur0, in_lines_cur1, linesize, offset, in_lines_cur2, in_lines_cur3
     pshufd                m2, m0, q1111
     SPLATD                m0
     pxor                  m1, m1
     mov              offsetq, 0
-    mov       in_lines_cur3q, [in_lines_cur0q+gprsize*3]
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
+    mov       in_lines_cur3p, [in_lines_cur0q+ptrsize*3]
+    mov       in_lines_cur2p, [in_lines_cur0q+ptrsize*2]
+    mov       in_lines_cur1p, [in_lines_cur0q+ptrsize]
+    mov       in_lines_cur0p, [in_lines_cur0q]
 
 .loop:
     movh                                   m4, [in_lines_cur0q+offsetq]
@@ -102,9 +102,9 @@ cglobal w3fdif_complex_low, 4, 7, 8, 0, work_line, in_lines_cur0, coef, linesize
 REP_RET
 
 %if ARCH_X86_64
-cglobal w3fdif_simple_high, 5, 9, 8, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
+cglobal w3fdif_simple_high, 5, 9, 8, 0, "p", work_line, "p", in_lines_cur0, "p", in_lines_adj0, "p", coef, "d", linesize
 %else
-cglobal w3fdif_simple_high, 4, 7, 8, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
+cglobal w3fdif_simple_high, 4, 7, 8, 0, "p", work_line, "p", in_lines_cur0, "p", in_lines_adj0, "p", coef, linesize
 %endif
     movq                  m2, [coefq]
 %if ARCH_X86_64
@@ -118,12 +118,12 @@ cglobal w3fdif_simple_high, 4, 7, 8, 0, work_line, in_lines_cur0, in_lines_adj0,
     pshufd                m0, m2, q0000
     SPLATW                m2, m2, 2
     pxor                  m7, m7
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-    mov       in_lines_adj2q, [in_lines_adj0q+gprsize*2]
-    mov       in_lines_adj1q, [in_lines_adj0q+gprsize]
-    mov       in_lines_adj0q, [in_lines_adj0q]
+    mov       in_lines_cur2p, [in_lines_cur0q+ptrsize*2]
+    mov       in_lines_cur1p, [in_lines_cur0q+ptrsize]
+    mov       in_lines_cur0p, [in_lines_cur0q]
+    mov       in_lines_adj2p, [in_lines_adj0q+ptrsize*2]
+    mov       in_lines_adj1p, [in_lines_adj0q+ptrsize]
+    mov       in_lines_adj0p, [in_lines_adj0q]
 
 %if ARCH_X86_32
     sub in_lines_cur1q, in_lines_cur0q
@@ -183,7 +183,7 @@ REP_RET
 
 %if ARCH_X86_64
 
-cglobal w3fdif_complex_high, 5, 13, 10, 0, work_line, in_lines_cur0, in_lines_adj0, coef, linesize
+cglobal w3fdif_complex_high, 5, 13, 10, 0, "p", work_line, "p", in_lines_cur0, "p", in_lines_adj0, "p", coef, "d", linesize
     movq                  m0, [coefq+0]
     movd                  m4, [coefq+8]
     DEFINE_ARGS    work_line, in_lines_cur0, in_lines_adj0, in_lines_cur1, linesize, offset, in_lines_cur2, in_lines_cur3, in_lines_cur4, in_lines_adj1, in_lines_adj2, in_lines_adj3, in_lines_adj4
@@ -192,16 +192,16 @@ cglobal w3fdif_complex_high, 5, 13, 10, 0, work_line, in_lines_cur0, in_lines_ad
     SPLATW                m4, m4
     pxor                  m3, m3
     mov              offsetq, 0
-    mov       in_lines_cur4q, [in_lines_cur0q+gprsize*4]
-    mov       in_lines_cur3q, [in_lines_cur0q+gprsize*3]
-    mov       in_lines_cur2q, [in_lines_cur0q+gprsize*2]
-    mov       in_lines_cur1q, [in_lines_cur0q+gprsize]
-    mov       in_lines_cur0q, [in_lines_cur0q]
-    mov       in_lines_adj4q, [in_lines_adj0q+gprsize*4]
-    mov       in_lines_adj3q, [in_lines_adj0q+gprsize*3]
-    mov       in_lines_adj2q, [in_lines_adj0q+gprsize*2]
-    mov       in_lines_adj1q, [in_lines_adj0q+gprsize]
-    mov       in_lines_adj0q, [in_lines_adj0q]
+    mov       in_lines_cur4p, [in_lines_cur0q+ptrsize*4]
+    mov       in_lines_cur3p, [in_lines_cur0q+ptrsize*3]
+    mov       in_lines_cur2p, [in_lines_cur0q+ptrsize*2]
+    mov       in_lines_cur1p, [in_lines_cur0q+ptrsize]
+    mov       in_lines_cur0p, [in_lines_cur0q]
+    mov       in_lines_adj4p, [in_lines_adj0q+ptrsize*4]
+    mov       in_lines_adj3p, [in_lines_adj0q+ptrsize*3]
+    mov       in_lines_adj2p, [in_lines_adj0q+ptrsize*2]
+    mov       in_lines_adj1p, [in_lines_adj0q+ptrsize]
+    mov       in_lines_adj0p, [in_lines_adj0q]
 
 .loop:
     movh                                   m5, [in_lines_cur0q+offsetq]
diff --git a/libavfilter/x86/vf_yadif.asm b/libavfilter/x86/vf_yadif.asm
index a29620ce55..6495ac6bdc 100644
--- a/libavfilter/x86/vf_yadif.asm
+++ b/libavfilter/x86/vf_yadif.asm
@@ -205,11 +205,11 @@ SECTION .text
 
 %macro YADIF 0
 %if ARCH_X86_32
-cglobal yadif_filter_line, 4, 6, 8, 80, dst, prev, cur, next, w, prefs, \
-                                        mrefs, parity, mode
+cglobal yadif_filter_line, 4, 6, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                        "d", mrefs, "d", parity, "d", mode
 %else
-cglobal yadif_filter_line, 4, 7, 8, 80, dst, prev, cur, next, w, prefs, \
-                                        mrefs, parity, mode
+cglobal yadif_filter_line, 4, 7, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, "d", prefs, \
+                                        "d", mrefs, "d", parity, "d", mode
 %endif
 %if ARCH_X86_32
     mov            r4, r5mp
diff --git a/libavfilter/x86/yadif-10.asm b/libavfilter/x86/yadif-10.asm
index 8853e0d2c7..59e1d28aad 100644
--- a/libavfilter/x86/yadif-10.asm
+++ b/libavfilter/x86/yadif-10.asm
@@ -217,11 +217,11 @@ SECTION .text
 
 %macro YADIF 0
 %if ARCH_X86_32
-cglobal yadif_filter_line_10bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
+cglobal yadif_filter_line_10bit, 4, 6, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, \
+                                              "d", prefs, "d", mrefs, "d", parity, "d", mode
 %else
-cglobal yadif_filter_line_10bit, 4, 7, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
+cglobal yadif_filter_line_10bit, 4, 7, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, \
+                                              "d", prefs, "d", mrefs, "d", parity, "d", mode
 %endif
 %if ARCH_X86_32
     mov            r4, r5mp
diff --git a/libavfilter/x86/yadif-16.asm b/libavfilter/x86/yadif-16.asm
index 9053b378a5..c6e995c006 100644
--- a/libavfilter/x86/yadif-16.asm
+++ b/libavfilter/x86/yadif-16.asm
@@ -253,11 +253,11 @@ SECTION .text
 
 %macro YADIF 0
 %if ARCH_X86_32
-cglobal yadif_filter_line_16bit, 4, 6, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
+cglobal yadif_filter_line_16bit, 4, 6, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, \
+                                              "d", prefs, "d", mrefs, "d", parity, "d", mode
 %else
-cglobal yadif_filter_line_16bit, 4, 7, 8, 80, dst, prev, cur, next, w, \
-                                              prefs, mrefs, parity, mode
+cglobal yadif_filter_line_16bit, 4, 7, 8, 80, "p", dst, "p", prev, "p", cur, "p", next, "d", w, \
+                                              "d", prefs, "d", mrefs, "d", parity, "d", mode
 %endif
 %if ARCH_X86_32
     mov            r4, r5mp
-- 
2.21.0


From 8520e322ea2cf1a5813776be73ffd124ff6c061e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Matthias=20R=C3=A4ncker?= <theonetruecamper@gmx.de>
Date: Sat, 20 Apr 2019 15:06:53 +0200
Subject: [PATCH 17/17] x32: libavcodec
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Matthias Räncker <theonetruecamper@gmx.de>
---
 libavcodec/x86/aacencdsp.asm            |   4 +-
 libavcodec/x86/aacpsdsp.asm             |  16 ++--
 libavcodec/x86/ac3dsp.asm               |  20 ++--
 libavcodec/x86/ac3dsp_downmix.asm       |  12 ++-
 libavcodec/x86/alacdsp.asm              |  30 +++---
 libavcodec/x86/audiodsp.asm             |   8 +-
 libavcodec/x86/blockdsp.asm             |   4 +-
 libavcodec/x86/bswapdsp.asm             |   4 +-
 libavcodec/x86/cavsidct.asm             |   4 +-
 libavcodec/x86/dcadsp.asm               |   4 +-
 libavcodec/x86/dct32.asm                |   4 +-
 libavcodec/x86/dirac_dwt.asm            |  29 ++----
 libavcodec/x86/diracdsp.asm             |  20 ++--
 libavcodec/x86/dnxhdenc.asm             |   2 +-
 libavcodec/x86/exrdsp.asm               |   4 +-
 libavcodec/x86/fft.asm                  |  60 ++++++------
 libavcodec/x86/flac_dsp_gpl.asm         |   6 +-
 libavcodec/x86/flacdsp.asm              |  34 +++----
 libavcodec/x86/fmtconvert.asm           |   6 +-
 libavcodec/x86/fpel.asm                 |   2 +-
 libavcodec/x86/g722dsp.asm              |   2 +-
 libavcodec/x86/h263_loopfilter.asm      |  10 +-
 libavcodec/x86/h264_chromamc.asm        |  10 +-
 libavcodec/x86/h264_chromamc_10bit.asm  |   6 +-
 libavcodec/x86/h264_deblock.asm         |  47 +++++-----
 libavcodec/x86/h264_deblock_10bit.asm   |  24 ++---
 libavcodec/x86/h264_idct.asm            |  92 ++++++++----------
 libavcodec/x86/h264_idct_10bit.asm      |  53 +++++------
 libavcodec/x86/h264_intrapred.asm       |  82 ++++++++--------
 libavcodec/x86/h264_intrapred_10bit.asm |  54 +++++------
 libavcodec/x86/h264_qpel_10bit.asm      |  10 +-
 libavcodec/x86/h264_qpel_8bit.asm       |  64 ++++---------
 libavcodec/x86/h264_weight.asm          |  20 ++--
 libavcodec/x86/h264_weight_10bit.asm    |  16 +---
 libavcodec/x86/hevc_add_res.asm         |  22 ++---
 libavcodec/x86/hevc_deblock.asm         |  24 ++---
 libavcodec/x86/hevc_idct.asm            |  12 +--
 libavcodec/x86/hevc_mc.asm              |  48 +++++-----
 libavcodec/x86/hevc_sao.asm             |   6 +-
 libavcodec/x86/hevc_sao_10bit.asm       |   6 +-
 libavcodec/x86/hpeldsp.asm              |  34 +++----
 libavcodec/x86/hpeldsp_vp3.asm          |   4 +-
 libavcodec/x86/huffyuvdsp.asm           |   6 +-
 libavcodec/x86/huffyuvencdsp.asm        |   4 +-
 libavcodec/x86/idctdsp.asm              |   6 +-
 libavcodec/x86/imdct36.asm              |   4 +-
 libavcodec/x86/jpeg2000dsp.asm          |   4 +-
 libavcodec/x86/lossless_audiodsp.asm    |   6 +-
 libavcodec/x86/lossless_videodsp.asm    |  20 ++--
 libavcodec/x86/lossless_videoencdsp.asm |   6 +-
 libavcodec/x86/mdct15.asm               |   4 +-
 libavcodec/x86/me_cmp.asm               |  22 ++---
 libavcodec/x86/mlpdsp.asm               |   6 +-
 libavcodec/x86/mpegvideoencdsp.asm      |   6 +-
 libavcodec/x86/opus_pvq_search.asm      |   2 +-
 libavcodec/x86/pixblockdsp.asm          |   6 +-
 libavcodec/x86/pngdsp.asm               |  11 +--
 libavcodec/x86/proresdsp.asm            |   2 +-
 libavcodec/x86/qpel.asm                 |  12 +--
 libavcodec/x86/qpeldsp.asm              |  23 ++---
 libavcodec/x86/rv34dsp.asm              |  12 +--
 libavcodec/x86/rv40dsp.asm              |  17 ++--
 libavcodec/x86/sbcdsp.asm               |   6 +-
 libavcodec/x86/sbrdsp.asm               |  28 +++---
 libavcodec/x86/simple_idct.asm          |  10 +-
 libavcodec/x86/simple_idct10.asm        |  14 +--
 libavcodec/x86/svq1enc.asm              |   2 +-
 libavcodec/x86/synth_filter.asm         |  10 +-
 libavcodec/x86/takdsp.asm               |   8 +-
 libavcodec/x86/ttadsp.asm               |   3 +-
 libavcodec/x86/ttaencdsp.asm            |   2 +-
 libavcodec/x86/utvideodsp.asm           |   7 +-
 libavcodec/x86/v210.asm                 |   3 +-
 libavcodec/x86/v210enc.asm              |   4 +-
 libavcodec/x86/vc1dsp_loopfilter.asm    |  22 ++---
 libavcodec/x86/vc1dsp_mc.asm            |  14 +--
 libavcodec/x86/vc1dsp_mmx.c             |  21 +++--
 libavcodec/x86/videodsp.asm             |  48 +++++-----
 libavcodec/x86/vorbisdsp.asm            |   4 +-
 libavcodec/x86/vp3dsp.asm               |  12 +--
 libavcodec/x86/vp6dsp.asm               |   4 +-
 libavcodec/x86/vp8dsp.asm               |  48 +++++-----
 libavcodec/x86/vp8dsp_loopfilter.asm    |  10 +-
 libavcodec/x86/vp9intrapred.asm         |  96 +++++++++----------
 libavcodec/x86/vp9intrapred_16bpp.asm   | 118 +++++++++++-------------
 libavcodec/x86/vp9itxfm.asm             |  29 +++---
 libavcodec/x86/vp9itxfm_16bpp.asm       |  30 +++---
 libavcodec/x86/vp9lpf.asm               |   6 +-
 libavcodec/x86/vp9lpf_16bpp.asm         |   2 +-
 libavcodec/x86/vp9mc.asm                |  20 ++--
 libavcodec/x86/vp9mc_16bpp.asm          |  24 ++---
 libavcodec/x86/xvididct.asm             |   8 +-
 92 files changed, 782 insertions(+), 929 deletions(-)

diff --git a/libavcodec/x86/aacencdsp.asm b/libavcodec/x86/aacencdsp.asm
index 97af571ec8..2ea01868f9 100644
--- a/libavcodec/x86/aacencdsp.asm
+++ b/libavcodec/x86/aacencdsp.asm
@@ -32,7 +32,7 @@ SECTION .text
 ;void ff_abs_pow34(float *out, const float *in, const int size);
 ;*******************************************************************
 INIT_XMM sse
-cglobal abs_pow34, 3, 3, 3, out, in, size
+cglobal abs_pow34, 3, 3, 3, "p", out, "p", in, "d-", size
     mova   m2, [float_abs_mask]
     shl    sizeq, 2
     add    inq, sizeq
@@ -54,7 +54,7 @@ cglobal abs_pow34, 3, 3, 3, out, in, size
 ;                           const float rounding)
 ;*******************************************************************
 INIT_XMM sse2
-cglobal aac_quantize_bands, 5, 5, 6, out, in, scaled, size, is_signed, maxval, Q34, rounding
+cglobal aac_quantize_bands, 5, 5, 6, "p", out, "p", in, "p", scaled, "d", size, "d", is_signed, "d", maxval, Q34, rounding
 %if UNIX64 == 0
     movss     m0, Q34m
     movss     m1, roundingm
diff --git a/libavcodec/x86/aacpsdsp.asm b/libavcodec/x86/aacpsdsp.asm
index 4acd087c85..f91c3822c8 100644
--- a/libavcodec/x86/aacpsdsp.asm
+++ b/libavcodec/x86/aacpsdsp.asm
@@ -32,7 +32,7 @@ SECTION .text
 ;void ff_ps_add_squares_<opt>(float *dst, const float (*src)[2], int n);
 ;*************************************************************************
 %macro PS_ADD_SQUARES 1
-cglobal ps_add_squares, 3, 3, %1, dst, src, n
+cglobal ps_add_squares, 3, 3, %1, "p", dst, "p", src, "d", n
     shl    nd, 3
     add  srcq, nq
     neg    nq
@@ -62,7 +62,7 @@ PS_ADD_SQUARES 3
 ;                                   float *src1, int n);
 ;*******************************************************************
 INIT_XMM sse
-cglobal ps_mul_pair_single, 4, 4, 4, dst, src1, src2, n
+cglobal ps_mul_pair_single, 4, 4, 4, "p", dst, "p", src1, "p", src2, "d", n
     shl      nd, 3
     add   src1q, nq
     add    dstq, nq
@@ -91,7 +91,7 @@ align 16
 ;                                   int len);
 ;***********************************************************************
 INIT_XMM sse3
-cglobal ps_stereo_interpolate, 5, 5, 6, l, r, h, h_step, n
+cglobal ps_stereo_interpolate, 5, 5, 6, "p", l, "p", r, "p", h, "p", h_step, "d", n
     movaps   m0, [hq]
     movaps   m1, [h_stepq]
     unpcklps m4, m0, m0
@@ -124,7 +124,7 @@ align 16
 ;                                       int len);
 ;***************************************************************************
 INIT_XMM sse3
-cglobal ps_stereo_interpolate_ipdopd, 5, 5, 10, l, r, h, h_step, n
+cglobal ps_stereo_interpolate_ipdopd, 5, 5, 10, "p", l, "p", r, "p", h, "p", h_step, "d", n
     movaps   m0, [hq]
     movaps   m1, [hq+mmsize]
 %if ARCH_X86_64
@@ -172,8 +172,7 @@ align 16
 ;                                   int i, int len)
 ;**********************************************************
 INIT_XMM sse
-cglobal ps_hybrid_analysis_ileave, 3, 7, 5, out, in, i, len, in0, in1, tmp
-    movsxdifnidn        iq, id
+cglobal ps_hybrid_analysis_ileave, 3, 7, 5, "p", out, "p", in, "d-", i, len, in0, in1, tmp
     mov               lend, 32 << 3
     lea                inq, [inq+iq*4]
     mov               tmpd, id
@@ -278,13 +277,12 @@ align 16
 ;                                    int i, int len)
 ;***********************************************************
 %macro HYBRID_SYNTHESIS_DEINT 0
-cglobal ps_hybrid_synthesis_deint, 3, 7, 5, out, in, i, len, out0, out1, tmp
+cglobal ps_hybrid_synthesis_deint, 3, 7, 5, "p", out, "p", in, "d-", i, len, out0, out1, tmp
 %if cpuflag(sse4)
 %define MOVH movsd
 %else
 %define MOVH movlps
 %endif
-    movsxdifnidn        iq, id
     mov               lend, 32 << 3
     lea               outq, [outq+iq*4]
     mov               tmpd, id
@@ -432,7 +430,7 @@ HYBRID_SYNTHESIS_DEINT
 %endmacro
 
 %macro PS_HYBRID_ANALYSIS 0
-cglobal ps_hybrid_analysis, 5, 5, 8, out, in, filter, stride, n
+cglobal ps_hybrid_analysis, 5, 5, 8, "p", out, "p", in, "p", filter, "p-", stride, "d", n
 %if cpuflag(sse3)
 %define MOVH movsd
 %else
diff --git a/libavcodec/x86/ac3dsp.asm b/libavcodec/x86/ac3dsp.asm
index 675ade3101..64fcf60188 100644
--- a/libavcodec/x86/ac3dsp.asm
+++ b/libavcodec/x86/ac3dsp.asm
@@ -46,7 +46,7 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 
 %macro AC3_EXPONENT_MIN 0
-cglobal ac3_exponent_min, 3, 4, 2, exp, reuse_blks, expn, offset
+cglobal ac3_exponent_min, 3, 4, 2, "p", exp, "d-", reuse_blks, "d", expn, offset
     shl  reuse_blksq, 8
     jz .end
     LOOP_ALIGN
@@ -117,7 +117,7 @@ AC3_EXPONENT_MIN
 %endmacro
 
 %macro AC3_MAX_MSB_ABS_INT16 1
-cglobal ac3_max_msb_abs_int16, 2,2,5, src, len
+cglobal ac3_max_msb_abs_int16, 2,2,5, "p", src, "d", len
     pxor        m2, m2
     pxor        m3, m3
 .loop:
@@ -168,7 +168,7 @@ AC3_MAX_MSB_ABS_INT16 or_abs
 ;-----------------------------------------------------------------------------
 
 %macro AC3_SHIFT 3 ; l/r, 16/32, shift instruction, instruction set
-cglobal ac3_%1shift_int%2, 3, 3, 5, src, len, shift
+cglobal ac3_%1shift_int%2, 3, 3, 5, "p", src, "d", len, "d", shift
     movd      m0, shiftd
 .loop:
     mova      m1, [srcq         ]
@@ -215,7 +215,7 @@ AC3_SHIFT r, 32, psrad
 ; The 3DNow! version is not bit-identical because pf2id uses truncation rather
 ; than round-to-nearest.
 INIT_MMX 3dnow
-cglobal float_to_fixed24, 3, 3, 0, dst, src, len
+cglobal float_to_fixed24, 3, 3, 0, "p", dst, "p", src, "d", len
     movq   m0, [pf_1_24]
 .loop:
     movq   m1, [srcq   ]
@@ -242,7 +242,7 @@ cglobal float_to_fixed24, 3, 3, 0, dst, src, len
     RET
 
 INIT_XMM sse
-cglobal float_to_fixed24, 3, 3, 3, dst, src, len
+cglobal float_to_fixed24, 3, 3, 3, "p", dst, "p", src, "d", len
     movaps     m0, [pf_1_24]
 .loop:
     movaps     m1, [srcq   ]
@@ -267,7 +267,7 @@ cglobal float_to_fixed24, 3, 3, 3, dst, src, len
     RET
 
 INIT_XMM sse2
-cglobal float_to_fixed24, 3, 3, 9, dst, src, len
+cglobal float_to_fixed24, 3, 3, 9, "p", dst, "p", src, "d", len
     movaps     m0, [pf_1_24]
 .loop:
     movaps     m1, [srcq    ]
@@ -332,7 +332,7 @@ cglobal float_to_fixed24, 3, 3, 9, dst, src, len
 %endmacro
 
 INIT_XMM sse2
-cglobal ac3_compute_mantissa_size, 1, 2, 4, mant_cnt, sum
+cglobal ac3_compute_mantissa_size, 1, 2, 4, "p", mant_cnt, sum
     movdqa      m0, [mant_cntq      ]
     movdqa      m1, [mant_cntq+ 1*16]
     paddw       m0, [mant_cntq+ 2*16]
@@ -384,7 +384,7 @@ cglobal ac3_compute_mantissa_size, 1, 2, 4, mant_cnt, sum
 %endmacro
 
 %macro AC3_EXTRACT_EXPONENTS 0
-cglobal ac3_extract_exponents, 3, 3, 4, exp, coef, len
+cglobal ac3_extract_exponents, 3, 3, 4, "p", exp, "p", coef, "d-", len
     add     expq, lenq
     lea    coefq, [coefq+4*lenq]
     neg     lenq
@@ -460,9 +460,9 @@ AC3_EXTRACT_EXPONENTS
 
 %macro APPLY_WINDOW_INT16 1 ; %1 bitexact version
 %if %1
-cglobal apply_window_int16, 4,5,6, output, input, window, offset, offset2
+cglobal apply_window_int16, 4,5,6, "p", output, "p", input, "p", window, "d-", offset, offset2
 %else
-cglobal apply_window_int16_round, 4,5,6, output, input, window, offset, offset2
+cglobal apply_window_int16_round, 4,5,6, "p", output, "p", input, "p", window, "d-", offset, offset2
 %endif
     lea     offset2q, [offsetq-mmsize]
 %if cpuflag(ssse3) && notcpuflag(atom)
diff --git a/libavcodec/x86/ac3dsp_downmix.asm b/libavcodec/x86/ac3dsp_downmix.asm
index 057cc6061c..4facef30ec 100644
--- a/libavcodec/x86/ac3dsp_downmix.asm
+++ b/libavcodec/x86/ac3dsp_downmix.asm
@@ -57,15 +57,17 @@ SECTION .text
     %assign matrix_elements_stack 0
 %endif
 
-cglobal ac3_downmix_%1_to_%2, 3,in_channels+1,total_mmregs,0-matrix_elements_stack*mmsize, src0, src1, len, src2, src3, src4, src5
+cglobal ac3_downmix_%1_to_%2, 3,in_channels+1,total_mmregs,0-matrix_elements_stack*mmsize, "p", src0, "p", src1, "d", len, src2, src3, src4, src5
 
 ; load matrix pointers
+%define matrix0p r1p
+%define matrix1p r3p
 %define matrix0q r1q
 %define matrix1q r3q
 %if stereo
-    mov      matrix1q, [matrix0q+gprsize]
+    mov      matrix1p, [matrix0q+ptrsize]
 %endif
-    mov      matrix0q, [matrix0q]
+    mov      matrix0p, [matrix0q]
 
 ; define matrix coeff names
 %assign %%i 0
@@ -120,11 +122,11 @@ cglobal ac3_downmix_%1_to_%2, 3,in_channels+1,total_mmregs,0-matrix_elements_sta
     ; load channel pointers to registers
 %assign %%i 1
 %rep (in_channels - 1)
-    mov         src %+ %%i %+ q, [src0q+%%i*gprsize]
+    mov         src %+ %%i %+ p, [src0q+%%i*ptrsize]
     add         src %+ %%i %+ q, lenq
     %assign %%i %%i+1
 %endrep
-    mov         src0q, [src0q]
+    mov         src0p, [src0q]
     add         src0q, lenq
     neg          lenq
 .loop:
diff --git a/libavcodec/x86/alacdsp.asm b/libavcodec/x86/alacdsp.asm
index bb2069f785..7565fe0c17 100644
--- a/libavcodec/x86/alacdsp.asm
+++ b/libavcodec/x86/alacdsp.asm
@@ -26,17 +26,18 @@ SECTION .text
 
 INIT_XMM sse4
 %if ARCH_X86_64
-cglobal alac_decorrelate_stereo, 2, 5, 8, buf0, len, shift, weight, buf1
+cglobal alac_decorrelate_stereo, 2, 5, 8, "p", buf0, "d", len, "d", shift, "d", weight, buf1
 %else
-cglobal alac_decorrelate_stereo, 2, 3, 8, buf0, len, shift, weight
+cglobal alac_decorrelate_stereo, 2, 3, 8, "p", buf0, "d", len, "d", shift, "d", weight
+%define  buf1p  r2p
 %define  buf1q  r2q
 %endif
     movd    m6, shiftm
     movd    m7, weightm
     SPLATD  m7
     shl   lend, 2
-    mov  buf1q, [buf0q + gprsize]
-    mov  buf0q, [buf0q]
+    mov  buf1p, [buf0q + ptrsize]
+    mov  buf0p, [buf0q]
     add  buf1q, lenq
     add  buf0q, lenq
     neg  lenq
@@ -65,14 +66,13 @@ align 16
     RET
 
 INIT_XMM sse2
-cglobal alac_append_extra_bits_stereo, 2, 5, 5, buf0, exbuf0, buf1, exbuf1, len
-    movifnidn lend, lenm
+cglobal alac_append_extra_bits_stereo, 5, 5, 5, "p", buf0, "p", exbuf0, "d*", buf1, "d*", exbuf1, "d", len
     movd      m4, r2m ; exbits
     shl     lend, 2
-    mov    buf1q, [buf0q + gprsize]
-    mov    buf0q, [buf0q]
-    mov  exbuf1q, [exbuf0q + gprsize]
-    mov  exbuf0q, [exbuf0q]
+    mov    buf1p, [buf0q + ptrsize]
+    mov    buf0p, [buf0q]
+    mov  exbuf1p, [exbuf0q + ptrsize]
+    mov  exbuf0p, [exbuf0q]
     add    buf1q, lenq
     add    buf0q, lenq
     add  exbuf1q, lenq
@@ -103,16 +103,16 @@ align 16
     REP_RET
 
 %if ARCH_X86_64
-cglobal alac_append_extra_bits_mono, 2, 5, 3, buf, exbuf, exbits, ch, len
+cglobal alac_append_extra_bits_mono, 5, 5, 3, "p", buf, "p", exbuf, "d*", exbits, "d*", ch, "d", len
 %else
-cglobal alac_append_extra_bits_mono, 2, 3, 3, buf, exbuf, len
+cglobal alac_append_extra_bits_mono, 2, 3, 3, "p", buf, "p", exbuf, len
 %define exbitsm r2m
+    mov    lend, r4m
 %endif
-    movifnidn lend, r4m
     movd     m2, exbitsm
     shl    lend, 2
-    mov    bufq, [bufq]
-    mov  exbufq, [exbufq]
+    mov    bufp, [bufq]
+    mov  exbufp, [exbufq]
     add    bufq, lenq
     add  exbufq, lenq
     neg lenq
diff --git a/libavcodec/x86/audiodsp.asm b/libavcodec/x86/audiodsp.asm
index de395e5fa8..84df7e7034 100644
--- a/libavcodec/x86/audiodsp.asm
+++ b/libavcodec/x86/audiodsp.asm
@@ -25,7 +25,7 @@ SECTION .text
 
 %macro SCALARPRODUCT 0
 ; int ff_scalarproduct_int16(int16_t *v1, int16_t *v2, int order)
-cglobal scalarproduct_int16, 3,3,3, v1, v2, order
+cglobal scalarproduct_int16, 3,3,3, "p", v1, "p", v2, "d", order
     add orderd, orderd
     add v1q, orderq
     add v2q, orderq
@@ -65,7 +65,7 @@ SCALARPRODUCT
 ; %4 = CLIPD function takes min/max as float instead of int (SSE2 version)
 ; %5 = suffix
 %macro VECTOR_CLIP_INT32 4-5
-cglobal vector_clip_int32%5, 5,5,%1, dst, src, min, max, len
+cglobal vector_clip_int32%5, 5,5,%1, "p", dst, "p", src, "d", min, "d", max, "d", len
 %if %4
     cvtsi2ss  m4, minm
     cvtsi2ss  m5, maxm
@@ -132,7 +132,7 @@ VECTOR_CLIP_INT32 6, 1, 0, 0
 ; void ff_vector_clipf_sse(float *dst, const float *src,
 ;                          int len, float min, float max)
 INIT_XMM sse
-cglobal vector_clipf, 3, 3, 6, dst, src, len, min, max
+cglobal vector_clipf, 3, 3, 6, "p", dst, "p", src, "d-", len, min, max
 %if ARCH_X86_32
     VBROADCASTSS m0, minm
     VBROADCASTSS m1, maxm
@@ -145,8 +145,6 @@ cglobal vector_clipf, 3, 3, 6, dst, src, len, min, max
     VBROADCASTSS m1, m1
 %endif
 
-    movsxdifnidn lenq, lend
-
 .loop:
     mova m2, [srcq + 4 * lenq - 4 * mmsize]
     mova m3, [srcq + 4 * lenq - 3 * mmsize]
diff --git a/libavcodec/x86/blockdsp.asm b/libavcodec/x86/blockdsp.asm
index 9d203df8f5..6544b08b62 100644
--- a/libavcodec/x86/blockdsp.asm
+++ b/libavcodec/x86/blockdsp.asm
@@ -33,7 +33,7 @@ SECTION .text
 ; %1 = number of xmm registers used
 ; %2 = number of inline store loops
 %macro CLEAR_BLOCK 2
-cglobal clear_block, 1, 1, %1, blocks
+cglobal clear_block, 1, 1, %1, "p", blocks
     ZERO  m0, m0, m0
 %assign %%i 0
 %rep %2
@@ -60,7 +60,7 @@ CLEAR_BLOCK 1, 1
 ;-----------------------------------------
 ; %1 = number of xmm registers used
 %macro CLEAR_BLOCKS 1
-cglobal clear_blocks, 1, 2, %1, blocks, len
+cglobal clear_blocks, 1, 2, %1, "p", blocks, len
     add   blocksq, 768
     mov      lenq, -768
     ZERO       m0, m0, m0
diff --git a/libavcodec/x86/bswapdsp.asm b/libavcodec/x86/bswapdsp.asm
index 31c6c48a21..2568766cfd 100644
--- a/libavcodec/x86/bswapdsp.asm
+++ b/libavcodec/x86/bswapdsp.asm
@@ -102,11 +102,11 @@ SECTION .text
 ; void ff_bswap_buf(uint32_t *dst, const uint32_t *src, int w);
 %macro BSWAP32_BUF 0
 %if cpuflag(ssse3)||cpuflag(avx2)
-cglobal bswap32_buf, 3,4,3
+cglobal bswap32_buf, 3,4,3, "p", dst, "p", src, "d", w
     mov      r3, r1
     VBROADCASTI128  m2, [pb_bswap32]
 %else
-cglobal bswap32_buf, 3,4,5
+cglobal bswap32_buf, 3,4,5, "p", dst, "p", src, "d", w
     mov      r3, r1
 %endif
     or       r3, r0
diff --git a/libavcodec/x86/cavsidct.asm b/libavcodec/x86/cavsidct.asm
index 6c768c2646..17b6b5d8ad 100644
--- a/libavcodec/x86/cavsidct.asm
+++ b/libavcodec/x86/cavsidct.asm
@@ -108,7 +108,7 @@ SECTION .text
 %endmacro
 
 INIT_MMX mmx
-cglobal cavs_idct8, 2, 4, 8, 8 * 16, out, in, cnt, tmp
+cglobal cavs_idct8, 2, 4, 8, 8 * 16, "p", out, "p", in, cnt, tmp
     mov           cntd, 2
     mov           tmpq, rsp
 
@@ -170,7 +170,7 @@ cglobal cavs_idct8, 2, 4, 8, 8 * 16, out, in, cnt, tmp
     RET
 
 INIT_XMM sse2
-cglobal cavs_idct8, 2, 2, 8 + ARCH_X86_64, 0 - 8 * 16, out, in
+cglobal cavs_idct8, 2, 2, 8 + ARCH_X86_64, 0 - 8 * 16, "p", out, "p", in
     CAVS_IDCT8_1D  inq, [pw_4]
     psraw           m7, 3
     psraw           m6, 3
diff --git a/libavcodec/x86/dcadsp.asm b/libavcodec/x86/dcadsp.asm
index 055361a765..e9a86667a3 100644
--- a/libavcodec/x86/dcadsp.asm
+++ b/libavcodec/x86/dcadsp.asm
@@ -27,7 +27,7 @@ SECTION .text
 %define FMA3_OFFSET (8 * cpuflag(fma3))
 
 %macro LFE_FIR0_FLOAT 0
-cglobal lfe_fir0_float, 4, 6, 12 + cpuflag(fma3)*4, samples, lfe, coeff, nblocks, cnt1, cnt2
+cglobal lfe_fir0_float, 4, 6, 12 + cpuflag(fma3)*4, "p", samples, "p", lfe, "p", coeff, "p", nblocks, cnt1, cnt2
     shr nblocksd, 1
     sub     lfeq, 7*sizeof_float
     mov    cnt1d, 32*sizeof_float
@@ -222,7 +222,7 @@ LFE_FIR0_FLOAT
 %endif
 
 %macro LFE_FIR1_FLOAT 0
-cglobal lfe_fir1_float, 4, 6, 10, samples, lfe, coeff, nblocks, cnt1, cnt2
+cglobal lfe_fir1_float, 4, 6, 10, "p", samples, "p", lfe, "p", coeff, "p", nblocks, cnt1, cnt2
     shr nblocksd, 2
     sub     lfeq, 3*sizeof_float
     mov    cnt1d, 64*sizeof_float
diff --git a/libavcodec/x86/dct32.asm b/libavcodec/x86/dct32.asm
index 21e2f21c97..2cc54018b8 100644
--- a/libavcodec/x86/dct32.asm
+++ b/libavcodec/x86/dct32.asm
@@ -192,7 +192,7 @@ INIT_YMM avx
 SECTION .text
 %if HAVE_AVX_EXTERNAL
 ; void ff_dct32_float_avx(FFTSample *out, const FFTSample *in)
-cglobal dct32_float, 2,3,8, out, in, tmp
+cglobal dct32_float, 2,3,8, "p", out, "p", in, tmp
     ; pass 1
     vmovaps     m4, [inq+0]
     vinsertf128 m5, m5, [inq+96], 1
@@ -389,7 +389,7 @@ INIT_XMM
 
 ; void ff_dct32_float_sse(FFTSample *out, const FFTSample *in)
 %macro DCT32_FUNC 0
-cglobal dct32_float, 2, 3, 16, out, in, tmp
+cglobal dct32_float, 2, 3, 16, "p", out, "p", in, tmp
     ; pass 1
 
     movaps      m0, [inq+0]
diff --git a/libavcodec/x86/dirac_dwt.asm b/libavcodec/x86/dirac_dwt.asm
index 22a5c2bbbb..e684664687 100644
--- a/libavcodec/x86/dirac_dwt.asm
+++ b/libavcodec/x86/dirac_dwt.asm
@@ -63,11 +63,8 @@ SECTION .text
 %macro COMPOSE_VERTICAL 1
 ; void vertical_compose53iL0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
 ;                                  int width)
-cglobal vertical_compose53iL0_%1, 4,4,1, b0, b1, b2, width
+cglobal vertical_compose53iL0_%1, 4,4,1, "p", b0, "p", b1, "p", b2, "d+", width
     mova    m2, [pw_2]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
 .loop:
     sub     widthq, mmsize/2
     mova    m1, [b0q+2*widthq]
@@ -79,11 +76,8 @@ cglobal vertical_compose53iL0_%1, 4,4,1, b0, b1, b2, width
 
 ; void vertical_compose_dirac53iH0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
 ;                                  int width)
-cglobal vertical_compose_dirac53iH0_%1, 4,4,1, b0, b1, b2, width
+cglobal vertical_compose_dirac53iH0_%1, 4,4,1, "p", b0, "p", b1, "p", b2, "d+", width
     mova    m1, [pw_1]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
 .loop:
     sub     widthq, mmsize/2
     mova    m0, [b0q+2*widthq]
@@ -97,12 +91,9 @@ cglobal vertical_compose_dirac53iH0_%1, 4,4,1, b0, b1, b2, width
 
 ; void vertical_compose_dd97iH0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
 ;                               IDWTELEM *b3, IDWTELEM *b4, int width)
-cglobal vertical_compose_dd97iH0_%1, 6,6,5, b0, b1, b2, b3, b4, width
+cglobal vertical_compose_dd97iH0_%1, 6,6,5, "p", b0, "p", b1, "p", b2, "p", b3, "p", b4, "d+", width
     mova    m3, [pw_8]
     mova    m4, [pw_1991]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
 .loop:
     sub     widthq, mmsize/2
     mova    m0, [b0q+2*widthq]
@@ -114,12 +105,9 @@ cglobal vertical_compose_dd97iH0_%1, 6,6,5, b0, b1, b2, b3, b4, width
 
 ; void vertical_compose_dd137iL0(IDWTELEM *b0, IDWTELEM *b1, IDWTELEM *b2,
 ;                                IDWTELEM *b3, IDWTELEM *b4, int width)
-cglobal vertical_compose_dd137iL0_%1, 6,6,6, b0, b1, b2, b3, b4, width
+cglobal vertical_compose_dd137iL0_%1, 6,6,6, "p", b0, "p", b1, "p", b2, "p", b3, "p", b4, "d+", width
     mova    m3, [pw_16]
     mova    m4, [pw_1991]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
 .loop:
     sub     widthq, mmsize/2
     mova    m0, [b0q+2*widthq]
@@ -142,11 +130,8 @@ cglobal vertical_compose_dd137iL0_%1, 6,6,6, b0, b1, b2, b3, b4, width
     REP_RET
 
 ; void vertical_compose_haar(IDWTELEM *b0, IDWTELEM *b1, int width)
-cglobal vertical_compose_haar_%1, 3,4,3, b0, b1, width
+cglobal vertical_compose_haar_%1, 3,4,3, "p", b0, "p", b1, "d+", width
     mova    m3, [pw_1]
-%if ARCH_X86_64
-    mov     widthd, widthd
-%endif
 .loop:
     sub     widthq, mmsize/2
     mova    m1, [b1q+2*widthq]
@@ -181,7 +166,7 @@ cglobal vertical_compose_haar_%1, 3,4,3, b0, b1, width
 
 %macro HAAR_HORIZONTAL 2
 ; void horizontal_compose_haari(IDWTELEM *b, IDWTELEM *tmp, int width)
-cglobal horizontal_compose_haar%2i_%1, 3,6,4, b, tmp, w, x, w2, b_w2
+cglobal horizontal_compose_haar%2i_%1, 3,6,4, "p", b, "p", tmp, "d+", w, x, w2, b_w2
     mov    w2d, wd
     xor     xq, xq
     shr    w2d, 1
@@ -231,7 +216,7 @@ cglobal horizontal_compose_haar%2i_%1, 3,6,4, b, tmp, w, x, w2, b_w2
 
 INIT_XMM
 ; void horizontal_compose_dd97i(IDWTELEM *b, IDWTELEM *tmp, int width)
-cglobal horizontal_compose_dd97i_ssse3, 3,6,8, b, tmp, w, x, w2, b_w2
+cglobal horizontal_compose_dd97i_ssse3, 3,6,8, "p", b, "p", tmp, "d+", w, x, w2, b_w2
     mov    w2d, wd
     xor     xd, xd
     shr    w2d, 1
diff --git a/libavcodec/x86/diracdsp.asm b/libavcodec/x86/diracdsp.asm
index cc8a26fca5..94a40611a8 100644
--- a/libavcodec/x86/diracdsp.asm
+++ b/libavcodec/x86/diracdsp.asm
@@ -47,7 +47,7 @@ SECTION .text
 
 %macro HPEL_FILTER 1
 ; dirac_hpel_filter_v_sse2(uint8_t *dst, uint8_t *src, int stride, int width);
-cglobal dirac_hpel_filter_v_%1, 4,6,8, dst, src, stride, width, src0, stridex3
+cglobal dirac_hpel_filter_v_%1, 4,6,8, "p", dst, "p", src, "d-", stride, "d", width, src0, stridex3
     mov     src0q, srcq
     lea     stridex3q, [3*strideq]
     sub     src0q, stridex3q
@@ -91,7 +91,7 @@ cglobal dirac_hpel_filter_v_%1, 4,6,8, dst, src, stride, width, src0, stridex3
     RET
 
 ; dirac_hpel_filter_h_sse2(uint8_t *dst, uint8_t *src, int width);
-cglobal dirac_hpel_filter_h_%1, 3,3,8, dst, src, width
+cglobal dirac_hpel_filter_h_%1, 3,3,8, "p", dst, "p", src, "d", width
     dec     widthd
     pxor    m7, m7
     and     widthd, ~(mmsize-1)
@@ -133,14 +133,12 @@ cglobal dirac_hpel_filter_h_%1, 3,3,8, dst, src, width
 
 %macro PUT_RECT 1
 ; void put_rect_clamped(uint8_t *dst, int dst_stride, int16_t *src, int src_stride, int width, int height)
-cglobal put_signed_rect_clamped_%1, 5,9,3, dst, dst_stride, src, src_stride, w, dst2, src2
+cglobal put_signed_rect_clamped_%1, 5,9,3, "p", dst, "d-", dst_stride, "p", src, "d-", src_stride, "d", w, dst2, src2
     mova    m0, [pb_80]
     add     wd, (mmsize-1)
     and     wd, ~(mmsize-1)
 
 %if ARCH_X86_64
-    movsxd   dst_strideq, dst_strided
-    movsxd   src_strideq, src_strided
     mov   r7d, r5m
     mov   r8d, wd
     %define wspill r8d
@@ -176,14 +174,12 @@ cglobal put_signed_rect_clamped_%1, 5,9,3, dst, dst_stride, src, src_stride, w,
 
 %macro ADD_RECT 1
 ; void add_rect_clamped(uint8_t *dst, uint16_t *src, int stride, int16_t *idwt, int idwt_stride, int width, int height)
-cglobal add_rect_clamped_%1, 7,9,3, dst, src, stride, idwt, idwt_stride, w, h
+cglobal add_rect_clamped_%1, 7,9,3, "p", dst, "p", src, "d-", stride, "p", idwt, "d-", idwt_stride, "d", w, "d", h
     mova    m0, [pw_32]
     add     wd, (mmsize-1)
     and     wd, ~(mmsize-1)
 
 %if ARCH_X86_64
-    movsxd   strideq, strided
-    movsxd   idwt_strideq, idwt_strided
     mov   r8d, wd
     %define wspill r8d
 %else
@@ -216,7 +212,7 @@ cglobal add_rect_clamped_%1, 7,9,3, dst, src, stride, idwt, idwt_stride, w, h
 
 %macro ADD_OBMC 2
 ; void add_obmc(uint16_t *dst, uint8_t *src, int stride, uint8_t *obmc_weight, int yblen)
-cglobal add_dirac_obmc%1_%2, 6,6,5, dst, src, stride, obmc, yblen
+cglobal add_dirac_obmc%1_%2, 6,6,5, "p", dst, "p", src, "d-", stride, "p", obmc, "d", yblen
     pxor        m4, m4
 .loop:
 %assign i 0
@@ -269,7 +265,7 @@ ADD_OBMC 16, sse2
 INIT_XMM sse4
 
 ; void dequant_subband_32(uint8_t *src, uint8_t *dst, ptrdiff_t stride, const int qf, const int qs, int tot_v, int tot_h)
-cglobal dequant_subband_32, 7, 7, 4, src, dst, stride, qf, qs, tot_v, tot_h
+cglobal dequant_subband_32, 7, 7, 4, "p", src, "p", dst, "p-", stride, "d", qf, "d", qs, "d", tot_v, "d", tot_h
     movd   m2, qfd
     movd   m3, qsd
     SPLATD m2
@@ -306,9 +302,9 @@ cglobal dequant_subband_32, 7, 7, 4, src, dst, stride, qf, qs, tot_v, tot_h
 INIT_XMM sse4
 ; void put_signed_rect_clamped_10(uint8_t *dst, int dst_stride, const uint8_t *src, int src_stride, int width, int height)
 %if ARCH_X86_64
-cglobal put_signed_rect_clamped_10, 6, 8, 5, dst, dst_stride, src, src_stride, w, h, t1, t2
+cglobal put_signed_rect_clamped_10, 6, 8, 5, "p", dst, "d-", dst_stride, "p", src, "d-", src_stride, "d", w, "d", h, t1, t2
 %else
-cglobal put_signed_rect_clamped_10, 5, 7, 5, dst, dst_stride, src, src_stride, w, t1, t2
+cglobal put_signed_rect_clamped_10, 5, 7, 5, "p", dst, "d-", dst_stride, "p", src, "d-", src_stride, "d", w, t1, t2
     %define  hd  r5mp
 %endif
     shl      wd, 2
diff --git a/libavcodec/x86/dnxhdenc.asm b/libavcodec/x86/dnxhdenc.asm
index b4f759552e..2b8f896410 100644
--- a/libavcodec/x86/dnxhdenc.asm
+++ b/libavcodec/x86/dnxhdenc.asm
@@ -27,7 +27,7 @@ SECTION .text
 ; void get_pixels_8x4_sym_sse2(int16_t *block, const uint8_t *pixels,
 ;                              ptrdiff_t line_size)
 INIT_XMM sse2
-cglobal get_pixels_8x4_sym, 3,3,5, block, pixels, linesize
+cglobal get_pixels_8x4_sym, 3,3,5, "p", block, "p", pixels, "p-", linesize
     pxor      m4,       m4
     movq      m0,       [pixelsq]
     add       pixelsq,  linesizeq
diff --git a/libavcodec/x86/exrdsp.asm b/libavcodec/x86/exrdsp.asm
index 3bf240c8b1..028ee553ee 100644
--- a/libavcodec/x86/exrdsp.asm
+++ b/libavcodec/x86/exrdsp.asm
@@ -36,7 +36,7 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 %macro REORDER_PIXELS 0
-cglobal reorder_pixels, 3,4,3, dst, src1, size, src2
+cglobal reorder_pixels, 3,4,3, "p", dst, "p", src1, "p-", size, src2
     lea                              src2q, [src1q+sizeq] ; src2 = src + 2 * half_size
     add                               dstq, sizeq         ; dst offset by size
     shr                              sizeq, 1             ; half_size
@@ -72,7 +72,7 @@ REORDER_PIXELS
 ;------------------------------------------------------------------------------
 
 %macro PREDICTOR 0
-cglobal predictor, 2,2,5, src, size
+cglobal predictor, 2,2,5, "p", src, "p-", size
     mova             m0, [pb_80]
     mova            xm1, [pb_15]
     mova            xm2, xm0
diff --git a/libavcodec/x86/fft.asm b/libavcodec/x86/fft.asm
index a671e8f48e..9b6c5cc4c7 100644
--- a/libavcodec/x86/fft.asm
+++ b/libavcodec/x86/fft.asm
@@ -30,25 +30,19 @@
 
 %include "libavutil/x86/x86util.asm"
 
-%if ARCH_X86_64
-%define pointer resq
-%else
-%define pointer resd
-%endif
-
 struc FFTContext
     .nbits:    resd 1
     .reverse:  resd 1
-    .revtab:   pointer 1
-    .tmpbuf:   pointer 1
+    .revtab:   resp 1
+    .tmpbuf:   resp 1
     .mdctsize: resd 1
     .mdctbits: resd 1
-    .tcos:     pointer 1
-    .tsin:     pointer 1
-    .fftperm:  pointer 1
-    .fftcalc:  pointer 1
-    .imdctcalc:pointer 1
-    .imdcthalf:pointer 1
+    .tcos:     resp 1
+    .tsin:     resp 1
+    .fftperm:  resp 1
+    .fftcalc:  resp 1
+    .imdctcalc:resp 1
+    .imdcthalf:resp 1
 endstruc
 
 SECTION_RODATA 32
@@ -78,12 +72,6 @@ cextern cos_ %+ i
 %assign i i<<1
 %endrep
 
-%if ARCH_X86_64
-    %define pointer dq
-%else
-    %define pointer dd
-%endif
-
 %macro IF0 1+
 %endmacro
 %macro IF1 1+
@@ -547,7 +535,7 @@ DEFINE_ARGS zc, w, n, o1, o3
 
 %macro FFT_DISPATCH 2; clobbers 5 GPRs, 8 XMMs
     lea r2, [dispatch_tab%1]
-    mov r2, [r2 + (%2q-2)*gprsize]
+    mov r2p, [r2 + (%2q-2)*ptrsize]
 %ifdef PIC
     lea r3, [$$]
     add r2, r3
@@ -561,7 +549,7 @@ INIT_YMM avx
 DECL_PASS pass_avx, PASS_BIG 1
 DECL_PASS pass_interleave_avx, PASS_BIG 0
 
-cglobal fft_calc, 2,5,8
+cglobal fft_calc, 2,5,8, "p", s, "p", z
     mov     r3d, [r0 + FFTContext.nbits]
     mov     r0, r1
     mov     r1, r3
@@ -576,7 +564,7 @@ DECL_PASS pass_sse, PASS_BIG 1
 DECL_PASS pass_interleave_sse, PASS_BIG 0
 
 %macro FFT_CALC_FUNC 0
-cglobal fft_calc, 2,5,8
+cglobal fft_calc, 2,5,8, "p", s, "p", z
     mov     r3d, [r0 + FFTContext.nbits]
     PUSH    r1
     PUSH    r3
@@ -623,9 +611,9 @@ FFT_CALC_FUNC
 INIT_XMM sse
 FFT_CALC_FUNC
 
-cglobal fft_permute, 2,7,1
-    mov     r4,  [r0 + FFTContext.revtab]
-    mov     r5,  [r0 + FFTContext.tmpbuf]
+cglobal fft_permute, 2,7,1, "p", s, "p", z
+    mov     r4p, [r0 + FFTContext.revtab]
+    mov     r5p, [r0 + FFTContext.tmpbuf]
     mov     ecx, [r0 + FFTContext.nbits]
     mov     r2, 1
     shl     r2, cl
@@ -657,9 +645,9 @@ cglobal fft_permute, 2,7,1
     REP_RET
 
 %macro IMDCT_CALC_FUNC 0
-cglobal imdct_calc, 3,5,3
+cglobal imdct_calc, 3,5,3, "p", s, "p", output, "p", input
     mov     r3d, [r0 + FFTContext.mdctsize]
-    mov     r4,  [r0 + FFTContext.imdcthalf]
+    mov     r4p, [r0 + FFTContext.imdcthalf]
     add     r1,  r3
     PUSH    r3
     PUSH    r1
@@ -774,7 +762,7 @@ fft %+ n %+ fullsuffix:
 %undef n
 
 align 8
-dispatch_tab %+ fullsuffix: pointer list_of_fft
+dispatch_tab %+ fullsuffix: dp list_of_fft
 %endmacro ; DECL_FFT
 
 %if HAVE_AVX_EXTERNAL
@@ -953,21 +941,27 @@ INIT_XMM sse
 %endmacro
 
 %macro DECL_IMDCT 0
-cglobal imdct_half, 3,12,8; FFTContext *s, FFTSample *output, const FFTSample *input
+cglobal imdct_half, 3,12,8, "p", s, "p", output, "p", input
 %if ARCH_X86_64
 %define rrevtab r7
 %define rtcos   r8
 %define rtsin   r9
+%define rrevtabp r7p
+%define rtcosp  r8p
+%define rtsinp  r9p
 %else
 %define rrevtab r6
 %define rtsin   r6
 %define rtcos   r5
+%define rrevtabp r6p
+%define rtsinp  r6p
+%define rtcosp  r5p
 %endif
     mov   r3d, [r0+FFTContext.mdctsize]
     add   r2, r3
     shr   r3, 1
-    mov   rtcos, [r0+FFTContext.tcos]
-    mov   rtsin, [r0+FFTContext.tsin]
+    mov   rtcosp, [r0+FFTContext.tcos]
+    mov   rtsinp, [r0+FFTContext.tsin]
     add   rtcos, r3
     add   rtsin, r3
 %if ARCH_X86_64 == 0
@@ -975,7 +969,7 @@ cglobal imdct_half, 3,12,8; FFTContext *s, FFTSample *output, const FFTSample *i
     push  rtsin
 %endif
     shr   r3, 1
-    mov   rrevtab, [r0+FFTContext.revtab]
+    mov   rrevtabp, [r0+FFTContext.revtab]
     add   rrevtab, r3
 %if ARCH_X86_64 == 0
     push  rrevtab
diff --git a/libavcodec/x86/flac_dsp_gpl.asm b/libavcodec/x86/flac_dsp_gpl.asm
index e285158185..9aa80e3a93 100644
--- a/libavcodec/x86/flac_dsp_gpl.asm
+++ b/libavcodec/x86/flac_dsp_gpl.asm
@@ -26,13 +26,11 @@ SECTION .text
 
 INIT_XMM sse4
 %if ARCH_X86_64
-    cglobal flac_enc_lpc_16, 5, 7, 8, 0, res, smp, len, order, coefs
+    cglobal flac_enc_lpc_16, 5, 7, 8, 0, "p", res, "p", smp, "d", len, "d-", order, "p", coefs
     DECLARE_REG_TMP 5, 6
     %define length r2d
-
-    movsxd orderq, orderd
 %else
-    cglobal flac_enc_lpc_16, 5, 6, 8, 0, res, smp, len, order, coefs
+    cglobal flac_enc_lpc_16, 5, 6, 8, 0, "p", res, "p", smp, "d", len, "d", order, "p", coefs
     DECLARE_REG_TMP 2, 5
     %define length r2mp
 %endif
diff --git a/libavcodec/x86/flacdsp.asm b/libavcodec/x86/flacdsp.asm
index 7138611526..1f63770cd7 100644
--- a/libavcodec/x86/flacdsp.asm
+++ b/libavcodec/x86/flacdsp.asm
@@ -36,7 +36,7 @@ SECTION .text
 
 %macro LPC_32 1
 INIT_XMM %1
-cglobal flac_lpc_32, 5,6,5, decoded, coeffs, pred_order, qlevel, len, j
+cglobal flac_lpc_32, 5,6,5, "p", decoded, "p", coeffs, "d-", pred_order, "d", qlevel, "d", len, j
     sub    lend, pred_orderd
     jle .ret
     lea    decodedq, [decodedq+pred_orderq*4-8]
@@ -88,15 +88,12 @@ LPC_32 sse4
 ;                                                   int len, int shift);
 ;----------------------------------------------------------------------------------
 %macro FLAC_DECORRELATE_16 3-4
-cglobal flac_decorrelate_%1_16, 2, 4, 4, out, in0, in1, len
-%if ARCH_X86_32
-    mov      lend, lenm
-%endif
+cglobal flac_decorrelate_%1_16, 4, 4, 4, "p", out, "p", in0, "d*", in1, "d", len
     movd       m3, r4m
     shl      lend, 2
-    mov      in1q, [in0q + gprsize]
-    mov      in0q, [in0q]
-    mov      outq, [outq]
+    mov      in1p, [in0q + ptrsize]
+    mov      in0p, [in0q]
+    mov      outp, [outq]
     add      in1q, lenq
     add      in0q, lenq
     add      outq, lenq
@@ -133,14 +130,11 @@ FLAC_DECORRELATE_16 ms, 2, 0, add
 ;                                        int len, int shift);
 ;----------------------------------------------------------------------------------
 %macro FLAC_DECORRELATE_32 5
-cglobal flac_decorrelate_%1_32, 2, 4, 4, out, in0, in1, len
-%if ARCH_X86_32
-    mov      lend, lenm
-%endif
+cglobal flac_decorrelate_%1_32, 4, 4, 4, "p", out, "p", in0, "d*", in1, "d", len
     movd       m3, r4m
-    mov      in1q, [in0q + gprsize]
-    mov      in0q, [in0q]
-    mov      outq, [outq]
+    mov      in1p, [in0q + ptrsize]
+    mov      in0p, [in0q]
+    mov      outp, [outq]
     sub      in1q, in0q
 
 align 16
@@ -182,10 +176,10 @@ FLAC_DECORRELATE_32 ms, 2, 0, 1, add
 ;%4 = word/dword (shift instruction)
 %macro FLAC_DECORRELATE_INDEP 4
 %define REPCOUNT %2/(32/%1) ; 16bits = channels / 2; 32bits = channels
-cglobal flac_decorrelate_indep%2_%1, 2, %2+2, %3+1, out, in0, in1, len, in2, in3, in4, in5, in6, in7
+cglobal flac_decorrelate_indep%2_%1, 2, %2+2, %3+1, "p", out, "p", in0, in1, "d", len, in2, in3, in4, in5, in6, in7
 %if ARCH_X86_32
 %if %2 == 6
-    DEFINE_ARGS out, in0, in1, in2, in3, in4, in5
+    DEFINE_ARGS "p", out, "p", in0, in1, in2, in3, in4, in5
     %define  lend  dword r3m
 %else
     mov      lend, lenm
@@ -195,12 +189,12 @@ cglobal flac_decorrelate_indep%2_%1, 2, %2+2, %3+1, out, in0, in1, len, in2, in3
 
 %assign %%i 1
 %rep %2-1
-    mov      in %+ %%i %+ q, [in0q+%%i*gprsize]
+    mov      in %+ %%i %+ p, [in0q+%%i*ptrsize]
 %assign %%i %%i+1
 %endrep
 
-    mov      in0q, [in0q]
-    mov      outq, [outq]
+    mov      in0p, [in0q]
+    mov      outp, [outq]
 
 %assign %%i 1
 %rep %2-1
diff --git a/libavcodec/x86/fmtconvert.asm b/libavcodec/x86/fmtconvert.asm
index 8f62a0a093..f47ab1f890 100644
--- a/libavcodec/x86/fmtconvert.asm
+++ b/libavcodec/x86/fmtconvert.asm
@@ -29,9 +29,9 @@ SECTION .text
 ;------------------------------------------------------------------------------
 %macro INT32_TO_FLOAT_FMUL_SCALAR 1
 %if UNIX64
-cglobal int32_to_float_fmul_scalar, 3, 3, %1, dst, src, len
+cglobal int32_to_float_fmul_scalar, 3, 3, %1, "p", dst, "p", src, "d", len
 %else
-cglobal int32_to_float_fmul_scalar, 4, 4, %1, dst, src, mul, len
+cglobal int32_to_float_fmul_scalar, 4, 4, %1, "p", dst, "p", src, "d*", mul, "d", len
 %endif
 %if WIN64
     SWAP 0, 2
@@ -81,7 +81,7 @@ INT32_TO_FLOAT_FMUL_SCALAR 3
 ;                                    const float *mul, int len);
 ;------------------------------------------------------------------------------
 %macro INT32_TO_FLOAT_FMUL_ARRAY8 0
-cglobal int32_to_float_fmul_array8, 5, 5, 5, c, dst, src, mul, len
+cglobal int32_to_float_fmul_array8, 5, 5, 5, "p", c, "p", dst, "p", src, "p", mul, "d", len
     shl     lend, 2
     add     srcq, lenq
     add     dstq, lenq
diff --git a/libavcodec/x86/fpel.asm b/libavcodec/x86/fpel.asm
index 961a1587a7..4a65d49c43 100644
--- a/libavcodec/x86/fpel.asm
+++ b/libavcodec/x86/fpel.asm
@@ -47,7 +47,7 @@ SECTION .text
 %define SAVE mova
 %define LEN  %2
 %endif
-cglobal %1_pixels%2, 4,5,4
+cglobal %1_pixels%2, 4,5,4, "p", block, "p", pixels, "p-", line_size, "d", h
     lea          r4, [r2*3]
 %ifidn %1, avg
 %if notcpuflag(mmxext)
diff --git a/libavcodec/x86/g722dsp.asm b/libavcodec/x86/g722dsp.asm
index a529422262..de17323932 100644
--- a/libavcodec/x86/g722dsp.asm
+++ b/libavcodec/x86/g722dsp.asm
@@ -32,7 +32,7 @@ pw_qmf_coeffs4: dw  53,    0,  -11,    0,  -11,    0,   3,    0
 SECTION .text
 
 INIT_XMM sse2
-cglobal g722_apply_qmf, 2, 2, 5, prev, out
+cglobal g722_apply_qmf, 2, 2, 5, "p", prev, "p", out
     movu m0, [prevq+mmsize*0]
     movu m1, [prevq+mmsize*1]
     movu m2, [prevq+mmsize*2]
diff --git a/libavcodec/x86/h263_loopfilter.asm b/libavcodec/x86/h263_loopfilter.asm
index 77c8cf154d..300af4fc73 100644
--- a/libavcodec/x86/h263_loopfilter.asm
+++ b/libavcodec/x86/h263_loopfilter.asm
@@ -102,10 +102,7 @@ SECTION .text
 
 INIT_MMX mmx
 ; void ff_h263_v_loop_filter_mmx(uint8_t *src, int stride, int qscale)
-cglobal h263_v_loop_filter, 3,5
-    movsxdifnidn r1, r1d
-    movsxdifnidn r2, r2d
-
+cglobal h263_v_loop_filter, 3,5, "p", src, "d-", stride, "d-", qscale
     lea          r4, [h263_loop_filter_strength]
     movzx       r3d, BYTE [r4+r2]
     movsx        r2, r3b
@@ -144,10 +141,7 @@ cglobal h263_v_loop_filter, 3,5
 
 ; void ff_h263_h_loop_filter_mmx(uint8_t *src, int stride, int qscale)
 INIT_MMX mmx
-cglobal h263_h_loop_filter, 3,5,0,32
-    movsxdifnidn r1, r1d
-    movsxdifnidn r2, r2d
-
+cglobal h263_h_loop_filter, 3,5,0,32, "p", src, "d-", stride, "d-", qscale
     lea          r4, [h263_loop_filter_strength]
     movzx       r3d, BYTE [r4+r2]
     movsx        r2, r3b
diff --git a/libavcodec/x86/h264_chromamc.asm b/libavcodec/x86/h264_chromamc.asm
index b5a78b537d..d45a9495e7 100644
--- a/libavcodec/x86/h264_chromamc.asm
+++ b/libavcodec/x86/h264_chromamc.asm
@@ -106,7 +106,7 @@ SECTION .text
 ; void ff_put/avg_h264_chroma_mc8_*(uint8_t *dst /* align 8 */,
 ;                                   uint8_t *src /* align 1 */,
 ;                                   ptrdiff_t stride, int h, int mx, int my)
-cglobal %1_%2_chroma_mc8%3, 6, 7 + extra_regs, 0
+cglobal %1_%2_chroma_mc8%3, 6, 7 + extra_regs, 0, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov          r6d, r5d
     or           r6d, r4d
     jne .at_least_one_non_zero
@@ -287,7 +287,7 @@ cglobal %1_%2_chroma_mc8%3, 6, 7 + extra_regs, 0
 %define extra_regs 1
 %endif ; PIC
 %endif ; rv40
-cglobal %1_%2_chroma_mc4, 6, 6 + extra_regs, 0
+cglobal %1_%2_chroma_mc4, 6, 6 + extra_regs, 0, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     pxor          m7, m7
     movd          m2, r4d         ; x
     movd          m3, r5d         ; y
@@ -369,7 +369,7 @@ cglobal %1_%2_chroma_mc4, 6, 6 + extra_regs, 0
 %endmacro
 
 %macro chroma_mc2_mmx_func 2
-cglobal %1_%2_chroma_mc2, 6, 7, 0
+cglobal %1_%2_chroma_mc2, 6, 7, 0, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov          r6d, r4d
     shl          r4d, 16
     sub          r4d, r6d
@@ -454,7 +454,7 @@ chroma_mc4_mmx_func avg, h264
 chroma_mc4_mmx_func avg, rv40
 
 %macro chroma_mc8_ssse3_func 2-3
-cglobal %1_%2_chroma_mc8%3, 6, 7, 8
+cglobal %1_%2_chroma_mc8%3, 6, 7, 8, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov          r6d, r5d
     or           r6d, r4d
     jne .at_least_one_non_zero
@@ -599,7 +599,7 @@ cglobal %1_%2_chroma_mc8%3, 6, 7, 8
 %endmacro
 
 %macro chroma_mc4_ssse3_func 2
-cglobal %1_%2_chroma_mc4, 6, 7, 0
+cglobal %1_%2_chroma_mc4, 6, 7, 0, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov           r6, r4
     shl          r4d, 8
     sub          r4d, r6d
diff --git a/libavcodec/x86/h264_chromamc_10bit.asm b/libavcodec/x86/h264_chromamc_10bit.asm
index 34bc41969b..db55f6b2eb 100644
--- a/libavcodec/x86/h264_chromamc_10bit.asm
+++ b/libavcodec/x86/h264_chromamc_10bit.asm
@@ -61,7 +61,7 @@ SECTION .text
 ;                                 int h, int mx, int my)
 ;-----------------------------------------------------------------------------
 %macro CHROMA_MC8 1
-cglobal %1_h264_chroma_mc8_10, 6,7,8
+cglobal %1_h264_chroma_mc8_10, 6,7,8, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov          r6d, r5d
     or           r6d, r4d
     jne .at_least_one_non_zero
@@ -172,7 +172,7 @@ cglobal %1_h264_chroma_mc8_10, 6,7,8
 %endmacro
 
 %macro CHROMA_MC4 1
-cglobal %1_h264_chroma_mc4_10, 6,6,7
+cglobal %1_h264_chroma_mc4_10, 6,6,7, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     movd          m2, r4m         ; x
     movd          m3, r5m         ; y
     mova          m4, [pw_8]
@@ -202,7 +202,7 @@ cglobal %1_h264_chroma_mc4_10, 6,6,7
 ;                                 int h, int mx, int my)
 ;-----------------------------------------------------------------------------
 %macro CHROMA_MC2 1
-cglobal %1_h264_chroma_mc2_10, 6,7
+cglobal %1_h264_chroma_mc2_10, 6,7, "p", dst, "p", src, "p-", stride, "d", h, "d", mx, "d", my
     mov          r6d, r4d
     shl          r4d, 16
     sub          r4d, r6d
diff --git a/libavcodec/x86/h264_deblock.asm b/libavcodec/x86/h264_deblock.asm
index 6702ae98d4..67fec508fc 100644
--- a/libavcodec/x86/h264_deblock.asm
+++ b/libavcodec/x86/h264_deblock.asm
@@ -282,7 +282,7 @@ cextern pb_3
 ;                        int8_t *tc0)
 ;-----------------------------------------------------------------------------
 %macro DEBLOCK_LUMA 0
-cglobal deblock_v_luma_8, 5,5,10, pix_, stride_, alpha_, beta_, base3_
+cglobal deblock_v_luma_8, 5,5,10, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", base3_
     movd    m8, [r4] ; tc0
     lea     r4, [stride_q*3]
     dec     alpha_d        ; alpha-1
@@ -328,7 +328,7 @@ cglobal deblock_v_luma_8, 5,5,10, pix_, stride_, alpha_, beta_, base3_
 ;                        int8_t *tc0)
 ;-----------------------------------------------------------------------------
 INIT_MMX cpuname
-cglobal deblock_h_luma_8, 5,9,0,0x60+16*WIN64
+cglobal deblock_h_luma_8, 5,9,0,0x60+16*WIN64, "p", pix_, "d", stride_, "d", alpha_, "d", beta_, "p", base3_
     movsxd r7,  r1d
     lea    r8,  [r7+r7*2]
     lea    r6,  [r0-4]
@@ -379,8 +379,7 @@ cglobal deblock_h_luma_8, 5,9,0,0x60+16*WIN64
 
 %macro DEBLOCK_H_LUMA_MBAFF 0
 
-cglobal deblock_h_luma_mbaff_8, 5, 9, 10, 8*16, pix_, stride_, alpha_, beta_, tc0_, base3_, stride3_
-    movsxd stride_q,   stride_d
+cglobal deblock_h_luma_mbaff_8, 5, 9, 10, 8*16, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_, base3_, stride3_
     dec    alpha_d
     dec    beta_d
     mov    base3_q,    pix_q
@@ -480,7 +479,7 @@ DEBLOCK_LUMA
 ; void ff_deblock_v8_luma(uint8_t *pix, int stride, int alpha, int beta,
 ;                         int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_%1_luma_8, 5,5,8,2*%2
+cglobal deblock_%1_luma_8, 5,5,8,2*%2, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     lea     r4, [r1*3]
     dec     r2     ; alpha-1
     neg     r4
@@ -530,7 +529,7 @@ cglobal deblock_%1_luma_8, 5,5,8,2*%2
 ;                        int8_t *tc0)
 ;-----------------------------------------------------------------------------
 INIT_MMX cpuname
-cglobal deblock_h_luma_8, 0,5,8,0x60+12
+cglobal deblock_h_luma_8, 0,5,8,0x60+12, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     mov    r0, r0mp
     mov    r3, r1m
     lea    r4, [r3*3]
@@ -730,9 +729,9 @@ DEBLOCK_LUMA v, 16
 ; void ff_deblock_v_luma_intra(uint8_t *pix, int stride, int alpha, int beta)
 ;-----------------------------------------------------------------------------
 %if WIN64
-cglobal deblock_%1_luma_intra_8, 4,6,16,0x10
+cglobal deblock_%1_luma_intra_8, 4,6,16,0x10, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
 %else
-cglobal deblock_%1_luma_intra_8, 4,6,16,ARCH_X86_64*0x50-0x50
+cglobal deblock_%1_luma_intra_8, 4,6,16,ARCH_X86_64*0x50-0x50, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
 %endif
     lea     r4, [r1*4]
     lea     r5, [r1*3] ; 3*stride
@@ -789,7 +788,7 @@ INIT_MMX cpuname
 ;-----------------------------------------------------------------------------
 ; void ff_deblock_h_luma_intra(uint8_t *pix, int stride, int alpha, int beta)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_8, 4,9,0,0x80
+cglobal deblock_h_luma_intra_8, 4,9,0,0x80, "p", pix_, "d", stride_, "d", alpha_, "d", beta_
     movsxd r7,  r1d
     lea    r8,  [r7*3]
     lea    r6,  [r0-4]
@@ -820,7 +819,7 @@ cglobal deblock_h_luma_intra_8, 4,9,0,0x80
     TRANSPOSE8x8_MEM  PASS8ROWS(pix_tmp, pix_tmp+0x30, 0x10, 0x30), PASS8ROWS(r6, r5, r7, r8)
     RET
 %else
-cglobal deblock_h_luma_intra_8, 2,4,8,0x80
+cglobal deblock_h_luma_intra_8, 2,4,8,0x80, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     lea    r3,  [r1*3]
     sub    r0,  4
     lea    r2,  [r0+r3]
@@ -895,7 +894,7 @@ INIT_MMX mmxext
 ; void ff_deblock_v_chroma(uint8_t *pix, int stride, int alpha, int beta,
 ;                          int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_8, 5,6
+cglobal deblock_v_chroma_8, 5,6, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_V_START
     movq  m0, [t5]
     movq  m1, [t5+r1]
@@ -910,7 +909,7 @@ cglobal deblock_v_chroma_8, 5,6
 ; void ff_deblock_h_chroma(uint8_t *pix, int stride, int alpha, int beta,
 ;                          int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma_8, 5,7
+cglobal deblock_h_chroma_8, 5,7, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
 %if ARCH_X86_64
     ; This could use the red zone on 64 bit unix to avoid the stack pointer
     ; readjustment, but valgrind assumes the red zone is clobbered on
@@ -951,7 +950,7 @@ ff_chroma_inter_body_mmxext:
 %define t5 r4
 %define t6 r5
 
-cglobal deblock_h_chroma422_8, 5, 6
+cglobal deblock_h_chroma422_8, 5, 6, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     SUB rsp, (1+ARCH_X86_64*2)*mmsize
     %if ARCH_X86_64
         %define buf0 [rsp+16]
@@ -1009,7 +1008,7 @@ RET
 ;------------------------------------------------------------------------------
 ; void ff_deblock_v_chroma_intra(uint8_t *pix, int stride, int alpha, int beta)
 ;------------------------------------------------------------------------------
-cglobal deblock_v_chroma_intra_8, 4,5
+cglobal deblock_v_chroma_intra_8, 4,5, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     CHROMA_V_START
     movq  m0, [t5]
     movq  m1, [t5+r1]
@@ -1023,14 +1022,14 @@ cglobal deblock_v_chroma_intra_8, 4,5
 ;------------------------------------------------------------------------------
 ; void ff_deblock_h_chroma_intra(uint8_t *pix, int stride, int alpha, int beta)
 ;------------------------------------------------------------------------------
-cglobal deblock_h_chroma_intra_8, 4,6
+cglobal deblock_h_chroma_intra_8, 4,6, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     CHROMA_H_START
     TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
     call ff_chroma_intra_body_mmxext
     TRANSPOSE8x4B_STORE PASS8ROWS(t5, r0, r1, t6)
     RET
 
-cglobal deblock_h_chroma422_intra_8, 4, 6
+cglobal deblock_h_chroma422_intra_8, 4, 6, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     CHROMA_H_START
     TRANSPOSE4x8_LOAD  bw, wd, dq, PASS8ROWS(t5, r0, r1, t6)
     call ff_chroma_intra_body_mmxext
@@ -1161,7 +1160,7 @@ ff_chroma_intra_body_mmxext:
 
 INIT_XMM %1
 
-cglobal deblock_v_chroma_8, 5, 6, 8, pix_, stride_, alpha_, beta_, tc0_
+cglobal deblock_v_chroma_8, 5, 6, 8, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_V_START_XMM r5
     movq m0, [r5]
     movq m1, [r5 + stride_q]
@@ -1172,7 +1171,7 @@ cglobal deblock_v_chroma_8, 5, 6, 8, pix_, stride_, alpha_, beta_, tc0_
     movq [pix_q], m2
 RET
 
-cglobal deblock_h_chroma_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_
+cglobal deblock_h_chroma_8, 5, 7, 8, 0-16, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_H_START_XMM r5, r6
     LOAD_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
     TRANSPOSE_8x4B_XMM
@@ -1185,7 +1184,7 @@ cglobal deblock_h_chroma_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_
     STORE_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
 RET
 
-cglobal deblock_h_chroma422_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_,
+cglobal deblock_h_chroma422_8, 5, 7, 8, 0-16, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_H_START_XMM r5, r6
     LOAD_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
     TRANSPOSE_8x4B_XMM
@@ -1212,7 +1211,7 @@ cglobal deblock_h_chroma422_8, 5, 7, 8, 0-16, pix_, stride_, alpha_, beta_, tc0_
     STORE_8_ROWS PASS8ROWS(pix_q - 2, r5 - 2, stride_q, r6)
 RET
 
-cglobal deblock_v_chroma_intra_8, 4, 5, 8, pix_, stride_, alpha_, beta_
+cglobal deblock_v_chroma_intra_8, 4, 5, 8, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_V_START_XMM r4
     movq m0, [r4]
     movq m1, [r4 + stride_q]
@@ -1223,7 +1222,7 @@ cglobal deblock_v_chroma_intra_8, 4, 5, 8, pix_, stride_, alpha_, beta_
     movq [pix_q], m2
 RET
 
-cglobal deblock_h_chroma_intra_8, 4, 6, 8, pix_, stride_, alpha_, beta_
+cglobal deblock_h_chroma_intra_8, 4, 6, 8, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_H_START_XMM r4, r5
     LOAD_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
     TRANSPOSE_8x4B_XMM
@@ -1232,7 +1231,7 @@ cglobal deblock_h_chroma_intra_8, 4, 6, 8, pix_, stride_, alpha_, beta_
     STORE_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
 RET
 
-cglobal deblock_h_chroma422_intra_8, 4, 6, 8, pix_, stride_, alpha_, beta_
+cglobal deblock_h_chroma422_intra_8, 4, 6, 8, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     CHROMA_H_START_XMM r4, r5
     LOAD_8_ROWS PASS8ROWS(pix_q - 2, r4 - 2, stride_q, r5)
     TRANSPOSE_8x4B_XMM
@@ -1366,8 +1365,8 @@ DEBLOCK_CHROMA_XMM avx
 %endmacro
 
 INIT_MMX mmxext
-cglobal h264_loop_filter_strength, 9, 9, 0, bs, nnz, ref, mv, bidir, edges, \
-                                            step, mask_mv0, mask_mv1, field
+cglobal h264_loop_filter_strength, 9, 9, 0, "p", bs, "p", nnz, "p", ref, "p", mv, "d", bidir, "d", edges, \
+                                            "d", step, "d", mask_mv0, "d", mask_mv1, "d", field
 %define b_idxq bidirq
 %define b_idxd bidird
     cmp    dword fieldm, 0
diff --git a/libavcodec/x86/h264_deblock_10bit.asm b/libavcodec/x86/h264_deblock_10bit.asm
index 1af3257a67..494bc2f309 100644
--- a/libavcodec/x86/h264_deblock_10bit.asm
+++ b/libavcodec/x86/h264_deblock_10bit.asm
@@ -153,7 +153,7 @@ cextern pw_1023
 ; void ff_deblock_v_luma_10(uint16_t *pix, int stride, int alpha, int beta,
 ;                           int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_v_luma_10, 5,5,8*(mmsize/16)
+cglobal deblock_v_luma_10, 5,5,8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     %assign pad 5*mmsize+12-(stack_offset&15)
     %define tcm [rsp]
     %define ms1 [rsp+mmsize]
@@ -208,7 +208,7 @@ cglobal deblock_v_luma_10, 5,5,8*(mmsize/16)
     ADD         rsp, pad
     RET
 
-cglobal deblock_h_luma_10, 5,6,8*(mmsize/16)
+cglobal deblock_h_luma_10, 5,6,8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     %assign pad 7*mmsize+12-(stack_offset&15)
     %define tcm [rsp]
     %define ms1 [rsp+mmsize]
@@ -337,7 +337,7 @@ cglobal deblock_h_luma_10, 5,6,8*(mmsize/16)
 %endmacro
 
 %macro DEBLOCK_LUMA_64 0
-cglobal deblock_v_luma_10, 5,5,15
+cglobal deblock_v_luma_10, 5,5,15, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     %define p2 m8
     %define p1 m0
     %define p0 m1
@@ -374,7 +374,7 @@ cglobal deblock_v_luma_10, 5,5,15
     jg .loop
     REP_RET
 
-cglobal deblock_h_luma_10, 5,7,15
+cglobal deblock_h_luma_10, 5,7,15, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     shl        r2d, 2
     shl        r3d, 2
     LOAD_AB    m12, m13, r2d, r3d
@@ -603,7 +603,7 @@ DEBLOCK_LUMA_64
 ;                                 int beta)
 ;-----------------------------------------------------------------------------
 %macro DEBLOCK_LUMA_INTRA_64 0
-cglobal deblock_v_luma_intra_10, 4,7,16
+cglobal deblock_v_luma_intra_10, 4,7,16, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     %define t0 m1
     %define t1 m2
     %define t2 m4
@@ -654,7 +654,7 @@ cglobal deblock_v_luma_intra_10, 4,7,16
 ; void ff_deblock_h_luma_intra_10(uint16_t *pix, int stride, int alpha,
 ;                                 int beta)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_10, 4,7,16
+cglobal deblock_h_luma_intra_10, 4,7,16, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     %define t0 m15
     %define t1 m14
     %define t2 m2
@@ -727,7 +727,7 @@ DEBLOCK_LUMA_INTRA_64
 ; void ff_deblock_v_luma_intra_10(uint16_t *pix, int stride, int alpha,
 ;                                 int beta)
 ;-----------------------------------------------------------------------------
-cglobal deblock_v_luma_intra_10, 4,7,8*(mmsize/16)
+cglobal deblock_v_luma_intra_10, 4,7,8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     LUMA_INTRA_INIT 3
     lea     r4, [r1*4]
     lea     r5, [r1*3]
@@ -756,7 +756,7 @@ cglobal deblock_v_luma_intra_10, 4,7,8*(mmsize/16)
 ; void ff_deblock_h_luma_intra_10(uint16_t *pix, int stride, int alpha,
 ;                                 int beta)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_luma_intra_10, 4,7,8*(mmsize/16)
+cglobal deblock_h_luma_intra_10, 4,7,8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     LUMA_INTRA_INIT 8
 %if mmsize == 8
     lea     r4, [r1*3]
@@ -932,7 +932,7 @@ DEBLOCK_LUMA_INTRA
 ; void ff_deblock_v_chroma_10(uint16_t *pix, int stride, int alpha, int beta,
 ;                             int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_10, 5,7-(mmsize/16),8*(mmsize/16)
+cglobal deblock_v_chroma_10, 5,7-(mmsize/16),8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     mov         r5, r0
     sub         r0, r1
     sub         r0, r1
@@ -967,7 +967,7 @@ cglobal deblock_v_chroma_10, 5,7-(mmsize/16),8*(mmsize/16)
 ; void ff_deblock_v_chroma_intra_10(uint16_t *pix, int stride, int alpha,
 ;                                   int beta)
 ;-----------------------------------------------------------------------------
-cglobal deblock_v_chroma_intra_10, 4,6-(mmsize/16),8*(mmsize/16)
+cglobal deblock_v_chroma_intra_10, 4,6-(mmsize/16),8*(mmsize/16), "p", pix_, "d-", stride_, "d", alpha_, "d", beta_
     mov         r4, r0
     sub         r0, r1
     sub         r0, r1
@@ -996,7 +996,7 @@ cglobal deblock_v_chroma_intra_10, 4,6-(mmsize/16),8*(mmsize/16)
 ; void ff_deblock_h_chroma_10(uint16_t *pix, int stride, int alpha, int beta,
 ;                             int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma_10, 5, 7, 8, 0-2*mmsize, pix_, stride_, alpha_, beta_, tc0_
+cglobal deblock_h_chroma_10, 5, 7, 8, 0-2*mmsize, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     shl alpha_d,  2
     shl beta_d,   2
     mov r5,       pix_q
@@ -1031,7 +1031,7 @@ RET
 ; void ff_deblock_h_chroma422_10(uint16_t *pix, int stride, int alpha, int beta,
 ;                                int8_t *tc0)
 ;-----------------------------------------------------------------------------
-cglobal deblock_h_chroma422_10, 5, 7, 8, 0-3*mmsize, pix_, stride_, alpha_, beta_, tc0_
+cglobal deblock_h_chroma422_10, 5, 7, 8, 0-3*mmsize, "p", pix_, "d-", stride_, "d", alpha_, "d", beta_, "p", tc0_
     shl alpha_d,  2
     shl beta_d,   2
 
diff --git a/libavcodec/x86/h264_idct.asm b/libavcodec/x86/h264_idct.asm
index c54f9f1a68..42d8d48c45 100644
--- a/libavcodec/x86/h264_idct.asm
+++ b/libavcodec/x86/h264_idct.asm
@@ -89,8 +89,7 @@ SECTION .text
 
 INIT_MMX mmx
 ; void ff_h264_idct_add_8_mmx(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct_add_8, 3, 3, 0
-    movsxdifnidn r2, r2d
+cglobal h264_idct_add_8, 3, 3, 0, "p", dst, "p", block, "d-", stride
     IDCT4_ADD    r0, r1, r2
     RET
 
@@ -209,8 +208,7 @@ cglobal h264_idct_add_8, 3, 3, 0
 
 INIT_MMX mmx
 ; void ff_h264_idct8_add_8_mmx(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_add_8, 3, 4, 0
-    movsxdifnidn r2, r2d
+cglobal h264_idct8_add_8, 3, 4, 0, "p", dst, "p", block, "d-", stride
     %assign pad 128+4-(stack_offset&7)
     SUB         rsp, pad
 
@@ -278,8 +276,7 @@ cglobal h264_idct8_add_8, 3, 4, 0
 
 INIT_XMM sse2
 ; void ff_h264_idct8_add_8_sse2(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_add_8, 3, 4, 10
-    movsxdifnidn  r2, r2d
+cglobal h264_idct8_add_8, 3, 4, 10, "p", dst, "p", block, "d-", stride
     IDCT8_ADD_SSE r0, r1, r2, r3
     RET
 
@@ -317,8 +314,7 @@ cglobal h264_idct8_add_8, 3, 4, 10
 INIT_MMX mmxext
 ; void ff_h264_idct_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
 %if ARCH_X86_64
-cglobal h264_idct_dc_add_8, 3, 4, 0
-    movsxd       r2, r2d
+cglobal h264_idct_dc_add_8, 3, 4, 0, "p", dst, "p", block, "d-", stride
     movsx        r3, word [r1]
     mov  dword [r1], 0
     DC_ADD_MMXEXT_INIT r3, r2
@@ -326,8 +322,7 @@ cglobal h264_idct_dc_add_8, 3, 4, 0
     RET
 
 ; void ff_h264_idct8_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_dc_add_8, 3, 4, 0
-    movsxd       r2, r2d
+cglobal h264_idct8_dc_add_8, 3, 4, 0, "p", dst, "p", block, "d-", stride
     movsx        r3, word [r1]
     mov  dword [r1], 0
     DC_ADD_MMXEXT_INIT r3, r2
@@ -337,7 +332,7 @@ cglobal h264_idct8_dc_add_8, 3, 4, 0
     RET
 %else
 ; void ff_h264_idct_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct_dc_add_8, 2, 3, 0
+cglobal h264_idct_dc_add_8, 2, 3, 0, "p", dst, "p", block, "d*", stride
     movsx        r2, word [r1]
     mov  dword [r1], 0
     mov          r1, r2m
@@ -346,7 +341,7 @@ cglobal h264_idct_dc_add_8, 2, 3, 0
     RET
 
 ; void ff_h264_idct8_dc_add_8_mmxext(uint8_t *dst, int16_t *block, int stride)
-cglobal h264_idct8_dc_add_8, 2, 3, 0
+cglobal h264_idct8_dc_add_8, 2, 3, 0, "p", dst, "p", block, "d*", stride
     movsx        r2, word [r1]
     mov  dword [r1], 0
     mov          r1, r2m
@@ -361,8 +356,7 @@ INIT_MMX mmx
 ; void ff_h264_idct_add16_8_mmx(uint8_t *dst, const int *block_offset,
 ;                               int16_t *block, int stride,
 ;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16_8, 5, 7 + npicregs, 0, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, picreg
     xor          r5, r5
 %ifdef PIC
     lea     picregq, [scan8_mem]
@@ -385,8 +379,7 @@ cglobal h264_idct_add16_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride,
 ; void ff_h264_idct8_add4_8_mmx(uint8_t *dst, const int *block_offset,
 ;                               int16_t *block, int stride,
 ;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct8_add4_8, 5, 7 + npicregs, 0, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, picreg
     %assign pad 128+4-(stack_offset&7)
     SUB         rsp, pad
 
@@ -420,8 +413,7 @@ INIT_MMX mmxext
 ; void ff_h264_idct_add16_8_mmxext(uint8_t *dst, const int *block_offset,
 ;                                  int16_t *block, int stride,
 ;                                  const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16_8, 5, 8 + npicregs, 0, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     xor          r5, r5
 %ifdef PIC
     lea     picregq, [scan8_mem]
@@ -468,8 +460,7 @@ INIT_MMX mmx
 ; void ff_h264_idct_add16intra_8_mmx(uint8_t *dst, const int *block_offset,
 ;                                    int16_t *block, int stride,
 ;                                    const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 7 + npicregs, 0, dst, block_offset, block, stride, nnzc, cntr, coeff, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16intra_8, 5, 7 + npicregs, 0, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, picreg
     xor          r5, r5
 %ifdef PIC
     lea     picregq, [scan8_mem]
@@ -494,8 +485,7 @@ INIT_MMX mmxext
 ; void ff_h264_idct_add16intra_8_mmxext(uint8_t *dst, const int *block_offset,
 ;                                       int16_t *block, int stride,
 ;                                       const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16intra_8, 5, 8 + npicregs, 0, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     xor          r5, r5
 %ifdef PIC
     lea     picregq, [scan8_mem]
@@ -539,8 +529,7 @@ cglobal h264_idct_add16intra_8, 5, 8 + npicregs, 0, dst1, block_offset, block, s
 ; void ff_h264_idct8_add4_8_mmxext(uint8_t *dst, const int *block_offset,
 ;                                  int16_t *block, int stride,
 ;                                  const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct8_add4_8, 5, 8 + npicregs, 0, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     %assign pad 128+4-(stack_offset&7)
     SUB         rsp, pad
 
@@ -602,8 +591,7 @@ INIT_XMM sse2
 ; void ff_h264_idct8_add4_8_sse2(uint8_t *dst, const int *block_offset,
 ;                                int16_t *block, int stride,
 ;                                const uint8_t nnzc[6 * 8])
-cglobal h264_idct8_add4_8, 5, 8 + npicregs, 10, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct8_add4_8, 5, 8 + npicregs, 10, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     xor          r5, r5
 %ifdef PIC
     lea     picregq, [scan8_mem]
@@ -664,7 +652,7 @@ h264_idct_add8_mmx_plane:
     jz .skipblock
 %if ARCH_X86_64
     mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
+    add         r0p, [dst2q]
 %else
     mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
     mov          r0, [r0]
@@ -681,8 +669,7 @@ h264_idct_add8_mmx_plane:
 ; void ff_h264_idct_add8_8_mmx(uint8_t **dest, const int *block_offset,
 ;                              int16_t *block, int stride,
 ;                              const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     mov          r5, 16
     add          r2, 512
 %ifdef PIC
@@ -695,16 +682,15 @@ cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride,
     mov          r5, 32
     add          r2, 384
 %if ARCH_X86_64
-    add       dst2q, gprsize
+    add       dst2q, ptrsize
 %else
-    add        r0mp, gprsize
+    add        r0mp, ptrsize
 %endif
     call         h264_idct_add8_mmx_plane
     RET ; TODO: check rep ret after a function call
 
-cglobal h264_idct_add8_422_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
+cglobal h264_idct_add8_422_8, 5, 8 + npicregs, 0, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
 ; dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
 %ifdef PIC
     lea     picregq, [scan8_mem]
 %endif
@@ -720,9 +706,9 @@ cglobal h264_idct_add8_422_8, 5, 8 + npicregs, 0, dst1, block_offset, block, str
     call         h264_idct_add8_mmx_plane
 
 %if ARCH_X86_64
-    add       dst2q, gprsize ; dest[1]
+    add       dst2q, ptrsize ; dest[1]
 %else
-    add        r0mp, gprsize
+    add        r0mp, ptrsize
 %endif
 
     add r5, 4   ; set to 32
@@ -743,7 +729,7 @@ h264_idct_add8_mmxext_plane:
     jz .try_dc
 %if ARCH_X86_64
     mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
+    add         r0p, [dst2q]
 %else
     mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
     mov          r0, [r0]
@@ -763,7 +749,7 @@ h264_idct_add8_mmxext_plane:
     DC_ADD_MMXEXT_INIT r6, r3
 %if ARCH_X86_64
     mov         r0d, dword [r1+r5*4]
-    add          r0, [dst2q]
+    add         r0p, [dst2q]
 %else
     mov          r0, r1m ; XXX r1m here is actually r0m of the calling func
     mov          r0, [r0]
@@ -781,8 +767,7 @@ INIT_MMX mmxext
 ; void ff_h264_idct_add8_8_mmxext(uint8_t **dest, const int *block_offset,
 ;                                 int16_t *block, int stride,
 ;                                 const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride, nnzc, cntr, coeff, dst2, picreg
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, "p", dst1, "p", block_offset, "p", block, "d-", stride, "p", nnzc, cntr, coeff, dst2, picreg
     mov          r5, 16
     add          r2, 512
 %if ARCH_X86_64
@@ -795,9 +780,9 @@ cglobal h264_idct_add8_8, 5, 8 + npicregs, 0, dst1, block_offset, block, stride,
     mov          r5, 32
     add          r2, 384
 %if ARCH_X86_64
-    add       dst2q, gprsize
+    add       dst2q, ptrsize
 %else
-    add        r0mp, gprsize
+    add        r0mp, ptrsize
 %endif
     call h264_idct_add8_mmxext_plane
     RET ; TODO: check rep ret after a function call
@@ -868,8 +853,7 @@ h264_add8x4_idct_sse2:
 ; void ff_h264_idct_add16_8_sse2(uint8_t *dst, const int *block_offset,
 ;                                int16_t *block, int stride,
 ;                                const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16_8, 5, 5 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16_8, 5, 5 + ARCH_X86_64, 8, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
 %if ARCH_X86_64
     mov         r5, r0
 %endif
@@ -917,8 +901,7 @@ REP_RET
 ; void ff_h264_idct_add16intra_8_sse2(uint8_t *dst, const int *block_offset,
 ;                                     int16_t *block, int stride,
 ;                                     const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add16intra_8, 5, 7 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16intra_8, 5, 7 + ARCH_X86_64, 8, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
 %if ARCH_X86_64
     mov         r7, r0
 %endif
@@ -938,7 +921,7 @@ REP_RET
     jz .try%1dc
 %if ARCH_X86_64
     mov        r0d, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-    add         r0, [r7]
+    add        r0p, [r7]
 %else
     mov         r0, r0m
     mov         r0, [r0]
@@ -952,7 +935,7 @@ REP_RET
     jz .cycle%1end
 %if ARCH_X86_64
     mov        r0d, dword [r1+(%1&1)*8+64*(1+(%1>>1))]
-    add         r0, [r7]
+    add        r0p, [r7]
 %else
     mov         r0, r0m
     mov         r0, [r0]
@@ -970,8 +953,7 @@ REP_RET
 ; void ff_h264_idct_add8_8_sse2(uint8_t **dest, const int *block_offset,
 ;                               int16_t *block, int stride,
 ;                               const uint8_t nnzc[6 * 8])
-cglobal h264_idct_add8_8, 5, 7 + ARCH_X86_64, 8
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add8_8, 5, 7 + ARCH_X86_64, 8, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
     add          r2, 512
 %if ARCH_X86_64
     mov          r7, r0
@@ -979,9 +961,9 @@ cglobal h264_idct_add8_8, 5, 7 + ARCH_X86_64, 8
     add8_sse2_cycle 0, 0x34
     add8_sse2_cycle 1, 0x3c
 %if ARCH_X86_64
-    add          r7, gprsize
+    add          r7, ptrsize
 %else
-    add        r0mp, gprsize
+    add        r0mp, ptrsize
 %endif
     add8_sse2_cycle 2, 0x5c
     add8_sse2_cycle 3, 0x64
@@ -1092,7 +1074,7 @@ REP_RET
 %endmacro
 
 %macro IDCT_DC_DEQUANT 1
-cglobal h264_luma_dc_dequant_idct, 3, 4, %1
+cglobal h264_luma_dc_dequant_idct, 3, 4, %1, "p", output, "p", input, "d", qmul
     ; manually spill XMM registers for Win64 because
     ; the code here is initialized with INIT_MMX
     WIN64_SPILL_XMM %1
@@ -1180,13 +1162,11 @@ IDCT_DC_DEQUANT 7
 
 INIT_XMM %1
 
-cglobal h264_idct_add_8, 3, 3, 8, dst_, block_, stride_
-    movsxdifnidn stride_q, stride_d
+cglobal h264_idct_add_8, 3, 3, 8, "p", dst_, "p", block_, "d-", stride_
     IDCT4_ADD    dst_q, block_q, stride_q
 RET
 
-cglobal h264_idct_dc_add_8, 3, 4, 6, dst_, block_, stride_
-    movsxdifnidn stride_q, stride_d
+cglobal h264_idct_dc_add_8, 3, 4, 6, "p", dst_, "p", block_, "d-", stride_
     movsx             r3d, word [block_q]
     mov   dword [block_q], 0
     DC_ADD_INIT r3
diff --git a/libavcodec/x86/h264_idct_10bit.asm b/libavcodec/x86/h264_idct_10bit.asm
index 9fd05abb2b..41cee544c0 100644
--- a/libavcodec/x86/h264_idct_10bit.asm
+++ b/libavcodec/x86/h264_idct_10bit.asm
@@ -75,8 +75,7 @@ cextern pd_32
 %endmacro
 
 %macro IDCT_ADD_10 0
-cglobal h264_idct_add_10, 3,3
-    movsxdifnidn r2, r2d
+cglobal h264_idct_add_10, 3, 3, "p", dst, "p", block, "d-", stride
     IDCT4_ADD_10 r0, r1, r2
     RET
 %endmacro
@@ -137,8 +136,7 @@ ADD4x4IDCT
 %endmacro
 
 %macro IDCT_ADD16_10 0
-cglobal h264_idct_add16_10, 5,6
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16_10, 5, 6, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
     ADD16_OP 0, 4+1*8
     ADD16_OP 1, 5+1*8
     ADD16_OP 2, 4+2*8
@@ -196,8 +194,7 @@ IDCT_ADD16_10
 %endmacro
 
 INIT_MMX mmxext
-cglobal h264_idct_dc_add_10,3,3
-    movsxdifnidn r2, r2d
+cglobal h264_idct_dc_add_10, 3, 3, "p", dst, "p", block, "d-", stride
     movd      m0, [r1]
     mov dword [r1], 0
     paddd     m0, [pd_32]
@@ -212,8 +209,7 @@ cglobal h264_idct_dc_add_10,3,3
 ; void ff_h264_idct8_dc_add_10(pixel *dst, int16_t *block, int stride)
 ;-----------------------------------------------------------------------------
 %macro IDCT8_DC_ADD 0
-cglobal h264_idct8_dc_add_10,3,4,7
-    movsxdifnidn r2, r2d
+cglobal h264_idct8_dc_add_10, 3, 4, 7, "p", dst, "p", block, "d-", stride
     movd      m0, [r1]
     mov dword[r1], 0
     paddd     m0, [pd_32]
@@ -282,8 +278,7 @@ idct_dc_add %+ SUFFIX:
     IDCT_DC_ADD_OP_10 r5, r3, r6
     ret
 
-cglobal h264_idct_add16intra_10,5,7,8
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add16intra_10, 5, 7, 8, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
     ADD16_OP_INTRA 0, 4+1*8
     ADD16_OP_INTRA 2, 4+2*8
     ADD16_OP_INTRA 4, 6+1*8
@@ -317,21 +312,20 @@ IDCT_ADD16INTRA_10
 ;                           const uint8_t nnzc[6*8])
 ;-----------------------------------------------------------------------------
 %macro IDCT_ADD8 0
-cglobal h264_idct_add8_10,5,8,7
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add8_10, 5, 8, 7, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
 %if ARCH_X86_64
     mov      r7, r0
 %endif
     add      r2, 1024
-    mov      r0, [r0]
+    mov      r0p, [r0]
     ADD16_OP_INTRA 16, 4+ 6*8
     ADD16_OP_INTRA 18, 4+ 7*8
     add      r2, 1024-128*2
 %if ARCH_X86_64
-    mov      r0, [r7+gprsize]
+    mov      r0p, [r7+ptrsize]
 %else
-    mov      r0, r0m
-    mov      r0, [r0+gprsize]
+    mov      r0p, r0mp
+    mov      r0p, [r0+ptrsize]
 %endif
     ADD16_OP_INTRA 32, 4+11*8
     ADD16_OP_INTRA 34, 4+12*8
@@ -359,14 +353,13 @@ IDCT_ADD8
 
 %macro IDCT_ADD8_422 0
 
-cglobal h264_idct_add8_422_10, 5, 8, 7
-    movsxdifnidn r3, r3d
+cglobal h264_idct_add8_422_10, 5, 8, 7, "p", dst, "p", block_offset, "p", block, "d-", stride, "p", nnzc
 %if ARCH_X86_64
     mov      r7, r0
 %endif
 
     add      r2, 1024
-    mov      r0, [r0]
+    mov      r0p, [r0]
     ADD16_OP_INTRA 16, 4+ 6*8
     ADD16_OP_INTRA 18, 4+ 7*8
     ADD16_OP_INTRA 24, 4+ 8*8 ; i+4
@@ -374,10 +367,10 @@ cglobal h264_idct_add8_422_10, 5, 8, 7
     add      r2, 1024-128*4
 
 %if ARCH_X86_64
-    mov      r0, [r7+gprsize]
+    mov      r0p, [r7+ptrsize]
 %else
-    mov      r0, r0m
-    mov      r0, [r0+gprsize]
+    mov      r0p, r0mp
+    mov      r0p, [r0+ptrsize]
 %endif
 
     ADD16_OP_INTRA 32, 4+11*8
@@ -507,8 +500,7 @@ IDCT_ADD8_422
 %endmacro
 
 %macro IDCT8_ADD 0
-cglobal h264_idct8_add_10, 3,4,16
-    movsxdifnidn r2, r2d
+cglobal h264_idct8_add_10, 3, 4, 16, "p", dst, "p", block, "d-", stride
 %if UNIX64 == 0
     %assign pad 16-gprsize-(stack_offset&15)
     sub  rsp, pad
@@ -632,15 +624,14 @@ IDCT8_ADD
 %endmacro
 
 %macro IDCT8_ADD4 0
-cglobal h264_idct8_add4_10, 0,7,16
-    movsxdifnidn r3, r3d
+cglobal h264_idct8_add4_10, 5, 7, 16, "p*", dst, "p*", block_offset, "p*", block, "d-*", stride, "p", nnzc
     %assign pad 16-gprsize-(stack_offset&15)
     SUB      rsp, pad
-    mov       r5, r0mp
-    mov       r6, r1mp
-    mov       r1, r2mp
-    mov      r2d, r3m
-    movifnidn r4, r4mp
+    ASSIGN_ARG dst, 5
+    ASSIGN_ARG block_offset, 6
+    ASSIGN_ARG block, 1
+    ASSIGN_ARG stride, 2
+    LOAD_ARG dst, block_offset, block, stride
     IDCT8_ADD4_OP  0, 4+1*8
     IDCT8_ADD4_OP  4, 6+1*8
     IDCT8_ADD4_OP  8, 4+3*8
diff --git a/libavcodec/x86/h264_intrapred.asm b/libavcodec/x86/h264_intrapred.asm
index f3aa3172f0..4bf2c0cb68 100644
--- a/libavcodec/x86/h264_intrapred.asm
+++ b/libavcodec/x86/h264_intrapred.asm
@@ -53,7 +53,7 @@ cextern pw_32
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmx
-cglobal pred16x16_vertical_8, 2,3
+cglobal pred16x16_vertical_8, 2,3, "p", src, "p-", stride
     sub   r0, r1
     mov   r2, 8
     movq mm0, [r0+0]
@@ -69,7 +69,7 @@ cglobal pred16x16_vertical_8, 2,3
     REP_RET
 
 INIT_XMM sse
-cglobal pred16x16_vertical_8, 2,3
+cglobal pred16x16_vertical_8, 2,3, "p", src, "p-", stride
     sub   r0, r1
     mov   r2, 4
     movaps xmm0, [r0]
@@ -89,7 +89,7 @@ cglobal pred16x16_vertical_8, 2,3
 ;-----------------------------------------------------------------------------
 
 %macro PRED16x16_H 0
-cglobal pred16x16_horizontal_8, 2,3
+cglobal pred16x16_horizontal_8, 2,3, "p", src, "p-", stride
     mov       r2, 8
 %if cpuflag(ssse3)
     mova      m2, [pb_3]
@@ -130,7 +130,7 @@ PRED16x16_H
 ;-----------------------------------------------------------------------------
 
 %macro PRED16x16_DC 0
-cglobal pred16x16_dc_8, 2,7
+cglobal pred16x16_dc_8, 2,7, "p", src, "p-", stride
     mov       r4, r0
     sub       r0, r1
     pxor      mm0, mm0
@@ -192,7 +192,7 @@ PRED16x16_DC
 ;-----------------------------------------------------------------------------
 
 %macro PRED16x16_TM 0
-cglobal pred16x16_tm_vp8_8, 2,5
+cglobal pred16x16_tm_vp8_8, 2,5, "p", src, "p-", stride
     sub        r0, r1
     pxor      mm7, mm7
     movq      mm0, [r0+0]
@@ -233,7 +233,7 @@ INIT_MMX mmxext
 PRED16x16_TM
 
 INIT_XMM sse2
-cglobal pred16x16_tm_vp8_8, 2,6,6
+cglobal pred16x16_tm_vp8_8, 2,6,6, "p", src, "p-", stride
     sub          r0, r1
     pxor       xmm2, xmm2
     movdqa     xmm0, [r0]
@@ -270,7 +270,7 @@ cglobal pred16x16_tm_vp8_8, 2,6,6
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal pred16x16_tm_vp8_8, 2, 4, 5, dst, stride, stride3, iteration
+cglobal pred16x16_tm_vp8_8, 2, 4, 5, "p", dst, "p-", stride, stride3, iteration
     sub                       dstq, strideq
     pmovzxbw                    m0, [dstq]
     vpbroadcastb               xm1, [r0-1]
@@ -310,7 +310,7 @@ cglobal pred16x16_tm_vp8_8, 2, 4, 5, dst, stride, stride3, iteration
 ;-----------------------------------------------------------------------------
 
 %macro H264_PRED16x16_PLANE 1
-cglobal pred16x16_plane_%1_8, 2,9,7
+cglobal pred16x16_plane_%1_8, 2,9,7, "p", src, "p-", stride
     mov          r2, r1           ; +stride
     neg          r1               ; -stride
 
@@ -591,7 +591,7 @@ H264_PRED16x16_PLANE svq3
 ;-----------------------------------------------------------------------------
 
 %macro H264_PRED8x8_PLANE 0
-cglobal pred8x8_plane_8, 2,9,7
+cglobal pred8x8_plane_8, 2,9,7, "p", src, "p-", stride
     mov          r2, r1           ; +stride
     neg          r1               ; -stride
 
@@ -765,7 +765,7 @@ H264_PRED8x8_PLANE
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmx
-cglobal pred8x8_vertical_8, 2,2
+cglobal pred8x8_vertical_8, 2,2, "p", src, "p-", stride
     sub    r0, r1
     movq  mm0, [r0]
 %rep 3
@@ -782,7 +782,7 @@ cglobal pred8x8_vertical_8, 2,2
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8_H 0
-cglobal pred8x8_horizontal_8, 2,3
+cglobal pred8x8_horizontal_8, 2,3, "p", src, "p-", stride
     mov       r2, 4
 %if cpuflag(ssse3)
     mova      m2, [pb_3]
@@ -809,7 +809,7 @@ PRED8x8_H
 ; void ff_pred8x8_top_dc_8_mmxext(uint8_t *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal pred8x8_top_dc_8, 2,5
+cglobal pred8x8_top_dc_8, 2,5, "p", src, "p-", stride
     sub         r0, r1
     movq       mm0, [r0]
     pxor       mm1, mm1
@@ -844,7 +844,7 @@ cglobal pred8x8_top_dc_8, 2,5
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8_dc_8, 2,5
+cglobal pred8x8_dc_8, 2,5, "p", src, "p-", stride
     sub       r0, r1
     pxor      m7, m7
     movd      m0, [r0+0]
@@ -905,7 +905,7 @@ cglobal pred8x8_dc_8, 2,5
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8_dc_rv40_8, 2,7
+cglobal pred8x8_dc_rv40_8, 2,7, "p", src, "p-", stride
     mov       r4, r0
     sub       r0, r1
     pxor      mm0, mm0
@@ -942,7 +942,7 @@ cglobal pred8x8_dc_rv40_8, 2,7
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8_TM 0
-cglobal pred8x8_tm_vp8_8, 2,6
+cglobal pred8x8_tm_vp8_8, 2,6, "p", src, "p-", stride
     sub        r0, r1
     pxor      mm7, mm7
     movq      mm0, [r0]
@@ -982,7 +982,7 @@ INIT_MMX mmxext
 PRED8x8_TM
 
 INIT_XMM sse2
-cglobal pred8x8_tm_vp8_8, 2,6,4
+cglobal pred8x8_tm_vp8_8, 2,6,4, "p", src, "p-", stride
     sub          r0, r1
     pxor       xmm1, xmm1
     movq       xmm0, [r0]
@@ -1011,7 +1011,7 @@ cglobal pred8x8_tm_vp8_8, 2,6,4
     REP_RET
 
 INIT_XMM ssse3
-cglobal pred8x8_tm_vp8_8, 2,3,6
+cglobal pred8x8_tm_vp8_8, 2,3,6, "p", src, "p-", stride
     sub          r0, r1
     movdqa     xmm4, [tm_shuf]
     pxor       xmm1, xmm1
@@ -1054,7 +1054,7 @@ cglobal pred8x8_tm_vp8_8, 2,3,6
 ;                           ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_TOP_DC 0
-cglobal pred8x8l_top_dc_8, 4,4
+cglobal pred8x8l_top_dc_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     pxor        mm7, mm7
     movq        mm0, [r0-8]
@@ -1111,7 +1111,7 @@ PRED8x8L_TOP_DC
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8L_DC 0
-cglobal pred8x8l_dc_8, 4,5
+cglobal pred8x8l_dc_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -1215,7 +1215,7 @@ PRED8x8L_DC
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8L_HORIZONTAL 0
-cglobal pred8x8l_horizontal_8, 4,4
+cglobal pred8x8l_horizontal_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r2, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -1287,7 +1287,7 @@ PRED8x8L_HORIZONTAL
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8L_VERTICAL 0
-cglobal pred8x8l_vertical_8, 4,4
+cglobal pred8x8l_vertical_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     movq        mm0, [r0-8]
     movq        mm3, [r0]
@@ -1338,7 +1338,7 @@ PRED8x8L_VERTICAL
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8l_down_left_8, 4,5
+cglobal pred8x8l_down_left_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     movq        mm0, [r0-8]
     movq        mm3, [r0]
@@ -1446,7 +1446,7 @@ cglobal pred8x8l_down_left_8, 4,5
     RET
 
 %macro PRED8x8L_DOWN_LEFT 0
-cglobal pred8x8l_down_left_8, 4,4
+cglobal pred8x8l_down_left_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     movq        mm0, [r0-8]
     movq        mm3, [r0]
@@ -1539,7 +1539,7 @@ PRED8x8L_DOWN_LEFT
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8l_down_right_8, 4,5
+cglobal pred8x8l_down_right_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -1671,7 +1671,7 @@ cglobal pred8x8l_down_right_8, 4,5
     RET
 
 %macro PRED8x8L_DOWN_RIGHT 0
-cglobal pred8x8l_down_right_8, 4,5
+cglobal pred8x8l_down_right_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -1791,7 +1791,7 @@ PRED8x8L_DOWN_RIGHT
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8l_vertical_right_8, 4,5
+cglobal pred8x8l_vertical_right_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -1898,7 +1898,7 @@ cglobal pred8x8l_vertical_right_8, 4,5
     RET
 
 %macro PRED8x8L_VERTICAL_RIGHT 0
-cglobal pred8x8l_vertical_right_8, 4,5,7
+cglobal pred8x8l_vertical_right_8, 4,5,7, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     ; manually spill XMM registers for Win64 because
     ; the code here is initialized with INIT_MMX
     WIN64_SPILL_XMM 7
@@ -2019,7 +2019,7 @@ PRED8x8L_VERTICAL_RIGHT
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8L_VERTICAL_LEFT 0
-cglobal pred8x8l_vertical_left_8, 4,4
+cglobal pred8x8l_vertical_left_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     movq        mm0, [r0-8]
     movq        mm3, [r0]
@@ -2109,7 +2109,7 @@ PRED8x8L_VERTICAL_LEFT
 ;-----------------------------------------------------------------------------
 
 %macro PRED8x8L_HORIZONTAL_UP 0
-cglobal pred8x8l_horizontal_up_8, 4,4
+cglobal pred8x8l_horizontal_up_8, 4,4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r2, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -2197,7 +2197,7 @@ PRED8x8L_HORIZONTAL_UP
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred8x8l_horizontal_down_8, 4,5
+cglobal pred8x8l_horizontal_down_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -2312,7 +2312,7 @@ cglobal pred8x8l_horizontal_down_8, 4,5
     RET
 
 %macro PRED8x8L_HORIZONTAL_DOWN 0
-cglobal pred8x8l_horizontal_down_8, 4,5
+cglobal pred8x8l_horizontal_down_8, 4,5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub          r0, r3
     lea          r4, [r0+r3*2]
     movq        mm0, [r0+r3*1-8]
@@ -2446,7 +2446,7 @@ PRED8x8L_HORIZONTAL_DOWN
 ;-------------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_dc_8, 3,5
+cglobal pred4x4_dc_8, 3,5, "p", src, "p", topright, "p-", stride
     pxor   mm7, mm7
     mov     r4, r0
     sub     r0, r2
@@ -2477,7 +2477,7 @@ cglobal pred4x4_dc_8, 3,5
 ;-----------------------------------------------------------------------------
 
 %macro PRED4x4_TM 0
-cglobal pred4x4_tm_vp8_8, 3,6
+cglobal pred4x4_tm_vp8_8, 3,6, "p", src, "p", topright, "p-", stride
     sub        r0, r2
     pxor      mm7, mm7
     movd      mm0, [r0]
@@ -2518,7 +2518,7 @@ INIT_MMX mmxext
 PRED4x4_TM
 
 INIT_XMM ssse3
-cglobal pred4x4_tm_vp8_8, 3,3
+cglobal pred4x4_tm_vp8_8, 3,3, "p", src, "p", topright, "p-", stride
     sub         r0, r2
     movq       mm6, [tm_shuf]
     pxor       mm1, mm1
@@ -2556,7 +2556,7 @@ cglobal pred4x4_tm_vp8_8, 3,3
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_vertical_vp8_8, 3,3
+cglobal pred4x4_vertical_vp8_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     movd      m1, [r0-1]
     movd      m0, [r0]
@@ -2576,7 +2576,7 @@ cglobal pred4x4_vertical_vp8_8, 3,3
 ;                                    ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal pred4x4_down_left_8, 3,3
+cglobal pred4x4_down_left_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     movq      m1, [r0]
     punpckldq m1, [r1]
@@ -2604,7 +2604,7 @@ cglobal pred4x4_down_left_8, 3,3
 ;------------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_vertical_left_8, 3,3
+cglobal pred4x4_vertical_left_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     movq      m1, [r0]
     punpckldq m1, [r1]
@@ -2630,7 +2630,7 @@ cglobal pred4x4_vertical_left_8, 3,3
 ;------------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_horizontal_up_8, 3,3
+cglobal pred4x4_horizontal_up_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     lea       r1, [r0+r2*2]
     movd      m0, [r0+r2*1-4]
@@ -2665,7 +2665,7 @@ cglobal pred4x4_horizontal_up_8, 3,3
 ;------------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_horizontal_down_8, 3,3
+cglobal pred4x4_horizontal_down_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     lea       r1, [r0+r2*2]
     movh      m0, [r0-4]      ; lt ..
@@ -2702,7 +2702,7 @@ cglobal pred4x4_horizontal_down_8, 3,3
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_vertical_right_8, 3,3
+cglobal pred4x4_vertical_right_8, 3,3, "p", src, "p", topright, "p-", stride
     sub     r0, r2
     lea     r1, [r0+r2*2]
     movh    m0, [r0]                    ; ........t3t2t1t0
@@ -2733,7 +2733,7 @@ cglobal pred4x4_vertical_right_8, 3,3
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_down_right_8, 3,3
+cglobal pred4x4_down_right_8, 3,3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     lea       r1, [r0+r2*2]
     movq      m1, [r1-8]
diff --git a/libavcodec/x86/h264_intrapred_10bit.asm b/libavcodec/x86/h264_intrapred_10bit.asm
index 629e0a72e3..57b5fadb00 100644
--- a/libavcodec/x86/h264_intrapred_10bit.asm
+++ b/libavcodec/x86/h264_intrapred_10bit.asm
@@ -55,7 +55,7 @@ SECTION .text
 ;                               ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED4x4_DR 0
-cglobal pred4x4_down_right_10, 3, 3
+cglobal pred4x4_down_right_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     lea       r1, [r0+r2*2]
     movhps    m1, [r1-8]
@@ -94,7 +94,7 @@ PRED4x4_DR
 ;                                   ptrdiff_t stride)
 ;------------------------------------------------------------------------------
 %macro PRED4x4_VR 0
-cglobal pred4x4_vertical_right_10, 3, 3, 6
+cglobal pred4x4_vertical_right_10, 3, 3, 6, "p", src, "p", topright, "p-", stride
     sub     r0, r2
     lea     r1, [r0+r2*2]
     movq    m5, [r0]            ; ........t3t2t1t0
@@ -134,7 +134,7 @@ PRED4x4_VR
 ;                                    ptrdiff_t stride)
 ;-------------------------------------------------------------------------------
 %macro PRED4x4_HD 0
-cglobal pred4x4_horizontal_down_10, 3, 3
+cglobal pred4x4_horizontal_down_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub        r0, r2
     lea        r1, [r0+r2*2]
     movq       m0, [r0-8]      ; lt ..
@@ -177,7 +177,7 @@ PRED4x4_HD
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmxext
-cglobal pred4x4_dc_10, 3, 3
+cglobal pred4x4_dc_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub    r0, r2
     lea    r1, [r0+r2*2]
     movq   m2, [r0+r2*1-8]
@@ -202,7 +202,7 @@ cglobal pred4x4_dc_10, 3, 3
 ;                              ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED4x4_DL 0
-cglobal pred4x4_down_left_10, 3, 3
+cglobal pred4x4_down_left_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub        r0, r2
     movq       m0, [r0]
     movhps     m0, [r1]
@@ -233,7 +233,7 @@ PRED4x4_DL
 ;                                  ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED4x4_VL 0
-cglobal pred4x4_vertical_left_10, 3, 3
+cglobal pred4x4_vertical_left_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub        r0, r2
     movu       m1, [r0]
     movhps     m1, [r1]
@@ -263,7 +263,7 @@ PRED4x4_VL
 ;                                  ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_MMX mmxext
-cglobal pred4x4_horizontal_up_10, 3, 3
+cglobal pred4x4_horizontal_up_10, 3, 3, "p", src, "p", topright, "p-", stride
     sub       r0, r2
     lea       r1, [r0+r2*2]
     movq      m0, [r0+r2*1-8]
@@ -297,7 +297,7 @@ cglobal pred4x4_horizontal_up_10, 3, 3
 ; void ff_pred8x8_vertical_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pred8x8_vertical_10, 2, 2
+cglobal pred8x8_vertical_10, 2, 2, "p", src, "p-", stride
     sub  r0, r1
     mova m0, [r0]
 %rep 3
@@ -313,7 +313,7 @@ cglobal pred8x8_vertical_10, 2, 2
 ; void ff_pred8x8_horizontal_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pred8x8_horizontal_10, 2, 3
+cglobal pred8x8_horizontal_10, 2, 3, "p", src, "p-", stride
     mov         r2d, 4
 .loop:
     movq         m0, [r0+r1*0-8]
@@ -343,7 +343,7 @@ cglobal pred8x8_horizontal_10, 2, 3
 %endmacro
 
 %macro PRED8x8_DC 1
-cglobal pred8x8_dc_10, 2, 6
+cglobal pred8x8_dc_10, 2, 6, "p", src, "p-", stride
     sub         r0, r1
     pxor        m4, m4
     movq        m0, [r0+0]
@@ -420,7 +420,7 @@ PRED8x8_DC pshuflw
 ; void ff_pred8x8_top_dc_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pred8x8_top_dc_10, 2, 4
+cglobal pred8x8_top_dc_10, 2, 4, "p", src, "p-", stride
     sub         r0, r1
     mova        m0, [r0]
     pshuflw     m1, m0, 0x4e
@@ -447,7 +447,7 @@ cglobal pred8x8_top_dc_10, 2, 4
 ; void ff_pred8x8_plane_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 INIT_XMM sse2
-cglobal pred8x8_plane_10, 2, 7, 7
+cglobal pred8x8_plane_10, 2, 7, 7, "p", src, "p-", stride
     sub       r0, r1
     lea       r2, [r1*3]
     lea       r3, [r0+r1*4]
@@ -511,7 +511,7 @@ cglobal pred8x8_plane_10, 2, 7, 7
 ;                            ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_128_DC 0
-cglobal pred8x8l_128_dc_10, 4, 4
+cglobal pred8x8l_128_dc_10, 4, 4, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     mova      m0, [pw_512] ; (1<<(BIT_DEPTH-1))
     lea       r1, [r3*3]
     lea       r2, [r0+r3*4]
@@ -536,7 +536,7 @@ PRED8x8L_128_DC
 ;                            ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_TOP_DC 0
-cglobal pred8x8l_top_dc_10, 4, 4, 6
+cglobal pred8x8l_top_dc_10, 4, 4, 6, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     mova        m0, [r0]
     shr        r1d, 14
@@ -577,7 +577,7 @@ PRED8x8L_TOP_DC
 ;-------------------------------------------------------------------------------
 ;TODO: see if scalar is faster
 %macro PRED8x8L_DC 0
-cglobal pred8x8l_dc_10, 4, 6, 6
+cglobal pred8x8l_dc_10, 4, 6, 6, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     lea         r4, [r0+r3*4]
     lea         r5, [r3*3]
@@ -636,7 +636,7 @@ PRED8x8L_DC
 ;                              ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_VERTICAL 0
-cglobal pred8x8l_vertical_10, 4, 4, 6
+cglobal pred8x8l_vertical_10, 4, 4, 6, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     mova        m0, [r0]
     shr        r1d, 14
@@ -672,7 +672,7 @@ PRED8x8L_VERTICAL
 ;                                int has_topright, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_HORIZONTAL 0
-cglobal pred8x8l_horizontal_10, 4, 4, 5
+cglobal pred8x8l_horizontal_10, 4, 4, 5, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     mova        m0, [r0-16]
     shr        r1d, 14
     dec         r1
@@ -729,7 +729,7 @@ PRED8x8L_HORIZONTAL
 ;                               ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_DOWN_LEFT 0
-cglobal pred8x8l_down_left_10, 4, 4, 7
+cglobal pred8x8l_down_left_10, 4, 4, 7, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     mova        m3, [r0]
     shr        r1d, 14
@@ -800,7 +800,7 @@ PRED8x8L_DOWN_LEFT
 %macro PRED8x8L_DOWN_RIGHT 0
 ; standard forbids this when has_topleft is false
 ; no need to check
-cglobal pred8x8l_down_right_10, 4, 5, 8
+cglobal pred8x8l_down_right_10, 4, 5, 8, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     lea         r4, [r0+r3*4]
     lea         r1, [r3*3]
@@ -874,7 +874,7 @@ PRED8x8L_DOWN_RIGHT
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_VERTICAL_RIGHT 0
 ; likewise with 8x8l_down_right
-cglobal pred8x8l_vertical_right_10, 4, 5, 7
+cglobal pred8x8l_vertical_right_10, 4, 5, 7, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     sub         r0, r3
     lea         r4, [r0+r3*4]
     lea         r1, [r3*3]
@@ -944,7 +944,7 @@ PRED8x8L_VERTICAL_RIGHT
 ;                                   int has_topright, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED8x8L_HORIZONTAL_UP 0
-cglobal pred8x8l_horizontal_up_10, 4, 4, 6
+cglobal pred8x8l_horizontal_up_10, 4, 4, 6, "p", src, "d", has_topleft, "d", has_topright, "p-", stride
     mova        m0, [r0+r3*0-16]
     punpckhwd   m0, [r0+r3*1-16]
     shr        r1d, 14
@@ -1015,7 +1015,7 @@ PRED8x8L_HORIZONTAL_UP
 %endmacro
 
 %macro PRED16x16_VERTICAL 0
-cglobal pred16x16_vertical_10, 2, 3
+cglobal pred16x16_vertical_10, 2, 3, "p", src, "p-", stride
     sub   r0, r1
     mov  r2d, 8
     mova  m0, [r0+ 0]
@@ -1042,7 +1042,7 @@ PRED16x16_VERTICAL
 ; void ff_pred16x16_horizontal_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED16x16_HORIZONTAL 0
-cglobal pred16x16_horizontal_10, 2, 3
+cglobal pred16x16_horizontal_10, 2, 3, "p", src, "p-", stride
     mov   r2d, 8
 .vloop:
     movd   m0, [r0+r1*0-4]
@@ -1066,7 +1066,7 @@ PRED16x16_HORIZONTAL
 ; void ff_pred16x16_dc_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED16x16_DC 0
-cglobal pred16x16_dc_10, 2, 6
+cglobal pred16x16_dc_10, 2, 6, "p", src, "p-", stride
     mov        r5, r0
     sub        r0, r1
     mova       m0, [r0+0]
@@ -1112,7 +1112,7 @@ PRED16x16_DC
 ; void ff_pred16x16_top_dc_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED16x16_TOP_DC 0
-cglobal pred16x16_top_dc_10, 2, 3
+cglobal pred16x16_top_dc_10, 2, 3, "p", src, "p-", stride
     sub        r0, r1
     mova       m0, [r0+0]
     paddw      m0, [r0+mmsize]
@@ -1144,7 +1144,7 @@ PRED16x16_TOP_DC
 ; void ff_pred16x16_left_dc_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED16x16_LEFT_DC 0
-cglobal pred16x16_left_dc_10, 2, 6
+cglobal pred16x16_left_dc_10, 2, 6, "p", src, "p-", stride
     mov        r5, r0
 
     sub        r0, 2
@@ -1181,7 +1181,7 @@ PRED16x16_LEFT_DC
 ; void ff_pred16x16_128_dc_10(pixel *src, ptrdiff_t stride)
 ;-----------------------------------------------------------------------------
 %macro PRED16x16_128_DC 0
-cglobal pred16x16_128_dc_10, 2,3
+cglobal pred16x16_128_dc_10, 2,3, "p", src, "p-", stride
     mova       m0, [pw_512]
     mov       r2d, 8
 .loop:
diff --git a/libavcodec/x86/h264_qpel_10bit.asm b/libavcodec/x86/h264_qpel_10bit.asm
index 872268300a..ae3e7f60aa 100644
--- a/libavcodec/x86/h264_qpel_10bit.asm
+++ b/libavcodec/x86/h264_qpel_10bit.asm
@@ -111,7 +111,7 @@ INIT_XMM sse2
 
 %macro MCAxA_OP 7
 %if ARCH_X86_32
-cglobal %1_h264_qpel%4_%2_10, %5,%6,%7
+cglobal %1_h264_qpel%4_%2_10, %5,%6,%7, "p", dst, "p", src, "p-", stride
     call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
     mov  r0, r0m
     mov  r1, r1m
@@ -130,7 +130,7 @@ cglobal %1_h264_qpel%4_%2_10, %5,%6,%7
     call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
     RET
 %else ; ARCH_X86_64
-cglobal %1_h264_qpel%4_%2_10, %5,%6 + 2,%7
+cglobal %1_h264_qpel%4_%2_10, %5,%6 + 2,%7, "p", dst, "p", src, "p-", stride
     mov r%6, r0
 %assign p1 %6+1
     mov r %+ p1, r1
@@ -157,7 +157,7 @@ cglobal %1_h264_qpel%4_%2_10, %5,%6 + 2,%7
 MCAxA_OP %1, %2, %3, i, %4,%5,%6
 %endif
 
-cglobal %1_h264_qpel%3_%2_10, %4,%5,%6
+cglobal %1_h264_qpel%3_%2_10, %4,%5,%6, "p", dst, "p", src, "p-", stride
 %if UNIX64 == 0 ; no prologue or epilogue for UNIX64
     call stub_%1_h264_qpel%3_%2_10 %+ SUFFIX
     RET
@@ -188,7 +188,7 @@ cglobal_mc %1, mc00, 4, 3,4,0
     ret
 
 INIT_XMM sse2
-cglobal %1_h264_qpel8_mc00_10, 3,4
+cglobal %1_h264_qpel8_mc00_10, 3,4, "p", dst, "p", src, "p-", stride
     lea  r3, [r2*3]
     COPY4
     lea  r0, [r0+r2*4]
@@ -196,7 +196,7 @@ cglobal %1_h264_qpel8_mc00_10, 3,4
     COPY4
     RET
 
-cglobal %1_h264_qpel16_mc00_10, 3,4
+cglobal %1_h264_qpel16_mc00_10, 3,4, "p", dst, "p", src, "p-", stride
     mov r3d, 8
 .loop:
     movu           m0, [r1      ]
diff --git a/libavcodec/x86/h264_qpel_8bit.asm b/libavcodec/x86/h264_qpel_8bit.asm
index 2d287ba443..62d9d15c00 100644
--- a/libavcodec/x86/h264_qpel_8bit.asm
+++ b/libavcodec/x86/h264_qpel_8bit.asm
@@ -54,9 +54,7 @@ SECTION .text
 %endmacro
 
 %macro QPEL4_H_LOWPASS_OP 1
-cglobal %1_h264_qpel4_h_lowpass, 4,5 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel4_h_lowpass, 4,5, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     pxor          m7, m7
     mova          m4, [pw_5]
     mova          m5, [pw_16]
@@ -97,9 +95,7 @@ QPEL4_H_LOWPASS_OP put
 QPEL4_H_LOWPASS_OP avg
 
 %macro QPEL8_H_LOWPASS_OP 1
-cglobal %1_h264_qpel8_h_lowpass, 4,5 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel8_h_lowpass, 4,5, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     mov          r4d, 8
     pxor          m7, m7
     mova          m6, [pw_5]
@@ -157,9 +153,7 @@ QPEL8_H_LOWPASS_OP put
 QPEL8_H_LOWPASS_OP avg
 
 %macro QPEL8_H_LOWPASS_OP_XMM 1
-cglobal %1_h264_qpel8_h_lowpass, 4,5,8 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel8_h_lowpass, 4,5,8, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     mov          r4d, 8
     pxor          m7, m7
     mova          m6, [pw_5]
@@ -201,9 +195,7 @@ QPEL8_H_LOWPASS_OP_XMM avg
 
 
 %macro QPEL4_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel4_h_lowpass_l2, 5,6 ; dst, src, src2, dstStride, srcStride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_h264_qpel4_h_lowpass_l2, 5,6, "p", dst, "p", src, "p", src2, "d-", dstStride, "d-", srcStride
     pxor          m7, m7
     mova          m4, [pw_5]
     mova          m5, [pw_16]
@@ -248,9 +240,7 @@ QPEL4_H_LOWPASS_L2_OP avg
 
 
 %macro QPEL8_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel8_h_lowpass_l2, 5,6 ; dst, src, src2, dstStride, srcStride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_h264_qpel8_h_lowpass_l2, 5,6, "p", dst, "p", src, "p", src2, "d-", dstStride, "d-", srcStride
     mov          r5d, 8
     pxor          m7, m7
     mova          m6, [pw_5]
@@ -312,9 +302,7 @@ QPEL8_H_LOWPASS_L2_OP avg
 
 
 %macro QPEL8_H_LOWPASS_L2_OP_XMM 1
-cglobal %1_h264_qpel8_h_lowpass_l2, 5,6,8 ; dst, src, src2, dstStride, src2Stride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_h264_qpel8_h_lowpass_l2, 5,6,8, "p", dst, "p", src, "p", src2, "d-", dstStride, "d-", src2Stride
     mov          r5d, 8
     pxor          m7, m7
     mova          m6, [pw_5]
@@ -381,9 +369,7 @@ QPEL8_H_LOWPASS_L2_OP_XMM avg
 %endmacro
 
 %macro QPEL4_V_LOWPASS_OP 1
-cglobal %1_h264_qpel4_v_lowpass, 4,4 ; dst, src, dstStride, srcStride
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel4_v_lowpass, 4,4, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     sub           r1, r3
     sub           r1, r3
     pxor          m7, m7
@@ -415,15 +401,11 @@ QPEL4_V_LOWPASS_OP avg
 
 %macro QPEL8OR16_V_LOWPASS_OP 1
 %if cpuflag(sse2)
-cglobal %1_h264_qpel8or16_v_lowpass, 5,5,8 ; dst, src, dstStride, srcStride, h
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel8or16_v_lowpass, 5,5,8, "p", dst, "p", src, "d-", dstStride, "d-", srcStride, "d", h
     sub           r1, r3
     sub           r1, r3
 %else
-cglobal %1_h264_qpel8or16_v_lowpass_op, 5,5,8 ; dst, src, dstStride, srcStride, h
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel8or16_v_lowpass_op, 5,5,8, "p", dst, "p", src, "d-", dstStride, "d-", srcStride, "d", h
 %endif
     pxor          m7, m7
     movh          m0, [r1]
@@ -490,8 +472,7 @@ QPEL8OR16_V_LOWPASS_OP avg
 %endmacro
 
 %macro QPEL4_HV1_LOWPASS_OP 1
-cglobal %1_h264_qpel4_hv_lowpass_v, 3,3 ; src, tmp, srcStride
-    movsxdifnidn  r2, r2d
+cglobal %1_h264_qpel4_hv_lowpass_v, 3,3, "p", src, "p", tmp, "d-", srcStride
     pxor          m7, m7
     movh          m0, [r0]
     movh          m1, [r0+r2]
@@ -512,8 +493,7 @@ cglobal %1_h264_qpel4_hv_lowpass_v, 3,3 ; src, tmp, srcStride
     FILT_HV       3*24
     RET
 
-cglobal %1_h264_qpel4_hv_lowpass_h, 3,4 ; tmp, dst, dstStride
-    movsxdifnidn  r2, r2d
+cglobal %1_h264_qpel4_hv_lowpass_h, 3,4, "p", tmp, "p", dst, "d-", dstStride
     mov          r3d, 4
 .loop:
     mova          m0, [r0]
@@ -543,8 +523,7 @@ QPEL4_HV1_LOWPASS_OP put
 QPEL4_HV1_LOWPASS_OP avg
 
 %macro QPEL8OR16_HV1_LOWPASS_OP 1
-cglobal %1_h264_qpel8or16_hv1_lowpass_op, 4,4,8 ; src, tmp, srcStride, size
-    movsxdifnidn  r2, r2d
+cglobal %1_h264_qpel8or16_hv1_lowpass_op, 4,4,8, "p", src, "p", tmp, "d-", srcStride, "d", size
     pxor          m7, m7
     movh          m0, [r0]
     movh          m1, [r0+r2]
@@ -592,8 +571,7 @@ QPEL8OR16_HV1_LOWPASS_OP put
 
 %macro QPEL8OR16_HV2_LOWPASS_OP 1
 ; unused is to match ssse3 and mmxext args
-cglobal %1_h264_qpel8or16_hv2_lowpass_op, 5,5 ; dst, tmp, dstStride, unused, h
-    movsxdifnidn  r2, r2d
+cglobal %1_h264_qpel8or16_hv2_lowpass_op, 5,5, "p", dst, "p", tmp, "d-", dstStride, "d*", unused, "d", h
 .loop:
     mova          m0, [r1]
     mova          m3, [r1+8]
@@ -635,9 +613,7 @@ QPEL8OR16_HV2_LOWPASS_OP put
 QPEL8OR16_HV2_LOWPASS_OP avg
 
 %macro QPEL8OR16_HV2_LOWPASS_OP_XMM 1
-cglobal %1_h264_qpel8or16_hv2_lowpass, 5,5,8 ; dst, tmp, dstStride, tmpStride, size
-    movsxdifnidn  r2, r2d
-    movsxdifnidn  r3, r3d
+cglobal %1_h264_qpel8or16_hv2_lowpass, 5,5,8, "p", dst, "p", tmp, "d-", dstStride, "d-", tmpStride, "d", size
     cmp          r4d, 16
     je         .op16
 .loop8:
@@ -727,9 +703,7 @@ QPEL8OR16_HV2_LOWPASS_OP_XMM avg
 
 
 %macro PIXELS4_L2_SHIFT5 1
-cglobal %1_pixels4_l2_shift5,6,6 ; dst, src16, src8, dstStride, src8Stride, h
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_pixels4_l2_shift5,6,6, "p", dst, "p", src16, "p", src8, "d-", dstStride, "d-", src8Stride, "d", h
     mova          m0, [r1]
     mova          m1, [r1+24]
     psraw         m0, 5
@@ -761,9 +735,7 @@ PIXELS4_L2_SHIFT5 avg
 
 
 %macro PIXELS8_L2_SHIFT5 1
-cglobal %1_pixels8_l2_shift5, 6, 6 ; dst, src16, src8, dstStride, src8Stride, h
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_pixels8_l2_shift5, 6, 6, "p", dst, "p", src16, "p", src8, "d-", dstStride, "d-", src8Stride, "d", h
 .loop:
     mova          m0, [r1]
     mova          m1, [r1+8]
@@ -794,9 +766,7 @@ PIXELS8_L2_SHIFT5 avg
 
 %if ARCH_X86_64
 %macro QPEL16_H_LOWPASS_L2_OP 1
-cglobal %1_h264_qpel16_h_lowpass_l2, 5, 6, 16 ; dst, src, src2, dstStride, src2Stride
-    movsxdifnidn  r3, r3d
-    movsxdifnidn  r4, r4d
+cglobal %1_h264_qpel16_h_lowpass_l2, 5, 6, 16, "p", dst, "p", src, "p", src2, "d-", dstStride, "d-", src2Stride
     mov          r5d, 16
     pxor         m15, m15
     mova         m14, [pw_5]
diff --git a/libavcodec/x86/h264_weight.asm b/libavcodec/x86/h264_weight.asm
index 0975d74fcf..6d68eab96e 100644
--- a/libavcodec/x86/h264_weight.asm
+++ b/libavcodec/x86/h264_weight.asm
@@ -28,11 +28,11 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 ; biweight pred:
 ;
-; void ff_h264_biweight_16_sse2(uint8_t *dst, uint8_t *src, int stride,
+; void ff_h264_biweight_16_sse2(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
 ;                               int height, int log2_denom, int weightd,
 ;                               int weights, int offset);
 ; and
-; void ff_h264_weight_16_sse2(uint8_t *dst, int stride, int height,
+; void ff_h264_weight_16_sse2(uint8_t *dst, ptrdiff_t stride, int height,
 ;                             int log2_denom, int weight, int offset);
 ;-----------------------------------------------------------------------------
 
@@ -71,7 +71,7 @@ SECTION .text
 %endmacro
 
 INIT_MMX mmxext
-cglobal h264_weight_16, 6, 6, 0
+cglobal h264_weight_16, 6, 6, 0, "p", dst, "p-", stride, "d", height, "d", log2_denom, "d", weight, "d", offset
     WEIGHT_SETUP
 .nextrow:
     WEIGHT_OP 0,  4
@@ -84,7 +84,7 @@ cglobal h264_weight_16, 6, 6, 0
     REP_RET
 
 %macro WEIGHT_FUNC_MM 2
-cglobal h264_weight_%1, 6, 6, %2
+cglobal h264_weight_%1, 6, 6, %2, "p", dst, "p-", stride, "d", height, "d", log2_denom, "d", weight, "d", offset
     WEIGHT_SETUP
 .nextrow:
     WEIGHT_OP 0, mmsize/2
@@ -101,7 +101,7 @@ INIT_XMM sse2
 WEIGHT_FUNC_MM 16, 8
 
 %macro WEIGHT_FUNC_HALF_MM 2
-cglobal h264_weight_%1, 6, 6, %2
+cglobal h264_weight_%1, 6, 6, %2, "p", dst, "p-", stride, "d", height, "d", log2_denom, "d", weight, "d", offset
     WEIGHT_SETUP
     sar       r2d, 1
     lea        r3, [r1*2]
@@ -199,7 +199,7 @@ WEIGHT_FUNC_HALF_MM 8, 8
 %endmacro
 
 INIT_MMX mmxext
-cglobal h264_biweight_16, 7, 8, 0
+cglobal h264_biweight_16, 7, 8, 0, "p", dst, "p", src, "p-", stride, "d", height_, "d", log2_denom_, "d", weightd_, "d", weights_, "d", offset
     BIWEIGHT_SETUP
     movifnidn r3d, r3m
 .nextrow:
@@ -218,7 +218,7 @@ cglobal h264_biweight_16, 7, 8, 0
     REP_RET
 
 %macro BIWEIGHT_FUNC_MM 2
-cglobal h264_biweight_%1, 7, 8, %2
+cglobal h264_biweight_%1, 7, 8, %2, "p", dst, "p", src, "p-", stride, "d", height_, "d", log2_denom_, "d", weightd_, "d", weights_, "d", offset
     BIWEIGHT_SETUP
     movifnidn r3d, r3m
 .nextrow:
@@ -239,7 +239,7 @@ INIT_XMM sse2
 BIWEIGHT_FUNC_MM 16, 8
 
 %macro BIWEIGHT_FUNC_HALF_MM 2
-cglobal h264_biweight_%1, 7, 8, %2
+cglobal h264_biweight_%1, 7, 8, %2, "p", dst, "p", src, "p-", stride, "d", height_, "d", log2_denom_, "d", weightd_, "d", weights_, "d", offset
     BIWEIGHT_SETUP
     movifnidn r3d, r3m
     sar        r3, 1
@@ -278,7 +278,7 @@ BIWEIGHT_FUNC_HALF_MM 8, 8
 %endmacro
 
 INIT_XMM ssse3
-cglobal h264_biweight_16, 7, 8, 8
+cglobal h264_biweight_16, 7, 8, 8, "p", dst, "p", src, "p-", stride, "d", height_, "d", log2_denom_, "d", weightd_, "d", weights_, "d", offset
     BIWEIGHT_SETUP
     movifnidn r3d, r3m
 
@@ -297,7 +297,7 @@ cglobal h264_biweight_16, 7, 8, 8
     REP_RET
 
 INIT_XMM ssse3
-cglobal h264_biweight_8, 7, 8, 8
+cglobal h264_biweight_8, 7, 8, 8, "p", dst, "p", src, "p-", stride, "d", height_, "d", log2_denom_, "d", weightd_, "d", weights_, "d", offset
     BIWEIGHT_SETUP
     movifnidn r3d, r3m
     sar        r3, 1
diff --git a/libavcodec/x86/h264_weight_10bit.asm b/libavcodec/x86/h264_weight_10bit.asm
index f924e55854..99754a0486 100644
--- a/libavcodec/x86/h264_weight_10bit.asm
+++ b/libavcodec/x86/h264_weight_10bit.asm
@@ -41,12 +41,7 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 %macro WEIGHT_PROLOGUE 0
 .prologue:
-    PROLOGUE 0,6,8
-    movifnidn  r0, r0mp
-    movifnidn r1d, r1m
-    movifnidn r2d, r2m
-    movifnidn r4d, r4m
-    movifnidn r5d, r5m
+    PROLOGUE 6, 6, 8, "p", dst, "d", stride, "d", height, "d*", log2_denom, "d", weight, "d", offset
 %endmacro
 
 %macro WEIGHT_SETUP 0
@@ -164,13 +159,8 @@ DECLARE_REG_TMP 7
 
 %macro BIWEIGHT_PROLOGUE 0
 .prologue:
-    PROLOGUE 0,8,8
-    movifnidn  r0, r0mp
-    movifnidn  r1, r1mp
-    movifnidn r2d, r2m
-    movifnidn r5d, r5m
-    movifnidn r6d, r6m
-    movifnidn t0d, r7m
+    PROLOGUE 7, 8, 8, "p", dst, "p", src, "d", stride, "d*", height, "d*", log2_denom, "d", weightd, "d", weights, "d", offset
+    movifnidn t0d, r7md
 %endmacro
 
 %macro BIWEIGHT_SETUP 0
diff --git a/libavcodec/x86/hevc_add_res.asm b/libavcodec/x86/hevc_add_res.asm
index 36d4d8e2e2..821aa22f42 100644
--- a/libavcodec/x86/hevc_add_res.asm
+++ b/libavcodec/x86/hevc_add_res.asm
@@ -50,7 +50,7 @@ cextern pw_1023
 
 INIT_MMX mmxext
 ; void ff_hevc_add_residual_4_8_mmxext(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_4_8, 3, 3, 6
+cglobal hevc_add_residual_4_8, 3, 3, 6, "p", dst, "p", res, "p-", stride
     ADD_RES_MMX_4_8
     add               r1, 16
     lea               r0, [r0+r2*2]
@@ -120,7 +120,7 @@ cglobal hevc_add_residual_4_8, 3, 3, 6
 
 %macro TRANSFORM_ADD_8 0
 ; void ff_hevc_add_residual_8_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_8_8, 3, 4, 8
+cglobal hevc_add_residual_8_8, 3, 4, 8, "p", dst, "p", res, "p-", stride
     lea               r3, [r2*3]
     ADD_RES_SSE_8_8
     add               r1, 64
@@ -129,7 +129,7 @@ cglobal hevc_add_residual_8_8, 3, 4, 8
     RET
 
 ; void ff_hevc_add_residual_16_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_16_8, 3, 5, 7
+cglobal hevc_add_residual_16_8, 3, 5, 7, "p", dst, "p", res, "p-", stride
     pxor                m0, m0
     lea                 r3, [r2*3]
     mov                r4d, 4
@@ -143,7 +143,7 @@ cglobal hevc_add_residual_16_8, 3, 5, 7
     RET
 
 ; void ff_hevc_add_residual_32_8_<opt>(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_32_8, 3, 5, 7
+cglobal hevc_add_residual_32_8, 3, 5, 7, "p", dst, "p", res, "p-", stride
     pxor                m0, m0
     mov                r4d, 16
 .loop:
@@ -164,7 +164,7 @@ TRANSFORM_ADD_8
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
 ; void ff_hevc_add_residual_32_8_avx2(uint8_t *dst, int16_t *res, ptrdiff_t stride)
-cglobal hevc_add_residual_32_8, 3, 5, 7
+cglobal hevc_add_residual_32_8, 3, 5, 7, "p", dst, "p", res, "p-", stride
     pxor                 m0, m0
     lea                  r3, [r2*3]
     mov                 r4d, 8
@@ -291,7 +291,7 @@ cglobal hevc_add_residual_32_8, 3, 5, 7
 
 ; void ff_hevc_add_residual_<4|8|16|32>_10(pixel *dst, int16_t *block, ptrdiff_t stride)
 INIT_MMX mmxext
-cglobal hevc_add_residual_4_10, 3, 3, 6
+cglobal hevc_add_residual_4_10, 3, 3, 6, "p", dst, "p", res, "p-", stride
     pxor              m2, m2
     mova              m3, [max_pixels_10]
     ADD_RES_MMX_4_10  r0, r2, r1
@@ -301,7 +301,7 @@ cglobal hevc_add_residual_4_10, 3, 3, 6
     RET
 
 INIT_XMM sse2
-cglobal hevc_add_residual_8_10, 3, 4, 6
+cglobal hevc_add_residual_8_10, 3, 4, 6, "p", dst, "p", res, "p-", stride
     pxor              m4, m4
     mova              m5, [max_pixels_10]
     lea               r3, [r2*3]
@@ -312,7 +312,7 @@ cglobal hevc_add_residual_8_10, 3, 4, 6
     ADD_RES_SSE_8_10  r0, r2, r3, r1
     RET
 
-cglobal hevc_add_residual_16_10, 3, 5, 6
+cglobal hevc_add_residual_16_10, 3, 5, 6, "p", dst, "p", res, "p-", stride
     pxor              m4, m4
     mova              m5, [max_pixels_10]
 
@@ -325,7 +325,7 @@ cglobal hevc_add_residual_16_10, 3, 5, 6
     jg .loop
     RET
 
-cglobal hevc_add_residual_32_10, 3, 5, 6
+cglobal hevc_add_residual_32_10, 3, 5, 6, "p", dst, "p", res, "p-", stride
     pxor              m4, m4
     mova              m5, [max_pixels_10]
 
@@ -340,7 +340,7 @@ cglobal hevc_add_residual_32_10, 3, 5, 6
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal hevc_add_residual_16_10, 3, 5, 6
+cglobal hevc_add_residual_16_10, 3, 5, 6, "p", dst, "p", res, "p-", stride
     pxor               m4, m4
     mova               m5, [max_pixels_10]
     lea                r3, [r2*3]
@@ -354,7 +354,7 @@ cglobal hevc_add_residual_16_10, 3, 5, 6
     jg .loop
     RET
 
-cglobal hevc_add_residual_32_10, 3, 5, 6
+cglobal hevc_add_residual_32_10, 3, 5, 6, "p", dst, "p", res, "p-", stride
     pxor               m4, m4
     mova               m5, [max_pixels_10]
 
diff --git a/libavcodec/x86/hevc_deblock.asm b/libavcodec/x86/hevc_deblock.asm
index 85ee4800bb..724739b951 100644
--- a/libavcodec/x86/hevc_deblock.asm
+++ b/libavcodec/x86/hevc_deblock.asm
@@ -625,7 +625,7 @@ ALIGN 16
 ;                                   uint8_t *_no_p, uint8_t *_no_q);
 ;-----------------------------------------------------------------------------
 %macro LOOP_FILTER_CHROMA 0
-cglobal hevc_v_loop_filter_chroma_8, 3, 5, 7, pix, stride, tc, pix0, r3stride
+cglobal hevc_v_loop_filter_chroma_8, 3, 5, 7, "p", pix, "p-", stride, "p", tc, pix0, r3stride
     sub            pixq, 2
     lea       r3strideq, [3*strideq]
     mov           pix0q, pixq
@@ -635,7 +635,7 @@ cglobal hevc_v_loop_filter_chroma_8, 3, 5, 7, pix, stride, tc, pix0, r3stride
     TRANSPOSE8x4B_STORE PASS8ROWS(pix0q, pixq, strideq, r3strideq)
     RET
 
-cglobal hevc_v_loop_filter_chroma_10, 3, 5, 7, pix, stride, tc, pix0, r3stride
+cglobal hevc_v_loop_filter_chroma_10, 3, 5, 7, "p", pix, "p-", stride, "p", tc, pix0, r3stride
     sub            pixq, 4
     lea       r3strideq, [3*strideq]
     mov           pix0q, pixq
@@ -645,7 +645,7 @@ cglobal hevc_v_loop_filter_chroma_10, 3, 5, 7, pix, stride, tc, pix0, r3stride
     TRANSPOSE8x4W_STORE PASS8ROWS(pix0q, pixq, strideq, r3strideq), [pw_pixel_max_10]
     RET
 
-cglobal hevc_v_loop_filter_chroma_12, 3, 5, 7, pix, stride, tc, pix0, r3stride
+cglobal hevc_v_loop_filter_chroma_12, 3, 5, 7, "p", pix, "p-", stride, "p", tc, pix0, r3stride
     sub            pixq, 4
     lea       r3strideq, [3*strideq]
     mov           pix0q, pixq
@@ -659,7 +659,7 @@ cglobal hevc_v_loop_filter_chroma_12, 3, 5, 7, pix, stride, tc, pix0, r3stride
 ; void ff_hevc_h_loop_filter_chroma(uint8_t *_pix, ptrdiff_t _stride, int32_t *tc,
 ;                                   uint8_t *_no_p, uint8_t *_no_q);
 ;-----------------------------------------------------------------------------
-cglobal hevc_h_loop_filter_chroma_8, 3, 4, 7, pix, stride, tc, pix0
+cglobal hevc_h_loop_filter_chroma_8, 3, 4, 7, "p", pix, "p-", stride, "p", tc, pix0
     mov           pix0q, pixq
     sub           pix0q, strideq
     sub           pix0q, strideq
@@ -678,7 +678,7 @@ cglobal hevc_h_loop_filter_chroma_8, 3, 4, 7, pix, stride, tc, pix0
     movhps       [pixq], m1
     RET
 
-cglobal hevc_h_loop_filter_chroma_10, 3, 4, 7, pix, stride, tc, pix0
+cglobal hevc_h_loop_filter_chroma_10, 3, 4, 7, "p", pix, "p-", stride, "p", tc, pix0
     mov          pix0q, pixq
     sub          pix0q, strideq
     sub          pix0q, strideq
@@ -694,7 +694,7 @@ cglobal hevc_h_loop_filter_chroma_10, 3, 4, 7, pix, stride, tc, pix0
     movu        [pixq], m2
     RET
 
-cglobal hevc_h_loop_filter_chroma_12, 3, 4, 7, pix, stride, tc, pix0
+cglobal hevc_h_loop_filter_chroma_12, 3, 4, 7, "p", pix, "p-", stride, "p", tc, pix0
     mov          pix0q, pixq
     sub          pix0q, strideq
     sub          pix0q, strideq
@@ -722,7 +722,7 @@ LOOP_FILTER_CHROMA
 ; void ff_hevc_v_loop_filter_luma(uint8_t *_pix, ptrdiff_t _stride, int beta,
 ;                                 int32_t *tc, uint8_t *_no_p, uint8_t *_no_q);
 ;-----------------------------------------------------------------------------
-cglobal hevc_v_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_v_loop_filter_luma_8, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     sub            pixq, 4
     lea           pix0q, [3 * r1]
     mov     src3strideq, pixq
@@ -734,7 +734,7 @@ cglobal hevc_v_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3s
 .bypassluma:
     RET
 
-cglobal hevc_v_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_v_loop_filter_luma_10, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     sub            pixq, 8
     lea           pix0q, [3 * strideq]
     mov     src3strideq, pixq
@@ -746,7 +746,7 @@ cglobal hevc_v_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3
 .bypassluma:
     RET
 
-cglobal hevc_v_loop_filter_luma_12, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_v_loop_filter_luma_12, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     sub            pixq, 8
     lea           pix0q, [3 * strideq]
     mov     src3strideq, pixq
@@ -762,7 +762,7 @@ cglobal hevc_v_loop_filter_luma_12, 4, 14, 16, pix, stride, beta, tc, pix0, src3
 ; void ff_hevc_h_loop_filter_luma(uint8_t *_pix, ptrdiff_t _stride, int beta,
 ;                                 int32_t *tc, uint8_t *_no_p, uint8_t *_no_q);
 ;-----------------------------------------------------------------------------
-cglobal hevc_h_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_h_loop_filter_luma_8, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     lea     src3strideq, [3 * strideq]
     mov           pix0q, pixq
     sub           pix0q, src3strideq
@@ -798,7 +798,7 @@ cglobal hevc_h_loop_filter_luma_8, 4, 14, 16, pix, stride, beta, tc, pix0, src3s
 .bypassluma:
     RET
 
-cglobal hevc_h_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_h_loop_filter_luma_10, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     lea                  src3strideq, [3 * strideq]
     mov                        pix0q, pixq
     sub                        pix0q, src3strideq
@@ -829,7 +829,7 @@ cglobal hevc_h_loop_filter_luma_10, 4, 14, 16, pix, stride, beta, tc, pix0, src3
 .bypassluma:
     RET
 
-cglobal hevc_h_loop_filter_luma_12, 4, 14, 16, pix, stride, beta, tc, pix0, src3stride
+cglobal hevc_h_loop_filter_luma_12, 4, 14, 16, "p", pix, "p-", stride, "d", beta, "p", tc, pix0, src3stride
     lea                  src3strideq, [3 * strideq]
     mov                        pix0q, pixq
     sub                        pix0q, src3strideq
diff --git a/libavcodec/x86/hevc_idct.asm b/libavcodec/x86/hevc_idct.asm
index 1eb1973f27..edcd8c5e41 100644
--- a/libavcodec/x86/hevc_idct.asm
+++ b/libavcodec/x86/hevc_idct.asm
@@ -241,7 +241,7 @@ SECTION .text
 ; %2 = number of loops
 ; %3 = bitdepth
 %macro IDCT_DC 3
-cglobal hevc_idct_%1x%1_dc_%3, 1, 2, 1, coeff, tmp
+cglobal hevc_idct_%1x%1_dc_%3, 1, 2, 1, "p", coeff, tmp
     movsx             tmpd, word [coeffq]
     add               tmpd, (1 << (14 - %3)) + 1
     sar               tmpd, (15 - %3)
@@ -267,7 +267,7 @@ cglobal hevc_idct_%1x%1_dc_%3, 1, 2, 1, coeff, tmp
 ; %1 = HxW
 ; %2 = bitdepth
 %macro IDCT_DC_NL 2 ; No loop
-cglobal hevc_idct_%1x%1_dc_%2, 1, 2, 1, coeff, tmp
+cglobal hevc_idct_%1x%1_dc_%2, 1, 2, 1, "p", coeff, tmp
     movsx             tmpd, word [coeffq]
     add               tmpd, (1 << (14 - %2)) + 1
     sar               tmpd, (15 - %2)
@@ -359,7 +359,7 @@ cglobal hevc_idct_%1x%1_dc_%2, 1, 2, 1, coeff, tmp
 ; void ff_hevc_idct_4x4__{8,10}_<opt>(int16_t *coeffs, int col_limit)
 ; %1 = bitdepth
 %macro IDCT_4x4 1
-cglobal hevc_idct_4x4_%1, 1, 1, 5, coeffs
+cglobal hevc_idct_4x4_%1, 1, 1, 5, "p", coeffs
     mova m0, [coeffsq]
     mova m1, [coeffsq + 16]
 
@@ -506,7 +506,7 @@ cglobal hevc_idct_transpose_8x8, 0, 0, 0
 ; void ff_hevc_idct_8x8_{8,10}_<opt>(int16_t *coeffs, int col_limit)
 ; %1 = bitdepth
 %macro IDCT_8x8 1
-cglobal hevc_idct_8x8_%1, 1, 1, 8, coeffs
+cglobal hevc_idct_8x8_%1, 1, 1, 8, "p", coeffs
     TR_8x4 0, 7, 32, 1, 16, 8, 1
     TR_8x4 8, 7, 32, 1, 16, 8, 1
 
@@ -628,7 +628,7 @@ cglobal hevc_idct_transpose_16x16, 0, 0, 0
 ; void ff_hevc_idct_16x16_{8,10}_<opt>(int16_t *coeffs, int col_limit)
 ; %1 = bitdepth
 %macro IDCT_16x16 1
-cglobal hevc_idct_16x16_%1, 1, 2, 16, coeffs
+cglobal hevc_idct_16x16_%1, 1, 2, 16, "p", coeffs
     mov r1d, 3
 .loop16:
     TR_16x4 8 * r1, 7, [pd_64], 64, 2, 32, 8, 16, 1, 0
@@ -790,7 +790,7 @@ cglobal hevc_idct_transpose_32x32, 0, 0, 0
 ; void ff_hevc_idct_32x32_{8,10}_<opt>(int16_t *coeffs, int col_limit)
 ; %1 = bitdepth
 %macro IDCT_32x32 1
-cglobal hevc_idct_32x32_%1, 1, 6, 16, 256, coeffs
+cglobal hevc_idct_32x32_%1, 1, 6, 16, 256, "p", coeffs
     mov r1d, 7
 .loop32:
     TR_32x4 8 * r1, %1, 1
diff --git a/libavcodec/x86/hevc_mc.asm b/libavcodec/x86/hevc_mc.asm
index ff6ed0711a..5acb62380f 100644
--- a/libavcodec/x86/hevc_mc.asm
+++ b/libavcodec/x86/hevc_mc.asm
@@ -682,7 +682,7 @@ HEVC_BI_PEL_PIXELS  %1, %2
 %endmacro
 
 %macro HEVC_PEL_PIXELS 2
-cglobal hevc_put_hevc_pel_pixels%1_%2, 4, 4, 3, dst, src, srcstride,height
+cglobal hevc_put_hevc_pel_pixels%1_%2, 4, 4, 3, "p", dst, "p", src, "p-", srcstride, "d", height
     pxor               m2, m2
 .loop:
     SIMPLE_LOAD       %1, %2, srcq, m0
@@ -693,7 +693,7 @@ cglobal hevc_put_hevc_pel_pixels%1_%2, 4, 4, 3, dst, src, srcstride,height
  %endmacro
 
 %macro HEVC_UNI_PEL_PIXELS 2
-cglobal hevc_put_hevc_uni_pel_pixels%1_%2, 5, 5, 2, dst, dststride, src, srcstride,height
+cglobal hevc_put_hevc_uni_pel_pixels%1_%2, 5, 5, 2, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height
 .loop:
     SIMPLE_LOAD       %1, %2, srcq, m0
     PEL_%2STORE%1   dstq, m0, m1
@@ -705,7 +705,7 @@ cglobal hevc_put_hevc_uni_pel_pixels%1_%2, 5, 5, 2, dst, dststride, src, srcstri
 %endmacro
 
 %macro HEVC_BI_PEL_PIXELS 2
-cglobal hevc_put_hevc_bi_pel_pixels%1_%2, 6, 6, 6, dst, dststride, src, srcstride, src2, height
+cglobal hevc_put_hevc_bi_pel_pixels%1_%2, 6, 6, 6, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height
     pxor              m2, m2
     movdqa            m5, [pw_bi_%2]
 .loop:
@@ -737,7 +737,7 @@ cglobal hevc_put_hevc_bi_pel_pixels%1_%2, 6, 6, 6, dst, dststride, src, srcstrid
 %define XMM_REGS  8
 %endif
 
-cglobal hevc_put_hevc_epel_h%1_%2, 5, 6, XMM_REGS, dst, src, srcstride, height, mx, rfilter
+cglobal hevc_put_hevc_epel_h%1_%2, 5, 6, XMM_REGS, "p", dst, "p", src, "p-", srcstride, "d", height, "p-", mx, rfilter
 %assign %%stride ((%2 + 7)/8)
     EPEL_FILTER       %2, mx, m4, m5, rfilter
 .loop:
@@ -747,7 +747,7 @@ cglobal hevc_put_hevc_epel_h%1_%2, 5, 6, XMM_REGS, dst, src, srcstride, height,
     LOOP_END         dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_epel_h%1_%2, 6, 7, XMM_REGS, dst, dststride, src, srcstride, height, mx, rfilter
+cglobal hevc_put_hevc_uni_epel_h%1_%2, 6, 7, XMM_REGS, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "p-", mx, rfilter
 %assign %%stride ((%2 + 7)/8)
     movdqa            m6, [pw_%2]
     EPEL_FILTER       %2, mx, m4, m5, rfilter
@@ -762,7 +762,7 @@ cglobal hevc_put_hevc_uni_epel_h%1_%2, 6, 7, XMM_REGS, dst, dststride, src, srcs
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_epel_h%1_%2, 7, 8, XMM_REGS, dst, dststride, src, srcstride, src2, height, mx, rfilter
+cglobal hevc_put_hevc_bi_epel_h%1_%2, 7, 8, XMM_REGS, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, "p-", mx, rfilter
     movdqa            m6, [pw_bi_%2]
     EPEL_FILTER       %2, mx, m4, m5, rfilter
 .loop:
@@ -784,7 +784,7 @@ cglobal hevc_put_hevc_bi_epel_h%1_%2, 7, 8, XMM_REGS, dst, dststride, src, srcst
 ;                      int height, int mx, int my, int width)
 ; ******************************
 
-cglobal hevc_put_hevc_epel_v%1_%2, 4, 6, XMM_REGS, dst, src, srcstride, height, r3src, my
+cglobal hevc_put_hevc_epel_v%1_%2, 4, 6, XMM_REGS, "p", dst, "p", src, "p-", srcstride, "d", height, r3src, my
     movifnidn        myd, mym
     sub             srcq, srcstrideq
     EPEL_FILTER       %2, my, m4, m5, r3src
@@ -796,7 +796,7 @@ cglobal hevc_put_hevc_epel_v%1_%2, 4, 6, XMM_REGS, dst, src, srcstride, height,
     LOOP_END          dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_epel_v%1_%2, 5, 7, XMM_REGS, dst, dststride, src, srcstride, height, r3src, my
+cglobal hevc_put_hevc_uni_epel_v%1_%2, 5, 7, XMM_REGS, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, r3src, my
     movifnidn        myd, mym
     movdqa            m6, [pw_%2]
     sub             srcq, srcstrideq
@@ -814,7 +814,7 @@ cglobal hevc_put_hevc_uni_epel_v%1_%2, 5, 7, XMM_REGS, dst, dststride, src, srcs
     RET
 
 
-cglobal hevc_put_hevc_bi_epel_v%1_%2, 6, 8, XMM_REGS, dst, dststride, src, srcstride, src2, height, r3src, my
+cglobal hevc_put_hevc_bi_epel_v%1_%2, 6, 8, XMM_REGS, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, r3src, my
     movifnidn        myd, mym
     movdqa            m6, [pw_bi_%2]
     sub             srcq, srcstrideq
@@ -842,7 +842,7 @@ cglobal hevc_put_hevc_bi_epel_v%1_%2, 6, 8, XMM_REGS, dst, dststride, src, srcst
 ; ******************************
 
 %macro HEVC_PUT_HEVC_EPEL_HV 2
-cglobal hevc_put_hevc_epel_hv%1_%2, 6, 7, 16 , dst, src, srcstride, height, mx, my, r3src
+cglobal hevc_put_hevc_epel_hv%1_%2, 6, 7, 16, "p", dst, "p", src, "p-", srcstride, "d", height, "p-", mx, "p-", my, r3src
 %assign %%stride ((%2 + 7)/8)
     sub             srcq, srcstrideq
     EPEL_HV_FILTER    %2
@@ -908,7 +908,7 @@ cglobal hevc_put_hevc_epel_hv%1_%2, 6, 7, 16 , dst, src, srcstride, height, mx,
     LOOP_END         dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_epel_hv%1_%2, 7, 8, 16 , dst, dststride, src, srcstride, height, mx, my, r3src
+cglobal hevc_put_hevc_uni_epel_hv%1_%2, 7, 8, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "p-", mx, "p-", my, r3src
 %assign %%stride ((%2 + 7)/8)
     sub             srcq, srcstrideq
     EPEL_HV_FILTER    %2
@@ -972,7 +972,7 @@ cglobal hevc_put_hevc_uni_epel_hv%1_%2, 7, 8, 16 , dst, dststride, src, srcstrid
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_epel_hv%1_%2, 8, 9, 16, dst, dststride, src, srcstride, src2, height, mx, my, r3src
+cglobal hevc_put_hevc_bi_epel_hv%1_%2, 8, 9, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, "p-", mx, "p-", my, r3src
 %assign %%stride ((%2 + 7)/8)
     sub             srcq, srcstrideq
     EPEL_HV_FILTER    %2
@@ -1053,7 +1053,7 @@ cglobal hevc_put_hevc_bi_epel_hv%1_%2, 8, 9, 16, dst, dststride, src, srcstride,
 ; ******************************
 
 %macro HEVC_PUT_HEVC_QPEL 2
-cglobal hevc_put_hevc_qpel_h%1_%2, 5, 6, 16, dst, src, srcstride, height, mx, rfilter
+cglobal hevc_put_hevc_qpel_h%1_%2, 5, 6, 16, "p", dst, "p", src, "p-", srcstride, "d", height, "p-", mx, rfilter
     QPEL_FILTER       %2, mx
 .loop:
     QPEL_H_LOAD       %2, srcq, %1, 10
@@ -1065,7 +1065,7 @@ cglobal hevc_put_hevc_qpel_h%1_%2, 5, 6, 16, dst, src, srcstride, height, mx, rf
     LOOP_END          dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_qpel_h%1_%2, 6, 7, 16 , dst, dststride, src, srcstride, height, mx, rfilter
+cglobal hevc_put_hevc_uni_qpel_h%1_%2, 6, 7, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "p-", mx, rfilter
     mova              m9, [pw_%2]
     QPEL_FILTER       %2, mx
 .loop:
@@ -1082,7 +1082,7 @@ cglobal hevc_put_hevc_uni_qpel_h%1_%2, 6, 7, 16 , dst, dststride, src, srcstride
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_qpel_h%1_%2, 7, 8, 16 , dst, dststride, src, srcstride, src2, height, mx, rfilter
+cglobal hevc_put_hevc_bi_qpel_h%1_%2, 7, 8, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, "p-", mx, rfilter
     movdqa            m9, [pw_bi_%2]
     QPEL_FILTER       %2, mx
 .loop:
@@ -1108,7 +1108,7 @@ cglobal hevc_put_hevc_bi_qpel_h%1_%2, 7, 8, 16 , dst, dststride, src, srcstride,
 ;                       int height, int mx, int my, int width)
 ; ******************************
 
-cglobal hevc_put_hevc_qpel_v%1_%2, 4, 8, 16, dst, src, srcstride, height, r3src, my, rfilter
+cglobal hevc_put_hevc_qpel_v%1_%2, 4, 8, 16, "p", dst, "p", src, "p-", srcstride, "d", height, r3src, my, rfilter
     movifnidn        myd, mym
     lea           r3srcq, [srcstrideq*3]
     QPEL_FILTER       %2, my
@@ -1122,7 +1122,7 @@ cglobal hevc_put_hevc_qpel_v%1_%2, 4, 8, 16, dst, src, srcstride, height, r3src,
     LOOP_END         dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_qpel_v%1_%2, 5, 9, 16, dst, dststride, src, srcstride, height, r3src, my, rfilter
+cglobal hevc_put_hevc_uni_qpel_v%1_%2, 5, 9, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, r3src, my, rfilter
     movifnidn        myd, mym
     movdqa            m9, [pw_%2]
     lea           r3srcq, [srcstrideq*3]
@@ -1141,7 +1141,7 @@ cglobal hevc_put_hevc_uni_qpel_v%1_%2, 5, 9, 16, dst, dststride, src, srcstride,
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_qpel_v%1_%2, 6, 10, 16, dst, dststride, src, srcstride, src2, height, r3src, my, rfilter
+cglobal hevc_put_hevc_bi_qpel_v%1_%2, 6, 10, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, r3src, my, rfilter
     movifnidn        myd, mym
     movdqa            m9, [pw_bi_%2]
     lea           r3srcq, [srcstrideq*3]
@@ -1170,7 +1170,7 @@ cglobal hevc_put_hevc_bi_qpel_v%1_%2, 6, 10, 16, dst, dststride, src, srcstride,
 ;                       int height, int mx, int my)
 ; ******************************
 %macro HEVC_PUT_HEVC_QPEL_HV 2
-cglobal hevc_put_hevc_qpel_hv%1_%2, 6, 8, 16, dst, src, srcstride, height, mx, my, r3src, rfilter
+cglobal hevc_put_hevc_qpel_hv%1_%2, 6, 8, 16, "p", dst, "p", src, "p-", srcstride, "d", height, "p-", mx, "p-", my, r3src, rfilter
 %if cpuflag(avx2)
 %assign %%shift  4
 %else
@@ -1246,7 +1246,7 @@ cglobal hevc_put_hevc_qpel_hv%1_%2, 6, 8, 16, dst, src, srcstride, height, mx, m
     LOOP_END         dst, src, srcstride
     RET
 
-cglobal hevc_put_hevc_uni_qpel_hv%1_%2, 7, 9, 16 , dst, dststride, src, srcstride, height, mx, my, r3src, rfilter
+cglobal hevc_put_hevc_uni_qpel_hv%1_%2, 7, 9, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "p-", mx, "p-", my, r3src, rfilter
 %if cpuflag(avx2)
 %assign %%shift  4
 %else
@@ -1327,7 +1327,7 @@ cglobal hevc_put_hevc_uni_qpel_hv%1_%2, 7, 9, 16 , dst, dststride, src, srcstrid
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_qpel_hv%1_%2, 8, 10, 16, dst, dststride, src, srcstride, src2, height, mx, my, r3src, rfilter
+cglobal hevc_put_hevc_bi_qpel_hv%1_%2, 8, 10, 16, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "p", src2, "d", height, "p-", mx, "p-", my, r3src, rfilter
 %if cpuflag(avx2)
 %assign %%shift  4
 %else
@@ -1413,11 +1413,11 @@ cglobal hevc_put_hevc_bi_qpel_hv%1_%2, 8, 10, 16, dst, dststride, src, srcstride
 
 %macro WEIGHTING_FUNCS 2
 %if WIN64 || ARCH_X86_32
-cglobal hevc_put_hevc_uni_w%1_%2, 4, 5, 7, dst, dststride, src, height, denom, wx, ox
+cglobal hevc_put_hevc_uni_w%1_%2, 4, 5, 7, "p", dst, "p-", dststride, "p", src, "d", height, denom, wx, ox
     mov             r4d, denomm
 %define SHIFT  r4d
 %else
-cglobal hevc_put_hevc_uni_w%1_%2, 6, 6, 7, dst, dststride, src, height, denom, wx, ox
+cglobal hevc_put_hevc_uni_w%1_%2, 6, 6, 7, "p", dst, "p-", dststride, "p", src, "d", height, "d", denom, "d", wx, ox
 %define SHIFT  denomd
 %endif
     lea           SHIFT, [SHIFT+14-%2]          ; shift = 14 - bitd + denom
@@ -1478,7 +1478,7 @@ cglobal hevc_put_hevc_uni_w%1_%2, 6, 6, 7, dst, dststride, src, height, denom, w
     jnz               .loop                      ; height loop
     RET
 
-cglobal hevc_put_hevc_bi_w%1_%2, 4, 6, 10, dst, dststride, src, src2, height, denom, wx0, wx1, ox0, ox1
+cglobal hevc_put_hevc_bi_w%1_%2, 4, 6, 10, "p", dst, "p-", dststride, "p", src, "p", src2, height, denom, wx0, wx1, ox0, ox1
     movifnidn        r5d, denomm
 %if %1 <= 4
     pxor              m1, m1
diff --git a/libavcodec/x86/hevc_sao.asm b/libavcodec/x86/hevc_sao.asm
index 756adfee57..8496b72bd3 100644
--- a/libavcodec/x86/hevc_sao.asm
+++ b/libavcodec/x86/hevc_sao.asm
@@ -122,7 +122,7 @@ DEFINE_ARGS dst, src, dststride, srcstride, offset, height
 ;void ff_hevc_sao_band_filter_<width>_8_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
 ;                                             int16_t *sao_offset_val, int sao_left_class, int width, int height);
 %macro HEVC_SAO_BAND_FILTER 2
-cglobal hevc_sao_band_filter_%1_8, 6, 6, 15, 7*mmsize*ARCH_X86_32, dst, src, dststride, srcstride, offset, left
+cglobal hevc_sao_band_filter_%1_8, 6, 6, 15, 7*mmsize*ARCH_X86_32, "p", dst, "p", src, "p-", dststride, "p-", srcstride, "p", offset, "d", left
     HEVC_SAO_BAND_FILTER_INIT
 
 align 16
@@ -253,13 +253,13 @@ HEVC_SAO_BAND_FILTER 64, 2
 ;                                             int eo, int width, int height);
 %macro HEVC_SAO_EDGE_FILTER 2-3
 %if ARCH_X86_64
-cglobal hevc_sao_edge_filter_%1_8, 4, 9, 8, dst, src, dststride, offset, eo, a_stride, b_stride, height, tmp
+cglobal hevc_sao_edge_filter_%1_8, 4, 9, 8, "p", dst, "p", src, "p-", dststride, "p", offset, "d", eo, a_stride, b_stride, height, tmp
 %define tmp2q heightq
     HEVC_SAO_EDGE_FILTER_INIT
     mov          heightd, r6m
 
 %else ; ARCH_X86_32
-cglobal hevc_sao_edge_filter_%1_8, 1, 6, 8, dst, src, dststride, a_stride, b_stride, height
+cglobal hevc_sao_edge_filter_%1_8, 1, 6, 8, "p", dst, "p", src, "p-", dststride, a_stride, b_stride, height
 %define eoq   srcq
 %define tmpq  heightq
 %define tmp2q dststrideq
diff --git a/libavcodec/x86/hevc_sao_10bit.asm b/libavcodec/x86/hevc_sao_10bit.asm
index b30583dd2f..878384aa3c 100644
--- a/libavcodec/x86/hevc_sao_10bit.asm
+++ b/libavcodec/x86/hevc_sao_10bit.asm
@@ -95,7 +95,7 @@ DEFINE_ARGS dst, src, dststride, srcstride, offset, height
 ;void ff_hevc_sao_band_filter_<width>_<depth>_<opt>(uint8_t *_dst, uint8_t *_src, ptrdiff_t _stride_dst, ptrdiff_t _stride_src,
 ;                                                   int16_t *sao_offset_val, int sao_left_class, int width, int height);
 %macro HEVC_SAO_BAND_FILTER 3
-cglobal hevc_sao_band_filter_%2_%1, 6, 6, 15, 7*mmsize*ARCH_X86_32, dst, src, dststride, srcstride, offset, left
+cglobal hevc_sao_band_filter_%2_%1, 6, 6, 15, 7*mmsize*ARCH_X86_32, "p", dst, "p", src, "p-", dststride, "p-", srcstride, "p", offset, "d", left
     HEVC_SAO_BAND_FILTER_INIT %1
 
 align 16
@@ -225,7 +225,7 @@ HEVC_SAO_BAND_FILTER 12, 64, 4
 ;                                                   int eo, int width, int height);
 %macro HEVC_SAO_EDGE_FILTER 3
 %if ARCH_X86_64
-cglobal hevc_sao_edge_filter_%2_%1, 4, 9, 16, dst, src, dststride, offset, eo, a_stride, b_stride, height, tmp
+cglobal hevc_sao_edge_filter_%2_%1, 4, 9, 16, "p", dst, "p", src, "p-", dststride, "p", offset, "d", eo, a_stride, b_stride, height, tmp
 %define tmp2q heightq
     HEVC_SAO_EDGE_FILTER_INIT
     mov          heightd, r6m
@@ -233,7 +233,7 @@ cglobal hevc_sao_edge_filter_%2_%1, 4, 9, 16, dst, src, dststride, offset, eo, a
     add        b_strideq, b_strideq
 
 %else ; ARCH_X86_32
-cglobal hevc_sao_edge_filter_%2_%1, 1, 6, 8, 5*mmsize, dst, src, dststride, a_stride, b_stride, height
+cglobal hevc_sao_edge_filter_%2_%1, 1, 6, 8, 5*mmsize, "p", dst, "p", src, "p-", dststride, a_stride, b_stride, height
 %define eoq   srcq
 %define tmpq  heightq
 %define tmp2q dststrideq
diff --git a/libavcodec/x86/hpeldsp.asm b/libavcodec/x86/hpeldsp.asm
index ce5d7a4e28..71bc143634 100644
--- a/libavcodec/x86/hpeldsp.asm
+++ b/libavcodec/x86/hpeldsp.asm
@@ -40,9 +40,9 @@ SECTION .text
 ; void ff_put_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_PIXELS8_X2 0
 %if cpuflag(sse2)
-cglobal put_pixels16_x2, 4,5,4
+cglobal put_pixels16_x2, 4,5,4, "p", block, "p", pixels, "p-", line_size, "d", h
 %else
-cglobal put_pixels8_x2, 4,5
+cglobal put_pixels8_x2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
 %endif
     lea          r4, [r2*2]
 .loop:
@@ -89,7 +89,7 @@ PUT_PIXELS8_X2
 
 ; void ff_put_pixels16_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_PIXELS_16 0
-cglobal put_pixels16_x2, 4,5
+cglobal put_pixels16_x2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     lea          r4, [r2*2]
 .loop:
     mova         m0, [r1]
@@ -136,7 +136,7 @@ PUT_PIXELS8_X2
 
 ; void ff_put_no_rnd_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_NO_RND_PIXELS8_X2 0
-cglobal put_no_rnd_pixels8_x2, 4,5
+cglobal put_no_rnd_pixels8_x2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     mova         m6, [pb_1]
     lea          r4, [r2*2]
 .loop:
@@ -178,9 +178,9 @@ PUT_NO_RND_PIXELS8_X2
 ; void ff_put_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_PIXELS8_Y2 0
 %if cpuflag(sse2)
-cglobal put_pixels16_y2, 4,5,3
+cglobal put_pixels16_y2, 4,5,3, "p", block, "p", pixels, "p-", line_size, "d", h
 %else
-cglobal put_pixels8_y2, 4,5
+cglobal put_pixels8_y2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
 %endif
     lea          r4, [r2*2]
     movu         m0, [r1]
@@ -218,7 +218,7 @@ PUT_PIXELS8_Y2
 
 ; void ff_put_no_rnd_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_NO_RND_PIXELS8_Y2 0
-cglobal put_no_rnd_pixels8_y2, 4,5
+cglobal put_no_rnd_pixels8_y2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     mova         m6, [pb_1]
     lea          r4, [r2+r2]
     mova         m0, [r1]
@@ -255,7 +255,7 @@ PUT_NO_RND_PIXELS8_Y2
 
 ; void ff_avg_pixels8(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro AVG_PIXELS8 0
-cglobal avg_pixels8, 4,5
+cglobal avg_pixels8, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     lea          r4, [r2*2]
 .loop:
     mova         m0, [r0]
@@ -286,9 +286,9 @@ AVG_PIXELS8
 ; void ff_avg_pixels8_x2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro AVG_PIXELS8_X2 0
 %if cpuflag(sse2)
-cglobal avg_pixels16_x2, 4,5,4
+cglobal avg_pixels16_x2, 4,5,4, "p", block, "p", pixels, "p-", line_size, "d", h
 %else
-cglobal avg_pixels8_x2, 4,5
+cglobal avg_pixels8_x2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
 %endif
     lea          r4, [r2*2]
 %if notcpuflag(mmxext)
@@ -349,9 +349,9 @@ AVG_PIXELS8_X2
 ; void ff_avg_pixels8_y2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro AVG_PIXELS8_Y2 0
 %if cpuflag(sse2)
-cglobal avg_pixels16_y2, 4,5,3
+cglobal avg_pixels16_y2, 4,5,3, "p", block, "p", pixels, "p-", line_size, "d", h
 %else
-cglobal avg_pixels8_y2, 4,5
+cglobal avg_pixels8_y2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
 %endif
     lea          r4, [r2*2]
     movu         m0, [r1]
@@ -395,7 +395,7 @@ AVG_PIXELS8_Y2
 ; Note this is not correctly rounded, and is therefore used for
 ; not-bitexact output
 %macro AVG_APPROX_PIXELS8_XY2 0
-cglobal avg_approx_pixels8_xy2, 4,5
+cglobal avg_approx_pixels8_xy2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     mova         m6, [pb_1]
     lea          r4, [r2*2]
     mova         m0, [r1]
@@ -440,9 +440,9 @@ AVG_APPROX_PIXELS8_XY2
 ; void ff_avg_pixels16_xy2(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro SET_PIXELS_XY2 1
 %if cpuflag(sse2)
-cglobal %1_pixels16_xy2, 4,5,8
+cglobal %1_pixels16_xy2, 4,5,8, "p", block, "p", pixels, "p-", line_size, "d", h
 %else
-cglobal %1_pixels8_xy2, 4,5
+cglobal %1_pixels8_xy2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
 %endif
     pxor        m7, m7
     mova        m6, [pw_2]
@@ -525,10 +525,10 @@ SET_PIXELS_XY2 avg
 
 %macro SSSE3_PIXELS_XY2 1-2
 %if %0 == 2 ; sse2
-cglobal %1_pixels16_xy2, 4,5,%2
+cglobal %1_pixels16_xy2, 4,5,%2, "p", block, "p", pixels, "p-", line_size, "d", h
     mova        m4, [pb_interleave16]
 %else
-cglobal %1_pixels8_xy2, 4,5
+cglobal %1_pixels8_xy2, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     mova        m4, [pb_interleave8]
 %endif
     mova        m5, [pb_1]
diff --git a/libavcodec/x86/hpeldsp_vp3.asm b/libavcodec/x86/hpeldsp_vp3.asm
index cba96d06cb..5cebd73f50 100644
--- a/libavcodec/x86/hpeldsp_vp3.asm
+++ b/libavcodec/x86/hpeldsp_vp3.asm
@@ -24,7 +24,7 @@ SECTION .text
 
 ; void ff_put_no_rnd_pixels8_x2_exact(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_NO_RND_PIXELS8_X2_EXACT 0
-cglobal put_no_rnd_pixels8_x2_exact, 4,5
+cglobal put_no_rnd_pixels8_x2_exact, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     lea          r4, [r2*3]
     pcmpeqb      m6, m6
 .loop:
@@ -71,7 +71,7 @@ PUT_NO_RND_PIXELS8_X2_EXACT
 
 ; void ff_put_no_rnd_pixels8_y2_exact(uint8_t *block, const uint8_t *pixels, ptrdiff_t line_size, int h)
 %macro PUT_NO_RND_PIXELS8_Y2_EXACT 0
-cglobal put_no_rnd_pixels8_y2_exact, 4,5
+cglobal put_no_rnd_pixels8_y2_exact, 4,5, "p", block, "p", pixels, "p-", line_size, "d", h
     lea          r4, [r2*3]
     mova         m0, [r1]
     pcmpeqb      m6, m6
diff --git a/libavcodec/x86/huffyuvdsp.asm b/libavcodec/x86/huffyuvdsp.asm
index a1231f1b22..42dcecfdde 100644
--- a/libavcodec/x86/huffyuvdsp.asm
+++ b/libavcodec/x86/huffyuvdsp.asm
@@ -31,7 +31,7 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 %macro ADD_INT16 0
-cglobal add_int16, 4,4,5, dst, src, mask, w, tmp
+cglobal add_int16, 4,4,5, "p", dst, "p", src, "d", mask, "d", w, tmp
 %if mmsize > 8
     test srcq, mmsize-1
     jnz .unaligned
@@ -61,7 +61,7 @@ ADD_INT16
 ; void add_hfyu_left_pred_bgr32(uint8_t *dst, const uint8_t *src,
 ;                               intptr_t w, uint8_t *left)
 %macro LEFT_BGR32 0
-cglobal add_hfyu_left_pred_bgr32, 4,4,3, dst, src, w, left
+cglobal add_hfyu_left_pred_bgr32, 4,4,3, "p", dst, "p", src, "p-", w, "p", left
     shl           wq, 2
     movd          m0, [leftq]
     lea         dstq, [dstq + wq]
@@ -100,7 +100,7 @@ LEFT_BGR32
 
 ; void add_hfyu_median_prediction_mmxext(uint8_t *dst, const uint8_t *top, const uint8_t *diff, int mask, int w, int *left, int *left_top)
 INIT_MMX mmxext
-cglobal add_hfyu_median_pred_int16, 7,7,0, dst, top, diff, mask, w, left, left_top
+cglobal add_hfyu_median_pred_int16, 7,7,0, "p", dst, "p", top, "p", diff, "d", mask, "d", w, "p", left, "p", left_top
     add      wd, wd
     movd    mm6, maskd
     SPLATW  mm6, mm6
diff --git a/libavcodec/x86/huffyuvencdsp.asm b/libavcodec/x86/huffyuvencdsp.asm
index d994fd0fd6..af2f07231d 100644
--- a/libavcodec/x86/huffyuvencdsp.asm
+++ b/libavcodec/x86/huffyuvencdsp.asm
@@ -35,7 +35,7 @@ SECTION .text
 ;------------------------------------------------------------------------------
 
 %macro DIFF_INT16 0
-cglobal diff_int16, 5,5,5, dst, src1, src2, mask, w, tmp
+cglobal diff_int16, 5,5,5, "p", dst, "p", src1, "p", src2, "d", mask, "d", w, tmp
 %if mmsize > 8
     test src1q, mmsize-1
     jnz .unaligned
@@ -65,7 +65,7 @@ DIFF_INT16
 %endif
 
 INIT_MMX mmxext
-cglobal sub_hfyu_median_pred_int16, 7,7,0, dst, src1, src2, mask, w, left, left_top
+cglobal sub_hfyu_median_pred_int16, 7,7,0, "p", dst, "p", src1, "p", src2, "d", mask, "d", w, "p", left, "p", left_top
     add      wd, wd
     movd    mm7, maskd
     SPLATW  mm7, mm7
diff --git a/libavcodec/x86/idctdsp.asm b/libavcodec/x86/idctdsp.asm
index 089425a9ab..94b7c997cb 100644
--- a/libavcodec/x86/idctdsp.asm
+++ b/libavcodec/x86/idctdsp.asm
@@ -65,7 +65,7 @@ SECTION .text
 %endmacro
 
 %macro PUT_SIGNED_PIXELS_CLAMPED 1
-cglobal put_signed_pixels_clamped, 3, 4, %1, block, pixels, lsize, lsize3
+cglobal put_signed_pixels_clamped, 3, 4, %1, "p", block, "p", pixels, "p-", lsize, lsize3
     mova     m0, [pb_80]
     lea      lsize3q, [lsizeq*3]
     PUT_SIGNED_PIXELS_CLAMPED_HALF 0
@@ -109,7 +109,7 @@ PUT_SIGNED_PIXELS_CLAMPED 3
 %endmacro
 
 %macro PUT_PIXELS_CLAMPED 0
-cglobal put_pixels_clamped, 3, 4, 2, block, pixels, lsize, lsize3
+cglobal put_pixels_clamped, 3, 4, 2, "p", block, "p", pixels, "p-", lsize, lsize3
     lea lsize3q, [lsizeq*3]
     PUT_PIXELS_CLAMPED_HALF 0
     lea pixelsq, [pixelsq+lsizeq*4]
@@ -165,7 +165,7 @@ PUT_PIXELS_CLAMPED
 %endmacro
 
 %macro ADD_PIXELS_CLAMPED 0
-cglobal add_pixels_clamped, 3, 3, 5, block, pixels, lsize
+cglobal add_pixels_clamped, 3, 3, 5, "p", block, "p", pixels, "p-", lsize
     pxor       m4, m4
     ADD_PIXELS_CLAMPED 0
     lea        pixelsq, [pixelsq+lsizeq*2]
diff --git a/libavcodec/x86/imdct36.asm b/libavcodec/x86/imdct36.asm
index b386ab95fc..ea272ad1db 100644
--- a/libavcodec/x86/imdct36.asm
+++ b/libavcodec/x86/imdct36.asm
@@ -176,7 +176,7 @@ SECTION .text
 %endmacro
 
 %macro DEFINE_IMDCT 0
-cglobal imdct36_float, 4,4,9, out, buf, in, win
+cglobal imdct36_float, 4,4,9, "p", out, "p", buf, "p", in, "p", win
 
     ; for(i=17;i>=1;i--) in[i] += in[i-1];
     LOADA64 m0, inq
@@ -409,7 +409,7 @@ INIT_XMM sse
 %endif
 
 %macro DEFINE_FOUR_IMDCT 0
-cglobal four_imdct36_float, 5,5,16, out, buf, in, win, tmp
+cglobal four_imdct36_float, 5,5,16, "p", out, "p", buf, "p", in, "p", win, "p", tmp
     movlps  m0, [inq+64]
     movhps  m0, [inq+64 +   72]
     movlps  m3, [inq+64 + 2*72]
diff --git a/libavcodec/x86/jpeg2000dsp.asm b/libavcodec/x86/jpeg2000dsp.asm
index 61dfdd4f71..60ae34b2ad 100644
--- a/libavcodec/x86/jpeg2000dsp.asm
+++ b/libavcodec/x86/jpeg2000dsp.asm
@@ -35,7 +35,7 @@ SECTION .text
 ; ff_ict_float_<opt>(float *src0, float *src1, float *src2, int csize)
 ;***********************************************************************
 %macro ICT_FLOAT 1
-cglobal ict_float, 4, 4, %1, src0, src1, src2, csize
+cglobal ict_float, 4, 4, %1, "p", src0, "p", src1, "p", src2, "d", csize
     shl  csized, 2
     add   src0q, csizeq
     add   src1q, csizeq
@@ -131,7 +131,7 @@ ICT_FLOAT 9
 ; ff_rct_int_<opt>(int32_t *src0, int32_t *src1, int32_t *src2, int csize)
 ;***************************************************************************
 %macro RCT_INT 0
-cglobal rct_int, 4, 4, 4, src0, src1, src2, csize
+cglobal rct_int, 4, 4, 4, "p", src0, "p", src1, "p", src2, "d", csize
     shl  csized, 2
     add   src0q, csizeq
     add   src1q, csizeq
diff --git a/libavcodec/x86/lossless_audiodsp.asm b/libavcodec/x86/lossless_audiodsp.asm
index 063d7b41af..08edf6546b 100644
--- a/libavcodec/x86/lossless_audiodsp.asm
+++ b/libavcodec/x86/lossless_audiodsp.asm
@@ -25,7 +25,7 @@ SECTION .text
 %macro SCALARPRODUCT 0
 ; int ff_scalarproduct_and_madd_int16(int16_t *v1, int16_t *v2, int16_t *v3,
 ;                                     int order, int mul)
-cglobal scalarproduct_and_madd_int16, 4,4,8, v1, v2, v3, order, mul
+cglobal scalarproduct_and_madd_int16, 4,4,8, "p", v1, "p", v2, "p", v3, "d-", order, "d", mul
     shl orderq, 1
     movd    m7, mulm
 %if mmsize == 16
@@ -71,7 +71,7 @@ SCALARPRODUCT
 INIT_XMM sse4
 ; int ff_scalarproduct_and_madd_int32(int16_t *v1, int32_t *v2, int16_t *v3,
 ;                                     int order, int mul)
-cglobal scalarproduct_and_madd_int32, 4,4,8, v1, v2, v3, order, mul
+cglobal scalarproduct_and_madd_int32, 4,4,8, "p", v1, "p", v2, "p", v3, "d-", order, "d", mul
     shl orderq, 1
     movd    m7, mulm
     SPLATW  m7, m7
@@ -149,7 +149,7 @@ align 16
 ; int ff_scalarproduct_and_madd_int16(int16_t *v1, int16_t *v2, int16_t *v3,
 ;                                     int order, int mul)
 INIT_XMM ssse3
-cglobal scalarproduct_and_madd_int16, 4,5,10, v1, v2, v3, order, mul
+cglobal scalarproduct_and_madd_int16, 4,5,10, "p", v1, "p", v2, "p", v3, "d-", order, "d", mul
     shl orderq, 1
     movd    m7, mulm
     pshuflw m7, m7, 0
diff --git a/libavcodec/x86/lossless_videodsp.asm b/libavcodec/x86/lossless_videodsp.asm
index 0a1b7091c9..e0b84d43fb 100644
--- a/libavcodec/x86/lossless_videodsp.asm
+++ b/libavcodec/x86/lossless_videodsp.asm
@@ -39,11 +39,11 @@ SECTION .text
 
 ;------------------------------------------------------------------------------
 ; void ff_add_median_pred_mmxext(uint8_t *dst, const uint8_t *top,
-;                                const uint8_t *diff, int w,
+;                                const uint8_t *diff, ptrdiff_t w,
 ;                                int *left, int *left_top)
 ;------------------------------------------------------------------------------
 %macro MEDIAN_PRED 0
-cglobal add_median_pred, 6,6,8, dst, top, diff, w, left, left_top
+cglobal add_median_pred, 6,6,8, "p", dst, "p", top, "p", diff, "p-", w, "p", left, "p", left_top
     movu    m0, [topq]
     mova    m2, m0
     movd    m4, [left_topq]
@@ -166,10 +166,10 @@ MEDIAN_PRED
 %endmacro
 
 ;------------------------------------------------------------------------------
-; int ff_add_left_pred(uint8_t *dst, const uint8_t *src, int w, int left)
+; int ff_add_left_pred(uint8_t *dst, const uint8_t *src, ptrdiff_t w, int left)
 ;------------------------------------------------------------------------------
 INIT_MMX ssse3
-cglobal add_left_pred, 3,3,7, dst, src, w, left
+cglobal add_left_pred, 3,3,7, "p", dst, "p", src, "p-", w, "d", left
 .skip_prologue:
     mova    m5, [pb_7]
     mova    m4, [pb_zzzz3333zzzzbbbb]
@@ -179,7 +179,7 @@ cglobal add_left_pred, 3,3,7, dst, src, w, left
     ADD_LEFT_LOOP 1, 1
 
 %macro ADD_LEFT_PRED_UNALIGNED 0
-cglobal add_left_pred_unaligned, 3,3,7, dst, src, w, left
+cglobal add_left_pred_unaligned, 3,3,7, "p", dst, "p", src, "p-", w, "d", left
     mova    xm5, [pb_15]
     VBROADCASTI128    m6, [pb_zzzzzzzz77777777]
     VBROADCASTI128    m4, [pb_zzzz3333zzzzbbbb]
@@ -209,7 +209,7 @@ ADD_LEFT_PRED_UNALIGNED
 ; void ff_add_bytes(uint8_t *dst, uint8_t *src, ptrdiff_t w);
 ;------------------------------------------------------------------------------
 %macro ADD_BYTES 0
-cglobal add_bytes, 3,4,2, dst, src, w, size
+cglobal add_bytes, 3,4,2, "p", dst, "p", src, "p-", w, size
     mov  sizeq, wq
     and  sizeq, -2*mmsize
     jz  .2
@@ -294,10 +294,10 @@ ADD_BYTES
 %endmacro
 
 ;---------------------------------------------------------------------------------------------
-; int add_left_pred_int16(uint16_t *dst, const uint16_t *src, unsigned mask, int w, int left)
+; int add_left_pred_int16(uint16_t *dst, const uint16_t *src, unsigned mask, ptrdiff_t w, int left)
 ;---------------------------------------------------------------------------------------------
 INIT_MMX ssse3
-cglobal add_left_pred_int16, 4,4,8, dst, src, mask, w, left
+cglobal add_left_pred_int16, 4,4,8, "p", dst, "p", src, "d", mask, "p-", w, "d", left
 .skip_prologue:
     mova    m5, [pb_67]
     mova    m3, [pb_zzzz2323zzzzabab]
@@ -308,7 +308,7 @@ cglobal add_left_pred_int16, 4,4,8, dst, src, mask, w, left
     ADD_HFYU_LEFT_LOOP_INT16 a, a
 
 INIT_XMM ssse3
-cglobal add_left_pred_int16_unaligned, 4,4,8, dst, src, mask, w, left
+cglobal add_left_pred_int16_unaligned, 4,4,8, "p", dst, "p", src, "d", mask, "p-", w, "d", left
     mova    m5, [pb_ef]
     mova    m4, [pb_zzzzzzzz67676767]
     mova    m3, [pb_zzzz2323zzzzabab]
@@ -331,7 +331,7 @@ cglobal add_left_pred_int16_unaligned, 4,4,8, dst, src, mask, w, left
 ; void add_gradient_pred(uint8_t *src, const ptrdiff_t stride, const ptrdiff_t width)
 ;---------------------------------------------------------------------------------------------
 %macro ADD_GRADIENT_PRED 0
-cglobal add_gradient_pred, 3,4,5, src, stride, width, tmp
+cglobal add_gradient_pred, 3,4,5, "p", src, "p-", stride, "p-", width, tmp
     mova         xm0, [pb_15]
 
 ;load src - 1 in xm1
diff --git a/libavcodec/x86/lossless_videoencdsp.asm b/libavcodec/x86/lossless_videoencdsp.asm
index fb1204f0f1..50ceeff596 100644
--- a/libavcodec/x86/lossless_videoencdsp.asm
+++ b/libavcodec/x86/lossless_videoencdsp.asm
@@ -33,12 +33,12 @@ SECTION .text
 ;                    intptr_t w);
 %macro DIFF_BYTES_PROLOGUE 0
 %if ARCH_X86_32
-cglobal diff_bytes, 3,5,2, dst, src1, src2
+cglobal diff_bytes, 3,5,2, "p", dst, "p", src1, "p", src2
 %define wq r4q
     DECLARE_REG_TMP 3
     mov               wq, r3mp
 %else
-cglobal diff_bytes, 4,5,2, dst, src1, src2, w
+cglobal diff_bytes, 4,5,2, "p", dst, "p", src1, "p", src2, "p-", w
     DECLARE_REG_TMP 4
 %endif ; ARCH_X86_32
 %define i t0q
@@ -158,7 +158,7 @@ DIFF_BYTES_PROLOGUE
 ;--------------------------------------------------------------------------------------------------
 
 INIT_XMM avx
-cglobal sub_left_predict, 5,6,5, dst, src, stride, width, height, x
+cglobal sub_left_predict, 5,6,5, "p", dst, "p", src, "p-", stride, "p-", width, "d", height, x
     mova             m1, [pb_80] ; prev initial
     add            dstq, widthq
     add            srcq, widthq
diff --git a/libavcodec/x86/mdct15.asm b/libavcodec/x86/mdct15.asm
index 2a2cdbd21b..00bed13577 100644
--- a/libavcodec/x86/mdct15.asm
+++ b/libavcodec/x86/mdct15.asm
@@ -111,7 +111,7 @@ SECTION .text
 %endmacro
 
 INIT_YMM avx
-cglobal fft15, 4, 5, 14, out, in, exptab, stride, stride5
+cglobal fft15, 4, 5, 14, "p", out, "p", in, "p", exptab, "p-", stride, stride5
     shl strideq, 3
 
     movaps xm5, [exptabq + 480 + 16*0]
@@ -159,7 +159,7 @@ cglobal fft15, 4, 5, 14, out, in, exptab, stride, stride5
 %endmacro
 
 %macro POSTROTATE_FN 1
-cglobal mdct15_postreindex, 5, 7, 8 + cpuflag(avx2)*2, out, in, exp, lut, len8, offset_p, offset_n
+cglobal mdct15_postreindex, 5, 7, 8 + cpuflag(avx2)*2, "p", out, "p", in, "p", exp, "p", lut, "p-", len8, offset_p, offset_n
 
     xor offset_nq, offset_nq
     lea offset_pq, [len8q*2 - %1]
diff --git a/libavcodec/x86/me_cmp.asm b/libavcodec/x86/me_cmp.asm
index ad06d485ab..fba61cbf75 100644
--- a/libavcodec/x86/me_cmp.asm
+++ b/libavcodec/x86/me_cmp.asm
@@ -148,7 +148,7 @@ SECTION .text
 %endmacro
 
 %macro hadamard8_16_wrapper 2
-cglobal hadamard8_diff, 4, 4, %1
+cglobal hadamard8_diff, 4, 4, %1, "p", s, "p", src1, "p", src2, "p-", stride
 %ifndef m8
     %assign pad %2*mmsize-(4+stack_offset&(mmsize-1))
     SUB            rsp, pad
@@ -159,7 +159,7 @@ cglobal hadamard8_diff, 4, 4, %1
 %endif
     RET
 
-cglobal hadamard8_diff16, 5, 6, %1
+cglobal hadamard8_diff16, 5, 6, %1, "p", s, "p", src1, "p", src2, "p-", stride, "d", h
 %ifndef m8
     %assign pad %2*mmsize-(4+stack_offset&(mmsize-1))
     SUB            rsp, pad
@@ -283,7 +283,7 @@ HADAMARD8_DIFF 9
 ;               ptrdiff_t line_size, int h)
 
 %macro SUM_SQUARED_ERRORS 1
-cglobal sse%1, 5,5,8, v, pix1, pix2, lsize, h
+cglobal sse%1, 5,5,8, "p", v, "p", pix1, "p", pix2, "p-", lsize, "d", h
 %if %1 == mmsize
     shr       hd, 1
 %endif
@@ -364,7 +364,7 @@ SUM_SQUARED_ERRORS 16
 ; %2 = number of inline loops
 
 %macro SUM_ABS_DCTELEM 2
-cglobal sum_abs_dctelem, 1, 1, %1, block
+cglobal sum_abs_dctelem, 1, 1, %1, "p", block
     pxor    m0, m0
     pxor    m1, m1
 %assign %%i 0
@@ -436,7 +436,7 @@ SUM_ABS_DCTELEM 6, 2
 
 ; %1 = 8/16
 %macro HF_NOISE 1
-cglobal hf_noise%1, 3,3,0, pix1, lsize, h
+cglobal hf_noise%1, 3,3,0, "p", pix1, "p-", lsize, "d", h
     sub        hd, 2
     pxor       m7, m7
     pxor       m6, m6
@@ -475,7 +475,7 @@ HF_NOISE 16
 ;---------------------------------------------------------------------------------------
 ;%1 = 8/16
 %macro SAD 1
-cglobal sad%1, 5, 5, 3, v, pix1, pix2, stride, h
+cglobal sad%1, 5, 5, 3, "p", v, "p", pix1, "p", pix2, "p-", stride, "d", h
     movu      m2, [pix2q]
     movu      m1, [pix2q+strideq]
     psadbw    m2, [pix1q]
@@ -530,7 +530,7 @@ SAD 16
 ;------------------------------------------------------------------------------------------
 ;%1 = 8/16
 %macro SAD_X2 1
-cglobal sad%1_x2, 5, 5, 5, v, pix1, pix2, stride, h
+cglobal sad%1_x2, 5, 5, 5, "p", v, "p", pix1, "p", pix2, "p-", stride, "d", h
     movu      m0, [pix2q]
     movu      m2, [pix2q+strideq]
 %if mmsize == 16
@@ -607,7 +607,7 @@ SAD_X2 16
 ;------------------------------------------------------------------------------------------
 ;%1 = 8/16
 %macro SAD_Y2 1
-cglobal sad%1_y2, 5, 5, 4, v, pix1, pix2, stride, h
+cglobal sad%1_y2, 5, 5, 4, "p", v, "p", pix1, "p", pix2, "p-", stride, "d", h
     movu      m1, [pix2q]
     movu      m0, [pix2q+strideq]
     movu      m3, [pix2q+2*strideq]
@@ -677,7 +677,7 @@ SAD_Y2 16
 ;-------------------------------------------------------------------------------------------
 ;%1 = 8/16
 %macro SAD_APPROX_XY2 1
-cglobal sad%1_approx_xy2, 5, 5, 7, v, pix1, pix2, stride, h
+cglobal sad%1_approx_xy2, 5, 5, 7, "p", v, "p", pix1, "p", pix2, "p-", stride, "d", h
     mova      m4, [pb_1]
     movu      m1, [pix2q]
     movu      m0, [pix2q+strideq]
@@ -779,7 +779,7 @@ SAD_APPROX_XY2 16
 ;--------------------------------------------------------------------
 ; %1 = 8/16
 %macro VSAD_INTRA 1
-cglobal vsad_intra%1, 5, 5, 3, v, pix1, pix2, lsize, h
+cglobal vsad_intra%1, 5, 5, 3, "p", v, "p", pix1, "p", pix2, "p-", lsize, "d", h
     mova      m0, [pix1q]
 %if %1 == mmsize
     mova      m2, [pix1q+lsizeq]
@@ -840,7 +840,7 @@ VSAD_INTRA 16
 ;---------------------------------------------------------------------
 ; %1 = 8/16
 %macro VSAD_APPROX 1
-cglobal vsad%1_approx, 5, 5, 5, v, pix1, pix2, lsize, h
+cglobal vsad%1_approx, 5, 5, 5, "p", v, "p", pix1, "p", pix2, "p-", lsize, "d", h
     mova   m1, [pb_80]
     mova   m0, [pix1q]
 %if %1 == mmsize ; vsad8_mmxext, vsad16_sse2
diff --git a/libavcodec/x86/mlpdsp.asm b/libavcodec/x86/mlpdsp.asm
index 3dc641e89e..c16d211109 100644
--- a/libavcodec/x86/mlpdsp.asm
+++ b/libavcodec/x86/mlpdsp.asm
@@ -95,9 +95,9 @@ SECTION .text
 ;                             unsigned int maxchan, int matrix_noise_shift,
 ;                             int access_unit_size_pow2, int32_t mask)
 %macro MLP_REMATRIX_CHANNEL 0
-cglobal mlp_rematrix_channel, 0, 13, 5, samples, coeffs, blsbs_ptr, blsbs, \
-                                        index, dest_ch, blockpos, maxchan, mns, \
-                                        accum, mask, cnt
+cglobal mlp_rematrix_channel, 0, 13, 5, "p", samples, "p", coeffs, "p", blsbs_ptr, "p", blsbs, \
+                                        "d", index, "d", dest_ch, "w", blockpos, "d", maxchan, "d", mns, \
+                                        "d", accum, "d", mask, cnt
     mov         mnsd, mnsm                          ; load matrix_noise_shift
     movzx  blockposq, word blockposm                ; load and zero extend blockpos (16bit)
     mov     maxchand, maxchanm                      ; load maxchan
diff --git a/libavcodec/x86/mpegvideoencdsp.asm b/libavcodec/x86/mpegvideoencdsp.asm
index aec73f82dc..e624e73d62 100644
--- a/libavcodec/x86/mpegvideoencdsp.asm
+++ b/libavcodec/x86/mpegvideoencdsp.asm
@@ -32,8 +32,7 @@ SECTION .text
 ; %1 = number of loops
 ; %2 = number of GPRs used
 %macro PIX_SUM16 3
-cglobal pix_sum16, 2, %2, 6
-    movsxdifnidn r1, r1d
+cglobal pix_sum16, 2, %2, 6, "p", pix, "d-", line_size
     mov          r2, %1
 %if mmsize == 16
     lea          r3, [r1*3]
@@ -111,8 +110,7 @@ PIX_SUM16  4, 4, 4
 ; %1 = number of xmm registers used
 ; %2 = number of loops
 %macro PIX_NORM1 2
-cglobal pix_norm1, 2, 3, %1
-    movsxdifnidn r1, r1d
+cglobal pix_norm1, 2, 3, %1, "p", pix, "d-", line_size
     mov          r2, %2
     pxor         m0, m0
     pxor         m5, m5
diff --git a/libavcodec/x86/opus_pvq_search.asm b/libavcodec/x86/opus_pvq_search.asm
index 5c1e6d6174..9811ed1da2 100644
--- a/libavcodec/x86/opus_pvq_search.asm
+++ b/libavcodec/x86/opus_pvq_search.asm
@@ -212,7 +212,7 @@ align 16
 ; uint32 N      - Number of vector elements. Must be 0 < N < 256
 ;
 %macro PVQ_FAST_SEARCH 1
-cglobal pvq_search%1, 4, 5+num_pic_regs, 11, 256*4, inX, outY, K, N
+cglobal pvq_search%1, 4, 5+num_pic_regs, 11, 256*4, "p", inX, "p", outY, "d", K, "d", N
 %define tmpX rsp
 %define tmpY outYq
 
diff --git a/libavcodec/x86/pixblockdsp.asm b/libavcodec/x86/pixblockdsp.asm
index 440fe29bcc..1179e07ff7 100644
--- a/libavcodec/x86/pixblockdsp.asm
+++ b/libavcodec/x86/pixblockdsp.asm
@@ -27,7 +27,7 @@ SECTION .text
 
 INIT_MMX mmx
 ; void ff_get_pixels_mmx(int16_t *block, const uint8_t *pixels, ptrdiff_t stride)
-cglobal get_pixels, 3,4
+cglobal get_pixels, 3,4, "p", block, "p", pixels, "p-", stride
     add          r0, 128
     mov          r3, -128
     pxor         m7, m7
@@ -50,7 +50,7 @@ cglobal get_pixels, 3,4
     REP_RET
 
 INIT_XMM sse2
-cglobal get_pixels, 3, 4, 5
+cglobal get_pixels, 3, 4, 5, "p", block, "p", pixels, "p-", stride
     lea          r3, [r2*3]
     pxor         m4, m4
     movh         m0, [r1]
@@ -83,7 +83,7 @@ cglobal get_pixels, 3, 4, 5
 ; void ff_diff_pixels_mmx(int16_t *block, const uint8_t *s1, const uint8_t *s2,
 ;                         ptrdiff_t stride);
 %macro DIFF_PIXELS 0
-cglobal diff_pixels, 4,5,5
+cglobal diff_pixels, 4,5,5, "p", block, "p", s1, "p", s2, "p-", stride
     pxor         m4, m4
     add          r0,  128
     mov          r4, -128
diff --git a/libavcodec/x86/pngdsp.asm b/libavcodec/x86/pngdsp.asm
index 50e4255dec..9d6d4b09d6 100644
--- a/libavcodec/x86/pngdsp.asm
+++ b/libavcodec/x86/pngdsp.asm
@@ -31,10 +31,7 @@ SECTION .text
 
 ; %1 = nr. of xmm registers used
 %macro ADD_BYTES_FN 1
-cglobal add_bytes_l2, 4, 6, %1, dst, src1, src2, wa, w, i
-%if ARCH_X86_64
-    movsxd             waq, wad
-%endif
+cglobal add_bytes_l2, 4, 6, %1, "p", dst, "p", src1, "p", src2, "d-", wa, w, i
     xor                 iq, iq
 
     ; vector loop
@@ -90,11 +87,7 @@ INIT_XMM sse2
 ADD_BYTES_FN 2
 
 %macro ADD_PAETH_PRED_FN 1
-cglobal add_png_paeth_prediction, 5, 7, %1, dst, src, top, w, bpp, end, cntr
-%if ARCH_X86_64
-    movsxd            bppq, bppd
-    movsxd              wq, wd
-%endif
+cglobal add_png_paeth_prediction, 5, 7, %1, "p", dst, "p", src, "p", top, "d-", w, "d-", bpp, end, cntr
     lea               endq, [dstq+wq-(mmsize/2-1)]
     sub               topq, dstq
     sub               srcq, dstq
diff --git a/libavcodec/x86/proresdsp.asm b/libavcodec/x86/proresdsp.asm
index 65c9fad51c..1c0443b3e6 100644
--- a/libavcodec/x86/proresdsp.asm
+++ b/libavcodec/x86/proresdsp.asm
@@ -53,7 +53,7 @@ SECTION .text
 define_constants _hi
 
 %macro idct_fn 0
-cglobal prores_idct_put_10, 4, 4, 15, pixels, lsize, block, qmat
+cglobal prores_idct_put_10, 4, 4, 15, "p", pixels, "p-", lsize, "p", block, "p", qmat
     IDCT_FN    pw_1, 15, pw_88, 18, "put", pw_4, pw_1019, r3
     RET
 %endmacro
diff --git a/libavcodec/x86/qpel.asm b/libavcodec/x86/qpel.asm
index 4e72d5084f..6d6a07bb93 100644
--- a/libavcodec/x86/qpel.asm
+++ b/libavcodec/x86/qpel.asm
@@ -48,9 +48,7 @@ SECTION .text
 ;                                   int dstStride, int src1Stride, int h)
 %macro PIXELS4_L2 1
 %define OP op_%1h
-cglobal %1_pixels4_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
+cglobal %1_pixels4_l2, 6,6, "p", dst, "p", src1, "p", src2, "d-", dstStride, "d-", src1Stride, "d", h
     test        r5d, 1
     je        .loop
     movd         m0, [r1]
@@ -92,9 +90,7 @@ PIXELS4_L2 avg
 ;                                   int dstStride, int src1Stride, int h)
 %macro PIXELS8_L2 1
 %define OP op_%1
-cglobal %1_pixels8_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
+cglobal %1_pixels8_l2, 6,6, "p", dst, "p", src1, "p", src2, "d-", dstStride, "d-", src1Stride, "d", h
     test        r5d, 1
     je        .loop
     mova         m0, [r1]
@@ -136,9 +132,7 @@ PIXELS8_L2 avg
 ;                                    int dstStride, int src1Stride, int h)
 %macro PIXELS16_L2 1
 %define OP op_%1
-cglobal %1_pixels16_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
+cglobal %1_pixels16_l2, 6,6, "p", dst, "p", src1, "p", src2, "d-", dstStride, "d-", src1Stride, "d", h
     test        r5d, 1
     je        .loop
     mova         m0, [r1]
diff --git a/libavcodec/x86/qpeldsp.asm b/libavcodec/x86/qpeldsp.asm
index 282faed14f..c6353dfac1 100644
--- a/libavcodec/x86/qpeldsp.asm
+++ b/libavcodec/x86/qpeldsp.asm
@@ -35,9 +35,7 @@ SECTION .text
 
 ; void ff_put_no_rnd_pixels8_l2(uint8_t *dst, uint8_t *src1, uint8_t *src2, int dstStride, int src1Stride, int h)
 %macro PUT_NO_RND_PIXELS8_L2 0
-cglobal put_no_rnd_pixels8_l2, 6,6
-    movsxdifnidn r4, r4d
-    movsxdifnidn r3, r3d
+cglobal put_no_rnd_pixels8_l2, 6,6, "p", dst, "p", src1, "p", src2, "d-", dstStride, "d-", src1Stride, "d", h
     pcmpeqb      m6, m6
     test        r5d, 1
     je .loop
@@ -101,9 +99,7 @@ PUT_NO_RND_PIXELS8_L2
 
 ; void ff_put_no_rnd_pixels16_l2(uint8_t *dst, uint8_t *src1, uint8_t *src2, int dstStride, int src1Stride, int h)
 %macro PUT_NO_RND_PIXELS16_l2 0
-cglobal put_no_rnd_pixels16_l2, 6,6
-    movsxdifnidn r3, r3d
-    movsxdifnidn r4, r4d
+cglobal put_no_rnd_pixels16_l2, 6,6, "p", dst, "p", src1, "p", src2, "d-", dstStride, "d-", src1Stride, "d", h
     pcmpeqb      m6, m6
     test        r5d, 1
     je .loop
@@ -170,9 +166,7 @@ INIT_MMX 3dnow
 PUT_NO_RND_PIXELS16_l2
 
 %macro MPEG4_QPEL16_H_LOWPASS 1
-cglobal %1_mpeg4_qpel16_h_lowpass, 5, 5, 0, 16
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
+cglobal %1_mpeg4_qpel16_h_lowpass, 5, 5, 0, 16, "p", dst, "p", src, "d-", dstStride, "d-", srcStride, "d", h
     pxor         m7, m7
 .loop:
     mova         m0, [r1]
@@ -303,9 +297,7 @@ MPEG4_QPEL16_H_LOWPASS put_no_rnd
 
 
 %macro MPEG4_QPEL8_H_LOWPASS 1
-cglobal %1_mpeg4_qpel8_h_lowpass, 5, 5, 0, 8
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
+cglobal %1_mpeg4_qpel8_h_lowpass, 5, 5, 0, 8, "p", dst, "p", src, "d-", dstStride, "d-", srcStride, "d", h
     pxor         m7, m7
 .loop:
     mova         m0, [r1]
@@ -399,10 +391,7 @@ MPEG4_QPEL8_H_LOWPASS put_no_rnd
 %endmacro
 
 %macro MPEG4_QPEL16_V_LOWPASS 1
-cglobal %1_mpeg4_qpel16_v_lowpass, 4, 6, 0, 544
-    movsxdifnidn r2, r2d
-    movsxdifnidn r3, r3d
-
+cglobal %1_mpeg4_qpel16_v_lowpass, 4, 6, 0, 544, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     mov         r4d, 17
     mov          r5, rsp
     pxor         m7, m7
@@ -495,7 +484,7 @@ MPEG4_QPEL16_V_LOWPASS put_no_rnd
 
 
 %macro MPEG4_QPEL8_V_LOWPASS 1
-cglobal %1_mpeg4_qpel8_v_lowpass, 4, 6, 0, 288
+cglobal %1_mpeg4_qpel8_v_lowpass, 4, 6, 0, 288, "p", dst, "p", src, "d-", dstStride, "d-", srcStride
     movsxdifnidn r2, r2d
     movsxdifnidn r3, r3d
 
diff --git a/libavcodec/x86/rv34dsp.asm b/libavcodec/x86/rv34dsp.asm
index 692b4acfcd..d3943c052c 100644
--- a/libavcodec/x86/rv34dsp.asm
+++ b/libavcodec/x86/rv34dsp.asm
@@ -45,7 +45,7 @@ SECTION .text
 %endmacro
 
 %macro rv34_idct 1
-cglobal rv34_idct_%1, 1, 2, 0
+cglobal rv34_idct_%1, 1, 2, 0, "p", block
     movsx   r1, word [r0]
     IDCT_DC r1
     movd    m0, r1d
@@ -63,10 +63,10 @@ rv34_idct dc
 %define IDCT_DC IDCT_DC_NOROUND
 rv34_idct dc_noround
 
-; ff_rv34_idct_dc_add_mmx(uint8_t *dst, int stride, int dc);
+; ff_rv34_idct_dc_add_mmx(uint8_t *dst, ptrdiff_t stride, int dc);
 %if ARCH_X86_32
 INIT_MMX mmx
-cglobal rv34_idct_dc_add, 3, 3
+cglobal rv34_idct_dc_add, 3, 3, "p", dst, "p-", stride, "d", dc
     ; calculate DC
     IDCT_DC_ROUND r2
     pxor       m1, m1
@@ -157,7 +157,7 @@ cglobal rv34_idct_dc_add, 3, 3
     movd         %1, %2
 %endmacro
 INIT_MMX mmxext
-cglobal rv34_idct_add, 3,3,0, d, s, b
+cglobal rv34_idct_add, 3,3,0, d, s, b, "p", dst, "p-", stride, "p", block
     ROW_TRANSFORM       bq
     COL_TRANSFORM     [dq], mm0, [pw_col_coeffs+ 0], [pw_col_coeffs+ 8]
     mova               mm0, [pw_col_coeffs+ 0]
@@ -168,9 +168,9 @@ cglobal rv34_idct_add, 3,3,0, d, s, b
     COL_TRANSFORM  [dq+sq], mm7, mm0, mm4
     ret
 
-; ff_rv34_idct_dc_add_sse4(uint8_t *dst, int stride, int dc);
+; ff_rv34_idct_dc_add_sse4(uint8_t *dst, ptrdiff_t stride, int dc);
 %macro RV34_IDCT_DC_ADD 0
-cglobal rv34_idct_dc_add, 3, 3, 6
+cglobal rv34_idct_dc_add, 3, 3, 6, "p", dst, "p-", stride, "d", dc
     ; load data
     IDCT_DC_ROUND r2
     pxor       m1, m1
diff --git a/libavcodec/x86/rv40dsp.asm b/libavcodec/x86/rv40dsp.asm
index bcad1aee80..d23f0e137a 100644
--- a/libavcodec/x86/rv40dsp.asm
+++ b/libavcodec/x86/rv40dsp.asm
@@ -76,8 +76,8 @@ SECTION .text
 ;-----------------------------------------------------------------------------
 ; subpel MC functions:
 ;
-; void ff_[put|rv40]_rv40_qpel_[h|v]_<opt>(uint8_t *dst, int deststride,
-;                                          uint8_t *src, int srcstride,
+; void ff_[put|rv40]_rv40_qpel_[h|v]_<opt>(uint8_t *dst, ptrdiff_t deststride,
+;                                          uint8_t *src, ptrdiff_t srcstride,
 ;                                          int len, int m);
 ;----------------------------------------------------------------------
 %macro LOAD  2
@@ -103,7 +103,7 @@ SECTION .text
 %endmacro
 
 %macro FILTER_V 1
-cglobal %1_rv40_qpel_v, 6,6+npicregs,12, dst, dststride, src, srcstride, height, my, picreg
+cglobal %1_rv40_qpel_v, 6,6+npicregs,12, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", my, picreg
 %ifdef PIC
     lea  picregq, [sixtap_filter_v_m]
 %endif
@@ -174,7 +174,7 @@ cglobal %1_rv40_qpel_v, 6,6+npicregs,12, dst, dststride, src, srcstride, height,
 %endmacro
 
 %macro FILTER_H  1
-cglobal %1_rv40_qpel_h, 6, 6+npicregs, 12, dst, dststride, src, srcstride, height, mx, picreg
+cglobal %1_rv40_qpel_h, 6, 6+npicregs, 12, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
 %ifdef PIC
     lea  picregq, [sixtap_filter_v_m]
 %endif
@@ -251,7 +251,7 @@ FILTER_V  put
 FILTER_V  avg
 
 %macro FILTER_SSSE3 1
-cglobal %1_rv40_qpel_v, 6,6+npicregs,8, dst, dststride, src, srcstride, height, my, picreg
+cglobal %1_rv40_qpel_v, 6,6+npicregs,8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", my, picreg
 %ifdef PIC
     lea  picregq, [sixtap_filter_hb_m]
 %endif
@@ -296,7 +296,7 @@ cglobal %1_rv40_qpel_v, 6,6+npicregs,8, dst, dststride, src, srcstride, height,
     jg       .nextrow
     REP_RET
 
-cglobal %1_rv40_qpel_h, 6,6+npicregs,8, dst, dststride, src, srcstride, height, mx, picreg
+cglobal %1_rv40_qpel_h, 6,6+npicregs,8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
 %ifdef PIC
     lea  picregq, [sixtap_filter_hb_m]
 %endif
@@ -436,8 +436,7 @@ FILTER_SSSE3  avg
 %endif
 
 %endmacro
-
-; void ff_rv40_weight_func_%1(uint8_t *dst, uint8_t *src1, uint8_t *src2, int w1, int w2, int stride)
+; void ff_rv40_weight_func_%1(uint8_t *dst, uint8_t *src1, uint8_t *src2, int w1, int w2, ptrdiff_t stride)
 ; %1=size  %2=num of xmm regs
 ; The weights are FP0.14 notation of fractions depending on pts.
 ; For timebases without rounding error (i.e. PAL), the fractions
@@ -445,7 +444,7 @@ FILTER_SSSE3  avg
 ; Therefore, we check here whether they are multiples of 2^9 for
 ; those simplifications to occur.
 %macro RV40_WEIGHT  3
-cglobal rv40_weight_func_%1_%2, 6, 7, 8
+cglobal rv40_weight_func_%1_%2, 6, 7, 8, "p", dst, "p", src1, "p", src2, "d", w1, "d", w2, "p-", stride
 %if cpuflag(ssse3)
     mova       m1, [pw_1024]
 %else
diff --git a/libavcodec/x86/sbcdsp.asm b/libavcodec/x86/sbcdsp.asm
index d68d3a9ae8..c452f3b986 100644
--- a/libavcodec/x86/sbcdsp.asm
+++ b/libavcodec/x86/sbcdsp.asm
@@ -63,7 +63,7 @@ SECTION .text
 ;void ff_sbc_analyze_4(const int16_t *in, int32_t *out, const int16_t *consts);
 ;*******************************************************************
 INIT_MMX mmx
-cglobal sbc_analyze_4, 3, 3, 4, in, out, consts
+cglobal sbc_analyze_4, 3, 3, 4, "p", in, "p", out, "p", consts
     ANALYZE_MAC_IN   m0, m1, m0, m1, [scale_mask], [scale_mask], 0
     ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 16
     ANALYZE_MAC_IN   m0, m1, m2, m3, m2, m3, 32
@@ -83,7 +83,7 @@ cglobal sbc_analyze_4, 3, 3, 4, in, out, consts
 ;void ff_sbc_analyze_8(const int16_t *in, int32_t *out, const int16_t *consts);
 ;*******************************************************************
 INIT_MMX mmx
-cglobal sbc_analyze_8, 3, 3, 4, in, out, consts
+cglobal sbc_analyze_8, 3, 3, 4, "p", in, "p", out, "p", consts
     ANALYZE_MAC_IN   m0, m1, m0, m1, [scale_mask], [scale_mask],  0
     ANALYZE_MAC_IN   m2, m3, m2, m3, [scale_mask], [scale_mask], 16
     ANALYZE_MAC_IN   m0, m1, m4, m5, m4, m5,  32
@@ -120,7 +120,7 @@ cglobal sbc_analyze_8, 3, 3, 4, in, out, consts
 ;                              int blocks, int channels, int subbands)
 ;*******************************************************************
 INIT_MMX mmx
-cglobal sbc_calc_scalefactors, 5, 7, 4, sb_sample_f, scale_factor, blocks, channels, subbands, ptr, blk
+cglobal sbc_calc_scalefactors, 5, 7, 4, "p", sb_sample_f, "p", scale_factor, "d", blocks, "d", channels, "d", subbands, ptr, blk
     ; subbands = 4 * subbands * channels
     movq          m3, [scale_mask]
     shl           subbandsd, 2
diff --git a/libavcodec/x86/sbrdsp.asm b/libavcodec/x86/sbrdsp.asm
index 62bbe512ec..16369bf4a8 100644
--- a/libavcodec/x86/sbrdsp.asm
+++ b/libavcodec/x86/sbrdsp.asm
@@ -37,7 +37,7 @@ cextern         ps_neg
 SECTION .text
 
 INIT_XMM sse
-cglobal sbr_sum_square, 2, 3, 6
+cglobal sbr_sum_square, 2, 3, 6, "p", x, "d", n
     mov        r2d, r1d
     xorps       m0, m0
     xorps       m1, m1
@@ -85,7 +85,7 @@ cglobal sbr_sum_square, 2, 3, 6
     RET
 
 %define STEP  40*4*2
-cglobal sbr_hf_g_filt, 5, 6, 5
+cglobal sbr_hf_g_filt, 5, 6, 5, "p", Y, "p", X_high, "p", g_filt, "d", m_max, "p-", ixh
     lea         r1, [r1 + 8*r4] ; offset by ixh elements into X_high
     mov         r5, r3
     and         r3, 0xFC
@@ -129,7 +129,7 @@ cglobal sbr_hf_g_filt, 5, 6, 5
 ;                        const float alpha0[2], const float alpha1[2],
 ;                        float bw, int start, int end)
 ;
-cglobal sbr_hf_gen, 4,4,8, X_high, X_low, alpha0, alpha1, BW, S, E
+cglobal sbr_hf_gen, 4,4,8, "p", X_high, "p", X_low, "p", alpha0, "p", alpha1, BW, S, E
     ; load alpha factors
 %define bw m0
 %if ARCH_X86_64 == 0 || WIN64
@@ -188,7 +188,7 @@ cglobal sbr_hf_gen, 4,4,8, X_high, X_low, alpha0, alpha1, BW, S, E
     jnz         .loop2
     RET
 
-cglobal sbr_sum64x5, 1,2,4,z
+cglobal sbr_sum64x5, 1,2,4, "p", z
     lea    r1q, [zq+ 256]
 .loop:
     mova    m0, [zq+   0]
@@ -211,7 +211,7 @@ cglobal sbr_sum64x5, 1,2,4,z
     REP_RET
 
 INIT_XMM sse
-cglobal sbr_qmf_post_shuffle, 2,3,4,W,z
+cglobal sbr_qmf_post_shuffle, 2,3,4, "p", W, "p", z
     lea              r2q, [zq + (64-4)*4]
     mova              m3, [ps_neg]
 .loop:
@@ -230,7 +230,7 @@ cglobal sbr_qmf_post_shuffle, 2,3,4,W,z
     REP_RET
 
 INIT_XMM sse
-cglobal sbr_neg_odd_64, 1,2,4,z
+cglobal sbr_neg_odd_64, 1,2,4, "p", z
     lea        r1q, [zq+256]
 .loop:
     mova        m0, [zq+ 0]
@@ -252,7 +252,7 @@ cglobal sbr_neg_odd_64, 1,2,4,z
 
 ; void ff_sbr_qmf_deint_bfly_sse2(float *v, const float *src0, const float *src1)
 %macro SBR_QMF_DEINT_BFLY  0
-cglobal sbr_qmf_deint_bfly, 3,5,8, v,src0,src1,vrev,c
+cglobal sbr_qmf_deint_bfly, 3,5,8, "p", v, "p", src0, "p", src1,vrev,c
     mov               cq, 64*4-2*mmsize
     lea            vrevq, [vq + 64*4]
 .loop:
@@ -293,7 +293,7 @@ INIT_XMM sse2
 SBR_QMF_DEINT_BFLY
 
 INIT_XMM sse2
-cglobal sbr_qmf_pre_shuffle, 1,4,6,z
+cglobal sbr_qmf_pre_shuffle, 1,4,6, "p", z
 %define OFFSET  (32*4-2*mmsize)
     mov       r3q, OFFSET
     lea       r1q, [zq + (32+1)*4]
@@ -347,14 +347,14 @@ INIT_XMM sse2
 ; sbr_hf_apply_noise_0(float (*Y)[2], const float *s_m,
 ;                      const float *q_filt, int noise,
 ;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_0, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
+cglobal sbr_hf_apply_noise_0, 5,5+NREGS+UNIX64,8, "p", Y, "p", s_m, "p", q_filt, "d", noise, "d", kx, "d", m_max
     mova       m0, [ps_noise0]
     jmp apply_noise_main
 
 ; sbr_hf_apply_noise_1(float (*Y)[2], const float *s_m,
 ;                      const float *q_filt, int noise,
 ;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_1, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
+cglobal sbr_hf_apply_noise_1, 5,5+NREGS+UNIX64,8, "p", Y, "p", s_m, "p", q_filt, "d", noise, "d", kx, "d", m_max
     and       kxq, 1
     shl       kxq, 4
     LOAD_NST  ps_noise13
@@ -363,14 +363,14 @@ cglobal sbr_hf_apply_noise_1, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
 ; sbr_hf_apply_noise_2(float (*Y)[2], const float *s_m,
 ;                      const float *q_filt, int noise,
 ;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_2, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
+cglobal sbr_hf_apply_noise_2, 5,5+NREGS+UNIX64,8, "p", Y, "p", s_m, "p", q_filt, "d", noise, "d", kx, "d", m_max
     mova       m0, [ps_noise2]
     jmp apply_noise_main
 
 ; sbr_hf_apply_noise_3(float (*Y)[2], const float *s_m,
 ;                      const float *q_filt, int noise,
 ;                      int kx, int m_max)
-cglobal sbr_hf_apply_noise_3, 5,5+NREGS+UNIX64,8, Y,s_m,q_filt,noise,kx,m_max
+cglobal sbr_hf_apply_noise_3, 5,5+NREGS+UNIX64,8, "p", Y, "p", s_m, "p", q_filt, "d", noise, "d", kx, "d", m_max
     and       kxq, 1
     shl       kxq, 4
     LOAD_NST  ps_noise13+16
@@ -427,7 +427,7 @@ apply_noise_main:
     RET
 
 INIT_XMM sse
-cglobal sbr_qmf_deint_neg, 2,4,4,v,src,vrev,c
+cglobal sbr_qmf_deint_neg, 2,4,4, "p", v, "p", src,vrev,c
 %define COUNT  32*4
 %define OFFSET 32*4
     mov        cq, -COUNT
@@ -449,7 +449,7 @@ cglobal sbr_qmf_deint_neg, 2,4,4,v,src,vrev,c
     REP_RET
 
 %macro SBR_AUTOCORRELATE 0
-cglobal sbr_autocorrelate, 2,3,8,32, x, phi, cnt
+cglobal sbr_autocorrelate, 2,3,8,32, "p", x, "p", phi, cnt
     mov   cntq, 37*8
     add     xq, cntq
     neg   cntq
diff --git a/libavcodec/x86/simple_idct.asm b/libavcodec/x86/simple_idct.asm
index 6fedbb5784..16577a6185 100644
--- a/libavcodec/x86/simple_idct.asm
+++ b/libavcodec/x86/simple_idct.asm
@@ -842,11 +842,11 @@ SECTION .text
 
 INIT_MMX mmx
 
-cglobal simple_idct, 1, 2, 8, 128, block, t0
+cglobal simple_idct, 1, 2, 8, 128, "p", block, t0
     IDCT
 RET
 
-cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
+cglobal simple_idct_put, 3, 5, 8, 128, "p", pixels, "p-", lsize, "p", block, lsize3, t0
     IDCT
     lea lsize3q, [lsizeq*3]
     PUT_PIXELS_CLAMPED_HALF 0
@@ -854,7 +854,7 @@ cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
     PUT_PIXELS_CLAMPED_HALF 64
 RET
 
-cglobal simple_idct_add, 3, 4, 8, 128, pixels, lsize, block, t0
+cglobal simple_idct_add, 3, 4, 8, 128, "p", pixels, "p-", lsize, "p", block, t0
     IDCT
     pxor       m4, m4
     ADD_PIXELS_CLAMPED 0
@@ -868,7 +868,7 @@ RET
 
 INIT_XMM sse2
 
-cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
+cglobal simple_idct_put, 3, 5, 8, 128, "p", pixels, "p-", lsize, "p", block, lsize3, t0
     IDCT
     lea lsize3q, [lsizeq*3]
     PUT_PIXELS_CLAMPED_HALF 0
@@ -876,7 +876,7 @@ cglobal simple_idct_put, 3, 5, 8, 128, pixels, lsize, block, lsize3, t0
     PUT_PIXELS_CLAMPED_HALF 64
 RET
 
-cglobal simple_idct_add, 3, 4, 8, 128, pixels, lsize, block, t0
+cglobal simple_idct_add, 3, 4, 8, 128, "p", pixels, "p-", lsize, "p", block, t0
     IDCT
     pxor       m4, m4
     ADD_PIXELS_CLAMPED 0
diff --git a/libavcodec/x86/simple_idct10.asm b/libavcodec/x86/simple_idct10.asm
index 069bb61378..a7436b018f 100644
--- a/libavcodec/x86/simple_idct10.asm
+++ b/libavcodec/x86/simple_idct10.asm
@@ -120,11 +120,11 @@ SECTION .text
 
 define_constants _lo
 
-cglobal simple_idct8, 1, 1, 16, 32, block
+cglobal simple_idct8, 1, 1, 16, 32, "p", block
     IDCT_FN    "", 11, pw_32, 20, "store"
 RET
 
-cglobal simple_idct8_put, 3, 4, 16, 32, pixels, lsize, block
+cglobal simple_idct8_put, 3, 4, 16, 32, "p", pixels, "p-", lsize, "p", block
     IDCT_FN    "", 11, pw_32, 20
     lea       r3, [3*lsizeq]
     lea       r2, [pixelsq + r3]
@@ -135,7 +135,7 @@ cglobal simple_idct8_put, 3, 4, 16, 32, pixels, lsize, block
     STORE_HI_LO PASS8ROWS(pixelsq, r2, lsizeq, r3), m8, m1, m4, m9
 RET
 
-cglobal simple_idct8_add, 3, 4, 16, 32, pixels, lsize, block
+cglobal simple_idct8_add, 3, 4, 16, 32, "p", pixels, "p-", lsize, "p", block
     IDCT_FN    "", 11, pw_32, 20
     lea r2, [3*lsizeq]
     %if cpuflag(sse4)
@@ -173,21 +173,21 @@ RET
 
 define_constants _hi
 
-cglobal simple_idct10, 1, 1, 16, block
+cglobal simple_idct10, 1, 1, 16, "p", block
     IDCT_FN    "", 12, "", 19, "store"
     RET
 
-cglobal simple_idct10_put, 3, 3, 16, pixels, lsize, block
+cglobal simple_idct10_put, 3, 3, 16, "p", pixels, "p-", lsize, "p", block
     IDCT_FN    "", 12, "", 19, "put", 0, pw_1023
     RET
 
-cglobal simple_idct12, 1, 1, 16, block
+cglobal simple_idct12, 1, 1, 16, "p", block
     ; coeffs are already 15bits, adding the offset would cause
     ; overflow in the input
     IDCT_FN    "", 15, pw_2, 16, "store"
     RET
 
-cglobal simple_idct12_put, 3, 3, 16, pixels, lsize, block
+cglobal simple_idct12_put, 3, 3, 16, "p", pixels, "p-", lsize, "p", block
     ; range isn't known, so the C simple_idct range is used
     ; Also, using a bias on input overflows, so use the bias
     ; on output of the first butterfly instead
diff --git a/libavcodec/x86/svq1enc.asm b/libavcodec/x86/svq1enc.asm
index a87632836d..e16976de82 100644
--- a/libavcodec/x86/svq1enc.asm
+++ b/libavcodec/x86/svq1enc.asm
@@ -24,7 +24,7 @@
 SECTION .text
 
 %macro SSD_INT8_VS_INT16 0
-cglobal ssd_int8_vs_int16, 3, 3, 3, pix1, pix2, size
+cglobal ssd_int8_vs_int16, 3, 3, 3, "p", pix1, "p", pix2, "p-", size
     pxor m0, m0
 .loop:
     sub       sizeq, 8
diff --git a/libavcodec/x86/synth_filter.asm b/libavcodec/x86/synth_filter.asm
index bc1a48f409..6ead1099e1 100644
--- a/libavcodec/x86/synth_filter.asm
+++ b/libavcodec/x86/synth_filter.asm
@@ -111,8 +111,8 @@ SECTION .text
 ;                                  const float window[512], float out[32],
 ;                                  intptr_t offset, float scale)
 %macro SYNTH_FILTER 0
-cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
-                              synth_buf, synth_buf2, window, out, off, scale
+cglobal synth_filter_inner, 5, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
+                              "p*", synth_buf, "p*", synth_buf2, "p*", window, "p*", out, "p-", off, scale
 %define scale m0
 %if ARCH_X86_32 || WIN64
 %if cpuflag(sse2) && notcpuflag(avx)
@@ -132,7 +132,7 @@ cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
 %endif
     ; prepare inner counter limit 1
     mov          r5q, 480
-    sub          r5q, offmp
+    sub          r5q, offmq
     and          r5q, -64
     shl          r5q, 2
 %if ARCH_X86_32 || notcpuflag(avx)
@@ -157,6 +157,7 @@ cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
 %if ARCH_X86_32
 %define ptr1     r0q
 %define ptr2     r1q
+%define ptr2p    r1p
 %define win      r2q
 %define j        r3q
     mov          win, windowm
@@ -168,6 +169,7 @@ cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
 %else ; ARCH_X86_64
 %define ptr1     r6q
 %define ptr2     r7q ; must be loaded
+%define ptr2p    r7p
 %define win      r8q
 %define j        r9q
     SETZERO       m9
@@ -177,7 +179,7 @@ cglobal synth_filter_inner, 0, 6 + 4 * ARCH_X86_64, 7 + 6 * ARCH_X86_64, \
     lea          win, [windowq + i]
     lea         ptr1, [synth_bufq + i]
 %endif
-    mov         ptr2, synth_bufmp
+    mov         ptr2p, synth_bufmp
     ; prepare the inner loop counter
     mov            j, OFFQ
 %if ARCH_X86_32 || notcpuflag(avx)
diff --git a/libavcodec/x86/takdsp.asm b/libavcodec/x86/takdsp.asm
index 5f3ded3ea2..0b7fb6c429 100644
--- a/libavcodec/x86/takdsp.asm
+++ b/libavcodec/x86/takdsp.asm
@@ -29,7 +29,7 @@ pd_128: times 4 dd 128
 SECTION .text
 
 INIT_XMM sse2
-cglobal tak_decorrelate_ls, 3, 3, 2, p1, p2, length
+cglobal tak_decorrelate_ls, 3, 3, 2, "p", p1, "p", p2, "d", length
     shl                     lengthd, 2
     add                         p1q, lengthq
     add                         p2q, lengthq
@@ -45,7 +45,7 @@ cglobal tak_decorrelate_ls, 3, 3, 2, p1, p2, length
     jl .loop
     REP_RET
 
-cglobal tak_decorrelate_sr, 3, 3, 2, p1, p2, length
+cglobal tak_decorrelate_sr, 3, 3, 2, "p", p1, "p", p2, "d", length
     shl                     lengthd, 2
     add                         p1q, lengthq
     add                         p2q, lengthq
@@ -62,7 +62,7 @@ cglobal tak_decorrelate_sr, 3, 3, 2, p1, p2, length
     jl .loop
     REP_RET
 
-cglobal tak_decorrelate_sm, 3, 3, 6, p1, p2, length
+cglobal tak_decorrelate_sm, 3, 3, 6, "p", p1, "p", p2, "d", length
     shl                     lengthd, 2
     add                         p1q, lengthq
     add                         p2q, lengthq
@@ -90,7 +90,7 @@ cglobal tak_decorrelate_sm, 3, 3, 6, p1, p2, length
     REP_RET
 
 INIT_XMM sse4
-cglobal tak_decorrelate_sf, 3, 3, 5, p1, p2, length, dshift, dfactor
+cglobal tak_decorrelate_sf, 3, 3, 5, "p", p1, "p", p2, "d", length, "d", dshift, "d", dfactor
     shl             lengthd, 2
     add                 p1q, lengthq
     add                 p2q, lengthq
diff --git a/libavcodec/x86/ttadsp.asm b/libavcodec/x86/ttadsp.asm
index db12a32eca..ba1ea2a0cb 100644
--- a/libavcodec/x86/ttadsp.asm
+++ b/libavcodec/x86/ttadsp.asm
@@ -28,10 +28,9 @@ pd_n0113: dd ~0, ~1, ~1, ~3
 pd_1224:  dd 1, 2, 2, 4
 
 SECTION .text
-
 %macro TTA_FILTER 2
 INIT_XMM %1
-cglobal tta_filter_process, 5,5,%2, qm, dx, dl, error, in, shift, round
+cglobal tta_filter_process, 5,5,%2, "p", qm, "p", dx, "p", dl, "p", error, "p", in, "d", shift, "d", round
     mova       m2, [qmq       ]
     mova       m3, [qmq + 0x10]
     mova       m4, [dxq       ]
diff --git a/libavcodec/x86/ttaencdsp.asm b/libavcodec/x86/ttaencdsp.asm
index c9cbd49874..4f8beaf3ea 100644
--- a/libavcodec/x86/ttaencdsp.asm
+++ b/libavcodec/x86/ttaencdsp.asm
@@ -31,7 +31,7 @@ SECTION .text
 
 %macro TTAENC_FILTER 2
 INIT_XMM %1
-cglobal ttaenc_filter_process, 5,5,%2, qm, dx, dl, error, in, shift, round
+cglobal ttaenc_filter_process, 5,5,%2, "p", qm, "p", dx, "p", dl, "p", error, "p", in, "d", shift, "d", round
     mova       m2, [qmq       ]
     mova       m3, [qmq + 0x10]
     mova       m4, [dxq       ]
diff --git a/libavcodec/x86/utvideodsp.asm b/libavcodec/x86/utvideodsp.asm
index b799c44b64..7ee8ebdd87 100644
--- a/libavcodec/x86/utvideodsp.asm
+++ b/libavcodec/x86/utvideodsp.asm
@@ -36,8 +36,8 @@ SECTION .text
 ;                         int width, int height)
 ;-------------------------------------------------------------------------------------------
 %macro RESTORE_RGB_PLANES 0
-cglobal restore_rgb_planes, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 4, src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, w, h, x
-    movsxdifnidn wq, wd
+cglobal restore_rgb_planes, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 4, "p", src_r, "p", src_g, "p", src_b, \
+                          "p-", linesize_r, "p-", linesize_g, "p-", linesize_b, "d-", w, "d", h, x
     add      src_rq, wq
     add      src_gq, wq
     add      src_bq, wq
@@ -86,7 +86,8 @@ RESTORE_RGB_PLANES
 ;                         int width, int height)
 ;-------------------------------------------------------------------------------------------
 %macro RESTORE_RGB_PLANES10 0
-cglobal restore_rgb_planes10, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 5, src_r, src_g, src_b, linesize_r, linesize_g, linesize_b, w, h, x
+cglobal restore_rgb_planes10, 7 + ARCH_X86_64, 7 + ARCH_X86_64 * 2, 5, "p", src_r, "p", src_g, "p", src_b, \
+                          "p-", linesize_r, "p-", linesize_g, "p-", linesize_b, "d", w, "d", h, x
     shl          wd, 1
     shl linesize_rq, 1
     shl linesize_gq, 1
diff --git a/libavcodec/x86/v210.asm b/libavcodec/x86/v210.asm
index c24c765e5b..9546319ef6 100644
--- a/libavcodec/x86/v210.asm
+++ b/libavcodec/x86/v210.asm
@@ -34,8 +34,7 @@ SECTION .text
 %macro v210_planar_unpack 1
 
 ; v210_planar_unpack(const uint32_t *src, uint16_t *y, uint16_t *u, uint16_t *v, int width)
-cglobal v210_planar_unpack_%1, 5, 5, 7
-    movsxdifnidn r4, r4d
+cglobal v210_planar_unpack_%1, 5, 5, 7, "p", src, "p", y, "p", u, "p", v, "d-", width
     lea    r1, [r1+2*r4]
     add    r2, r4
     add    r3, r4
diff --git a/libavcodec/x86/v210enc.asm b/libavcodec/x86/v210enc.asm
index 965f2bea3c..f34aeb96e7 100644
--- a/libavcodec/x86/v210enc.asm
+++ b/libavcodec/x86/v210enc.asm
@@ -51,7 +51,7 @@ SECTION .text
 %macro v210_planar_pack_10 0
 
 ; v210_planar_pack_10(const uint16_t *y, const uint16_t *u, const uint16_t *v, uint8_t *dst, ptrdiff_t width)
-cglobal v210_planar_pack_10, 5, 5, 4+cpuflag(avx2), y, u, v, dst, width
+cglobal v210_planar_pack_10, 5, 5, 4+cpuflag(avx2), "p", y, "p", u, "p", v, "p", dst, "p-", width
     lea     r0, [yq+2*widthq]
     add     uq, widthq
     add     vq, widthq
@@ -106,7 +106,7 @@ v210_planar_pack_10
 %macro v210_planar_pack_8 0
 
 ; v210_planar_pack_8(const uint8_t *y, const uint8_t *u, const uint8_t *v, uint8_t *dst, ptrdiff_t width)
-cglobal v210_planar_pack_8, 5, 5, 7, y, u, v, dst, width
+cglobal v210_planar_pack_8, 5, 5, 7, "p", y, "p", u, "p", v, "p", dst, "p-", width
     add     yq, widthq
     shr     widthq, 1
     add     uq, widthq
diff --git a/libavcodec/x86/vc1dsp_loopfilter.asm b/libavcodec/x86/vc1dsp_loopfilter.asm
index fd33bd13dc..26e519b71d 100644
--- a/libavcodec/x86/vc1dsp_loopfilter.asm
+++ b/libavcodec/x86/vc1dsp_loopfilter.asm
@@ -238,19 +238,19 @@ cglobal vc1_h_loop_filter_internal
     ret
 
 ; void ff_vc1_v_loop_filter4_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter4, 3,5,0
+cglobal vc1_v_loop_filter4, 3,5,0, "p", src, "d", stride, "d", pq
     START_V_FILTER
     call vc1_v_loop_filter_internal
     RET
 
 ; void ff_vc1_h_loop_filter4_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter4, 3,5,0
+cglobal vc1_h_loop_filter4, 3,5,0, "p", src, "d", stride, "d", pq
     START_H_FILTER 4
     call vc1_h_loop_filter_internal
     RET
 
 ; void ff_vc1_v_loop_filter8_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,0
+cglobal vc1_v_loop_filter8, 3,5,0, "p", src, "d", stride, "d", pq
     START_V_FILTER
     call vc1_v_loop_filter_internal
     add  r4, 4
@@ -259,7 +259,7 @@ cglobal vc1_v_loop_filter8, 3,5,0
     RET
 
 ; void ff_vc1_h_loop_filter8_mmxext(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,5,0
+cglobal vc1_h_loop_filter8, 3,5,0, "p", src, "d", stride, "d", pq
     START_H_FILTER 4
     call vc1_h_loop_filter_internal
     lea  r0, [r0+4*r1]
@@ -272,46 +272,46 @@ VC1_LF
 
 INIT_XMM sse2
 ; void ff_vc1_v_loop_filter8_sse2(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,8
+cglobal vc1_v_loop_filter8, 3,5,8, "p", src, "d", stride, "d", pq
     START_V_FILTER
     VC1_V_LOOP_FILTER 8, q
     RET
 
 ; void ff_vc1_h_loop_filter8_sse2(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,6,8
+cglobal vc1_h_loop_filter8, 3,6,8, "p", src, "d", stride, "d", pq
     START_H_FILTER 8
     VC1_H_LOOP_FILTER 8, r5
     RET
 
 INIT_MMX ssse3
 ; void ff_vc1_v_loop_filter4_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter4, 3,5,0
+cglobal vc1_v_loop_filter4, 3,5,0, "p", src, "d", stride, "d", pq
     START_V_FILTER
     VC1_V_LOOP_FILTER 4, d
     RET
 
 ; void ff_vc1_h_loop_filter4_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter4, 3,5,0
+cglobal vc1_h_loop_filter4, 3,5,0, "p", src, "d", stride, "d", pq
     START_H_FILTER 4
     VC1_H_LOOP_FILTER 4, r4
     RET
 
 INIT_XMM ssse3
 ; void ff_vc1_v_loop_filter8_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_v_loop_filter8, 3,5,8
+cglobal vc1_v_loop_filter8, 3,5,8, "p", src, "d", stride, "d", pq
     START_V_FILTER
     VC1_V_LOOP_FILTER 8, q
     RET
 
 ; void ff_vc1_h_loop_filter8_ssse3(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,6,8
+cglobal vc1_h_loop_filter8, 3,6,8, "p", src, "d", stride, "d", pq
     START_H_FILTER 8
     VC1_H_LOOP_FILTER 8, r5
     RET
 
 INIT_XMM sse4
 ; void ff_vc1_h_loop_filter8_sse4(uint8_t *src, int stride, int pq)
-cglobal vc1_h_loop_filter8, 3,5,8
+cglobal vc1_h_loop_filter8, 3,5,8, "p", src, "d", stride, "d", pq
     START_H_FILTER 8
     VC1_H_LOOP_FILTER 8
     RET
diff --git a/libavcodec/x86/vc1dsp_mc.asm b/libavcodec/x86/vc1dsp_mc.asm
index 0e6d87dd8b..51a4eb7412 100644
--- a/libavcodec/x86/vc1dsp_mc.asm
+++ b/libavcodec/x86/vc1dsp_mc.asm
@@ -92,12 +92,12 @@ INIT_MMX mmx
 ;                                    x86_reg stride, int rnd, int64_t shift)
 ; Sacrificing m6 makes it possible to pipeline loads from src
 %if ARCH_X86_32
-cglobal vc1_put_ver_16b_shift2, 3,6,0, dst, src, stride
+cglobal vc1_put_ver_16b_shift2, 3,6,0, "p", dst, "p", src, "q", stride
     DECLARE_REG_TMP     3, 4, 5
     %define rnd r3mp
     %define shift qword r4m
 %else ; X86_64
-cglobal vc1_put_ver_16b_shift2, 4,7,0, dst, src, stride
+cglobal vc1_put_ver_16b_shift2, 4,7,0, "p", dst, "p", src, "q", stride
     DECLARE_REG_TMP     4, 5, 6
     %define   rnd r3d
     ; We need shift either in memory or in a mm reg as it's used in psraw
@@ -151,7 +151,7 @@ cglobal vc1_put_ver_16b_shift2, 4,7,0, dst, src, stride
 ; Data is already unpacked, so some operations can directly be made from
 ; memory.
 %macro HOR_16B_SHIFT2 2 ; op, opname
-cglobal vc1_%2_hor_16b_shift2, 4, 5, 0, dst, stride, src, rnd, h
+cglobal vc1_%2_hor_16b_shift2, 4, 5, 0, "p", dst, "q", stride, "p", src, "d", rnd, h
     mov                hq, 8
     sub              srcq, 2
     sub              rndd, (-1+9+9-1) * 1024 ; add -1024 bias
@@ -227,7 +227,7 @@ HOR_16B_SHIFT2 OP_AVG, avg
 
 ; ff_vc1_inv_trans_?x?_dc_mmxext(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
 INIT_MMX mmxext
-cglobal vc1_inv_trans_4x4_dc, 3,4,0, dest, linesize, block
+cglobal vc1_inv_trans_4x4_dc, 3,4,0, "p", dest, "p-", linesize, "p", block
     movsx         r3d, WORD [blockq]
     mov        blockd, r3d             ; dc
     shl        blockd, 4               ; 16 * dc
@@ -244,7 +244,7 @@ cglobal vc1_inv_trans_4x4_dc, 3,4,0, dest, linesize, block
     RET
 
 INIT_MMX mmxext
-cglobal vc1_inv_trans_4x8_dc, 3,4,0, dest, linesize, block
+cglobal vc1_inv_trans_4x8_dc, 3,4,0, "p", dest, "p-", linesize, "p", block
     movsx         r3d, WORD [blockq]
     mov        blockd, r3d             ; dc
     shl        blockd, 4               ; 16 * dc
@@ -262,7 +262,7 @@ cglobal vc1_inv_trans_4x8_dc, 3,4,0, dest, linesize, block
     RET
 
 INIT_MMX mmxext
-cglobal vc1_inv_trans_8x4_dc, 3,4,0, dest, linesize, block
+cglobal vc1_inv_trans_8x4_dc, 3,4,0, "p", dest, "p-", linesize, "p", block
     movsx      blockd, WORD [blockq]   ; dc
     lea        blockd, [blockq*3+1]    ;  3 * dc + 1
     sar        blockd, 1               ; >> 1
@@ -277,7 +277,7 @@ cglobal vc1_inv_trans_8x4_dc, 3,4,0, dest, linesize, block
     RET
 
 INIT_MMX mmxext
-cglobal vc1_inv_trans_8x8_dc, 3,3,0, dest, linesize, block
+cglobal vc1_inv_trans_8x8_dc, 3,3,0, "p", dest, "p-", linesize, "p", block
     movsx      blockd, WORD [blockq]   ; dc
     lea        blockd, [blockq*3+1]    ;  3 * dc + 1
     sar        blockd, 1               ; >> 1
diff --git a/libavcodec/x86/vc1dsp_mmx.c b/libavcodec/x86/vc1dsp_mmx.c
index 45c8a68f29..06d8f827a5 100644
--- a/libavcodec/x86/vc1dsp_mmx.c
+++ b/libavcodec/x86/vc1dsp_mmx.c
@@ -79,9 +79,11 @@ void ff_vc1_avg_hor_16b_shift2_mmxext(uint8_t *dst, x86_reg stride,
  * Sacrifice mm6 for *9 factor.
  */
 #define VC1_SHIFT2(OP, OPNAME)\
-static void OPNAME ## vc1_shift2_mmx(uint8_t *dst, const uint8_t *src,\
+static void OPNAME ## vc1_shift2_mmx(uint8_t *dst_, const uint8_t *src_,\
                                      x86_reg stride, int rnd, x86_reg offset)\
 {\
+    x86_reg dst = (uintptr_t)dst_;\
+    x86_reg src = (uintptr_t)src_;\
     rnd = 8-rnd;\
     __asm__ volatile(\
         "mov       $8, %%"FF_REG_c"        \n\t"\
@@ -184,12 +186,13 @@ VC1_SHIFT2(OP_AVG, avg_)
  */
 #define MSPEL_FILTER13_VER_16B(NAME, A1, A2, A3, A4)                    \
 static void                                                             \
-vc1_put_ver_16b_ ## NAME ## _mmx(int16_t *dst, const uint8_t *src,      \
+vc1_put_ver_16b_ ## NAME ## _mmx(int16_t *dst_, const uint8_t *src_,    \
                                  x86_reg src_stride,                   \
                                  int rnd, int64_t shift)                \
 {                                                                       \
+    x86_reg dst = (uintptr_t)dst_;                                      \
+    x86_reg src = (uintptr_t)(src_ - src_stride);                       \
     int h = 8;                                                          \
-    src -= src_stride;                                                  \
     __asm__ volatile(                                                       \
         LOAD_ROUNDER_MMX("%5")                                          \
         "movq      "MANGLE(ff_pw_53)", %%mm5\n\t"                       \
@@ -241,11 +244,12 @@ vc1_put_ver_16b_ ## NAME ## _mmx(int16_t *dst, const uint8_t *src,      \
  */
 #define MSPEL_FILTER13_HOR_16B(NAME, A1, A2, A3, A4, OP, OPNAME)        \
 static void                                                             \
-OPNAME ## vc1_hor_16b_ ## NAME ## _mmx(uint8_t *dst, x86_reg stride,    \
-                                 const int16_t *src, int rnd)           \
+OPNAME ## vc1_hor_16b_ ## NAME ## _mmx(uint8_t *dst_, x86_reg stride,   \
+                                 const int16_t *src_, int rnd)          \
 {                                                                       \
+    x86_reg dst = (uintptr_t)dst_;                                      \
+    x86_reg src = (uintptr_t)(src_ - 1);                                \
     int h = 8;                                                          \
-    src -= 1;                                                           \
     rnd -= (-4+58+13-3)*256; /* Add -256 bias */                        \
     __asm__ volatile(                                                       \
         LOAD_ROUNDER_MMX("%4")                                          \
@@ -280,11 +284,12 @@ OPNAME ## vc1_hor_16b_ ## NAME ## _mmx(uint8_t *dst, x86_reg stride,    \
  */
 #define MSPEL_FILTER13_8B(NAME, A1, A2, A3, A4, OP, OPNAME)             \
 static void                                                             \
-OPNAME ## vc1_## NAME ## _mmx(uint8_t *dst, const uint8_t *src,         \
+OPNAME ## vc1_## NAME ## _mmx(uint8_t *dst_, const uint8_t *src_,       \
                         x86_reg stride, int rnd, x86_reg offset)      \
 {                                                                       \
+    x86_reg dst = (uintptr_t)dst_;                                      \
+    x86_reg src = (uintptr_t)(src_ - offset);                           \
     int h = 8;                                                          \
-    src -= offset;                                                      \
     rnd = 32-rnd;                                                       \
     __asm__ volatile (                                                      \
         LOAD_ROUNDER_MMX("%6")                                          \
diff --git a/libavcodec/x86/videodsp.asm b/libavcodec/x86/videodsp.asm
index e237860700..a28f2e1313 100644
--- a/libavcodec/x86/videodsp.asm
+++ b/libavcodec/x86/videodsp.asm
@@ -28,7 +28,7 @@ SECTION .text
 
 %macro V_COPY_ROW 2 ; type (top/body/bottom), h
 .%1_y_loop:                                     ; do {
-    mov              wq, r7mp                   ;   initialize w (r7mp = wmp)
+    mov              wq, r7mq                   ;   initialize w (r7mp = wmp)
 .%1_x_loop:                                     ;   do {
     movu             m0, [srcq+wq]              ;     m0 = read($mmsize)
     movu      [dstq+wq], m0                     ;     write(m0, $mmsize)
@@ -54,10 +54,10 @@ SECTION .text
 ; |    |    <- bottom is copied from last line in body of source
 ; '----' <- bh
 %if ARCH_X86_64
-cglobal emu_edge_vvar, 7, 8, 1, dst, dst_stride, src, src_stride, \
-                                start_y, end_y, bh, w
+cglobal emu_edge_vvar, 7, 8, 1, "p", dst, "q", dst_stride, "p", src, "q", src_stride, \
+                                "q", start_y, "q", end_y, "q", bh, "q", w
 %else ; x86-32
-cglobal emu_edge_vvar, 1, 6, 1, dst, src, start_y, end_y, bh, w
+cglobal emu_edge_vvar, 1, 6, 1, "p", dst, src, start_y, end_y, bh, w
 %define src_strideq r3mp
 %define dst_strideq r1mp
     mov            srcq, r2mp
@@ -67,9 +67,9 @@ cglobal emu_edge_vvar, 1, 6, 1, dst, src, start_y, end_y, bh, w
 %endif
     sub             bhq, end_yq                 ; bh    -= end_q
     sub          end_yq, start_yq               ; end_q -= start_q
-    add            srcq, r7mp                   ; (r7mp = wmp)
-    add            dstq, r7mp                   ; (r7mp = wmp)
-    neg            r7mp                         ; (r7mp = wmp)
+    add            srcq, r7mq                   ; src   += wm
+    add            dstq, r7mq                   ; dst   += wm
+    neg            r7mq                         ; wm    =  -wm
     test       start_yq, start_yq               ; if (start_q) {
     jz .body
     V_COPY_ROW      top, start_yq               ;   v_copy_row(top, start_yq)
@@ -92,7 +92,7 @@ INIT_XMM sse
 vvar_fn
 
 %macro hvar_fn 0
-cglobal emu_edge_hvar, 5, 6, 1, dst, dst_stride, start_x, n_words, h, w
+cglobal emu_edge_hvar, 5, 6, 1, "p", dst, "q", dst_stride, "q", start_x, "q", n_words, "q", h, w
     lea            dstq, [dstq+n_wordsq*2]
     neg        n_wordsq
     lea        start_xq, [start_xq+n_wordsq*2]
@@ -269,25 +269,25 @@ hvar_fn
 %rep 1+%2-%1
 %if %%n <= 3
 %if ARCH_X86_64
-cglobal emu_edge_vfix %+ %%n, 6, 8, 0, dst, dst_stride, src, src_stride, \
-                                       start_y, end_y, val, bh
-    mov             bhq, r6mp                   ; r6mp = bhmp
+cglobal emu_edge_vfix %+ %%n, 6, 8, 0, "p", dst, "q", dst_stride, "p", src, "q", src_stride, \
+                                       "q", start_y, "q", end_y, val, bh
+    mov             bhq, r6mq                   ; r6mp = bhmp
 %else ; x86-32
 cglobal emu_edge_vfix %+ %%n, 0, 6, 0, val, dst, src, start_y, end_y, bh
-    mov            dstq, r0mp
-    mov            srcq, r2mp
-    mov        start_yq, r4mp
-    mov          end_yq, r5mp
-    mov             bhq, r6mp
-%define dst_strideq r1mp
-%define src_strideq r3mp
+    mov            dstp, r0mp
+    mov            srcp, r2mp
+    mov        start_yq, r4mq
+    mov          end_yq, r5mq
+    mov             bhq, r6mq
+%define dst_strideq r1mq
+%define src_strideq r3mq
 %endif ; x86-64/32
 %else
 %if ARCH_X86_64
-cglobal emu_edge_vfix %+ %%n, 7, 7, 1, dst, dst_stride, src, src_stride, \
-                                       start_y, end_y, bh
+cglobal emu_edge_vfix %+ %%n, 7, 7, 1, "p", dst, "q", dst_stride, "p", src, "q", src_stride, \
+                                       "q", start_y, "q", end_y, "q", bh
 %else ; x86-32
-cglobal emu_edge_vfix %+ %%n, 1, 5, 1, dst, src, start_y, end_y, bh
+cglobal emu_edge_vfix %+ %%n, 1, 5, 1, "p", dst, src, start_y, end_y, bh
     mov            srcq, r2mp
     mov        start_yq, r4mp
     mov          end_yq, r5mp
@@ -421,9 +421,9 @@ VERTICAL_EXTEND 16, 22
 %assign %%n %1
 %rep 1+(%2-%1)/2
 %if cpuflag(avx2)
-cglobal emu_edge_hfix %+ %%n, 4, 4, 1, dst, dst_stride, start_x, bh
+cglobal emu_edge_hfix %+ %%n, 4, 4, 1, "p", dst, "q", dst_stride, "q", start_x, "q", bh
 %else
-cglobal emu_edge_hfix %+ %%n, 4, 5, 1, dst, dst_stride, start_x, bh, val
+cglobal emu_edge_hfix %+ %%n, 4, 5, 1, "p", dst, "q", dst_stride, "q", start_x, "q", bh, val
 %endif
 .loop_y:                                        ; do {
     READ_V_PIXEL    %%n, [dstq+start_xq]        ;   $variable_regs = read($n)
@@ -451,7 +451,7 @@ H_EXTEND 8, 22
 %endif
 
 %macro PREFETCH_FN 1
-cglobal prefetch, 3, 3, 0, buf, stride, h
+cglobal prefetch, 3, 3, 0, "p", buf, "p-", stride, "d", h
 .loop:
     %1      [bufq]
     add      bufq, strideq
diff --git a/libavcodec/x86/vorbisdsp.asm b/libavcodec/x86/vorbisdsp.asm
index d952296716..c219167e93 100644
--- a/libavcodec/x86/vorbisdsp.asm
+++ b/libavcodec/x86/vorbisdsp.asm
@@ -29,7 +29,7 @@ SECTION .text
 
 %if ARCH_X86_32
 INIT_MMX 3dnow
-cglobal vorbis_inverse_coupling, 3, 3, 6, mag, ang, block_size
+cglobal vorbis_inverse_coupling, 3, 3, 6, "p", mag, "p", ang, "p-", block_size
     pxor                     m7, m7
     lea                    magq, [magq+block_sizeq*4]
     lea                    angq, [angq+block_sizeq*4]
@@ -57,7 +57,7 @@ cglobal vorbis_inverse_coupling, 3, 3, 6, mag, ang, block_size
 %endif
 
 INIT_XMM sse
-cglobal vorbis_inverse_coupling, 3, 3, 6, mag, ang, block_size
+cglobal vorbis_inverse_coupling, 3, 3, 6, "p", mag, "p", ang, "p-", block_size
     mova                     m5, [pdw_80000000]
     shl             block_sized, 2
     add                    magq, block_sizeq
diff --git a/libavcodec/x86/vp3dsp.asm b/libavcodec/x86/vp3dsp.asm
index d88d5a1edf..8ec599d95d 100644
--- a/libavcodec/x86/vp3dsp.asm
+++ b/libavcodec/x86/vp3dsp.asm
@@ -104,7 +104,7 @@ SECTION .text
 %endmacro
 
 INIT_MMX mmxext
-cglobal vp3_v_loop_filter, 3, 4
+cglobal vp3_v_loop_filter, 3, 4, "p", src, "p-", stride, "p", bounding_values
     mov           r3, r1
     neg           r1
     movq          m6, [r0+r1*2]
@@ -118,7 +118,7 @@ cglobal vp3_v_loop_filter, 3, 4
     movq     [r0   ], m3
     RET
 
-cglobal vp3_h_loop_filter, 3, 4
+cglobal vp3_h_loop_filter, 3, 4, "p", src, "p-", stride, "p", bounding_values
     lea           r3, [r1*3]
 
     movd          m6, [r0     -2]
@@ -158,7 +158,7 @@ cglobal vp3_h_loop_filter, 3, 4
 %endmacro
 
 INIT_MMX mmx
-cglobal put_vp_no_rnd_pixels8_l2, 5, 6, 0, dst, src1, src2, stride, h, stride3
+cglobal put_vp_no_rnd_pixels8_l2, 5, 6, 0, "p", dst, "p", src1, "p", src2, "p-", stride, "d", h, stride3
     mova   m6, [pb_FE]
     lea    stride3q,[strideq+strideq*2]
 .loop:
@@ -560,7 +560,7 @@ cglobal put_vp_no_rnd_pixels8_l2, 5, 6, 0, dst, src1, src2, stride, h, stride3
 %endmacro
 
 %macro vp3_idct_funcs 0
-cglobal vp3_idct_put, 3, 4, 9
+cglobal vp3_idct_put, 3, 4, 9, "p", dst, "p-", stride, "p", block
     VP3_IDCT      r2
 
     mova          m4, [pb_80]
@@ -616,7 +616,7 @@ cglobal vp3_idct_put, 3, 4, 9
 %endrep
     RET
 
-cglobal vp3_idct_add, 3, 4, 9
+cglobal vp3_idct_add, 3, 4, 9, "p", dst, "p-", stride, "p", block
     VP3_IDCT      r2
 
     lea           r3, [r1*3]
@@ -724,7 +724,7 @@ vp3_idct_funcs
 %endmacro
 
 INIT_MMX mmxext
-cglobal vp3_idct_dc_add, 3, 4
+cglobal vp3_idct_dc_add, 3, 4, "p", dst, "p-", stride, "p", block
     movsx         r3, word [r2]
     mov    word [r2], 0
     lea           r2, [r1*3]
diff --git a/libavcodec/x86/vp6dsp.asm b/libavcodec/x86/vp6dsp.asm
index 0be531e5c2..756dd91b57 100644
--- a/libavcodec/x86/vp6dsp.asm
+++ b/libavcodec/x86/vp6dsp.asm
@@ -116,8 +116,8 @@ SECTION .text
 
 %macro vp6_filter_diag4 0
 ; void ff_vp6_filter_diag4_<opt>(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
-;                                const int16_t h_weight[4], const int16_t v_weights[4])
-cglobal vp6_filter_diag4, 5, 7, 8
+;                                const int16_t h_weights[4], const int16_t v_weights[4])
+cglobal vp6_filter_diag4, 5, 7, 8, "p", dst, "p", src, "p-", stride, "p", h_weights, "p", v_weights
     mov          r5, rsp         ; backup stack pointer
     and         rsp, ~(mmsize-1) ; align stack
 %if mmsize == 16
diff --git a/libavcodec/x86/vp8dsp.asm b/libavcodec/x86/vp8dsp.asm
index 75de5690a1..b25e2f6355 100644
--- a/libavcodec/x86/vp8dsp.asm
+++ b/libavcodec/x86/vp8dsp.asm
@@ -162,7 +162,7 @@ SECTION .text
 ;-------------------------------------------------------------------------------
 
 %macro FILTER_SSSE3 1
-cglobal put_vp8_epel%1_h6, 6, 6 + npicregs, 8, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel%1_h6, 6, 6 + npicregs, 8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     lea      mxd, [mxq*3]
     mova      m3, [filter_h6_shuf2]
     mova      m4, [filter_h6_shuf3]
@@ -202,7 +202,7 @@ cglobal put_vp8_epel%1_h6, 6, 6 + npicregs, 8, dst, dststride, src, srcstride, h
     jg .nextrow
     REP_RET
 
-cglobal put_vp8_epel%1_h4, 6, 6 + npicregs, 7, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel%1_h4, 6, 6 + npicregs, 7, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     shl      mxd, 4
     mova      m2, [pw_256]
     mova      m3, [filter_h2_shuf]
@@ -232,7 +232,7 @@ cglobal put_vp8_epel%1_h4, 6, 6 + npicregs, 7, dst, dststride, src, srcstride, h
     jg .nextrow
     REP_RET
 
-cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_epel%1_v4, 7, 7, 8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     shl      myd, 4
 %ifdef PIC
     lea  picregq, [fourtap_filter_hb_m]
@@ -270,7 +270,7 @@ cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picr
     jg .nextrow
     REP_RET
 
-cglobal put_vp8_epel%1_v6, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_epel%1_v6, 7, 7, 8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     lea      myd, [myq*3]
 %ifdef PIC
     lea  picregq, [sixtap_filter_hb_m]
@@ -324,7 +324,7 @@ FILTER_SSSE3 8
 
 ; 4x4 block, H-only 4-tap filter
 INIT_MMX mmxext
-cglobal put_vp8_epel4_h4, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel4_h4, 6, 6 + npicregs, 0, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     shl       mxd, 4
 %ifdef PIC
     lea   picregq, [fourtap_filter_hw_m]
@@ -372,7 +372,7 @@ cglobal put_vp8_epel4_h4, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, he
 
 ; 4x4 block, H-only 6-tap filter
 INIT_MMX mmxext
-cglobal put_vp8_epel4_h6, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel4_h6, 6, 6 + npicregs, 0, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     lea       mxd, [mxq*3]
 %ifdef PIC
     lea   picregq, [sixtap_filter_hw_m]
@@ -429,7 +429,7 @@ cglobal put_vp8_epel4_h6, 6, 6 + npicregs, 0, dst, dststride, src, srcstride, he
     REP_RET
 
 INIT_XMM sse2
-cglobal put_vp8_epel8_h4, 6, 6 + npicregs, 10, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel8_h4, 6, 6 + npicregs, 10, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     shl      mxd, 5
 %ifdef PIC
     lea  picregq, [fourtap_filter_v_m]
@@ -477,7 +477,7 @@ cglobal put_vp8_epel8_h4, 6, 6 + npicregs, 10, dst, dststride, src, srcstride, h
     REP_RET
 
 INIT_XMM sse2
-cglobal put_vp8_epel8_h6, 6, 6 + npicregs, 14, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_epel8_h6, 6, 6 + npicregs, 14, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     lea      mxd, [mxq*3]
     shl      mxd, 4
 %ifdef PIC
@@ -541,7 +541,7 @@ cglobal put_vp8_epel8_h6, 6, 6 + npicregs, 14, dst, dststride, src, srcstride, h
 
 %macro FILTER_V 1
 ; 4x4 block, V-only 4-tap filter
-cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_epel%1_v4, 7, 7, 8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     shl      myd, 5
 %ifdef PIC
     lea  picregq, [fourtap_filter_v_m]
@@ -594,7 +594,7 @@ cglobal put_vp8_epel%1_v4, 7, 7, 8, dst, dststride, src, srcstride, height, picr
 
 
 ; 4x4 block, V-only 6-tap filter
-cglobal put_vp8_epel%1_v6, 7, 7, 8, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_epel%1_v6, 7, 7, 8, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     shl      myd, 4
     lea      myq, [myq*3]
 %ifdef PIC
@@ -665,7 +665,7 @@ FILTER_V 8
 
 %macro FILTER_BILINEAR 1
 %if cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_v, 7, 7, 5, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_bilinear%1_v, 7, 7, 5, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     shl      myd, 4
 %ifdef PIC
     lea  picregq, [bilinear_filter_vb_m]
@@ -695,7 +695,7 @@ cglobal put_vp8_bilinear%1_v, 7, 7, 5, dst, dststride, src, srcstride, height, p
     movhps [dstq+dststrideq*1], m0
 %endif
 %else ; cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_v, 7, 7, 7, dst, dststride, src, srcstride, height, picreg, my
+cglobal put_vp8_bilinear%1_v, 7, 7, 7, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", picreg, "d", my
     shl      myd, 4
 %ifdef PIC
     lea  picregq, [bilinear_filter_vw_m]
@@ -741,7 +741,7 @@ cglobal put_vp8_bilinear%1_v, 7, 7, 7, dst, dststride, src, srcstride, height, p
     REP_RET
 
 %if cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 5, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 5, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     shl      mxd, 4
 %ifdef PIC
     lea  picregq, [bilinear_filter_vb_m]
@@ -771,7 +771,7 @@ cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 5, dst, dststride, src, srcstride
     movhps [dstq+dststrideq*1], m0
 %endif
 %else ; cpuflag(ssse3)
-cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 7, dst, dststride, src, srcstride, height, mx, picreg
+cglobal put_vp8_bilinear%1_h, 6, 6 + npicregs, 7, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height, "d", mx, picreg
     shl      mxd, 4
 %ifdef PIC
     lea  picregq, [bilinear_filter_vw_m]
@@ -828,7 +828,7 @@ INIT_XMM ssse3
 FILTER_BILINEAR 8
 
 INIT_MMX mmx
-cglobal put_vp8_pixels8, 5, 5, 0, dst, dststride, src, srcstride, height
+cglobal put_vp8_pixels8, 5, 5, 0, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height
 .nextrow:
     movq    mm0, [srcq+srcstrideq*0]
     movq    mm1, [srcq+srcstrideq*1]
@@ -842,7 +842,7 @@ cglobal put_vp8_pixels8, 5, 5, 0, dst, dststride, src, srcstride, height
 
 %if ARCH_X86_32
 INIT_MMX mmx
-cglobal put_vp8_pixels16, 5, 5, 0, dst, dststride, src, srcstride, height
+cglobal put_vp8_pixels16, 5, 5, 0, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height
 .nextrow:
     movq    mm0, [srcq+srcstrideq*0+0]
     movq    mm1, [srcq+srcstrideq*0+8]
@@ -860,7 +860,7 @@ cglobal put_vp8_pixels16, 5, 5, 0, dst, dststride, src, srcstride, height
 %endif
 
 INIT_XMM sse
-cglobal put_vp8_pixels16, 5, 5, 2, dst, dststride, src, srcstride, height
+cglobal put_vp8_pixels16, 5, 5, 2, "p", dst, "p-", dststride, "p", src, "p-", srcstride, "d", height
 .nextrow:
     movups xmm0, [srcq+srcstrideq*0]
     movups xmm1, [srcq+srcstrideq*1]
@@ -897,7 +897,7 @@ cglobal put_vp8_pixels16, 5, 5, 2, dst, dststride, src, srcstride, height
 
 %if ARCH_X86_32
 INIT_MMX mmx
-cglobal vp8_idct_dc_add, 3, 3, 0, dst, block, stride
+cglobal vp8_idct_dc_add, 3, 3, 0, "p", dst, "p", block, "p-", stride
     ; load data
     movd       m0, [blockq]
 
@@ -922,7 +922,7 @@ cglobal vp8_idct_dc_add, 3, 3, 0, dst, block, stride
 %endif
 
 %macro VP8_IDCT_DC_ADD 0
-cglobal vp8_idct_dc_add, 3, 3, 6, dst, block, stride
+cglobal vp8_idct_dc_add, 3, 3, 6, "p", dst, "p", block, "p-", stride
     ; load data
     movd       m0, [blockq]
     pxor       m1, m1
@@ -973,7 +973,7 @@ VP8_IDCT_DC_ADD
 
 %if ARCH_X86_32
 INIT_MMX mmx
-cglobal vp8_idct_dc_add4y, 3, 3, 0, dst, block, stride
+cglobal vp8_idct_dc_add4y, 3, 3, 0, "p", dst, "p", block, "p-", stride
     ; load data
     movd      m0, [blockq+32*0] ; A
     movd      m1, [blockq+32*2] ; C
@@ -1010,7 +1010,7 @@ cglobal vp8_idct_dc_add4y, 3, 3, 0, dst, block, stride
 %endif
 
 INIT_XMM sse2
-cglobal vp8_idct_dc_add4y, 3, 3, 6, dst, block, stride
+cglobal vp8_idct_dc_add4y, 3, 3, 6, "p", dst, "p", block, "p-", stride
     ; load data
     movd      m0, [blockq+32*0] ; A
     movd      m1, [blockq+32*2] ; C
@@ -1045,7 +1045,7 @@ cglobal vp8_idct_dc_add4y, 3, 3, 6, dst, block, stride
 ;-----------------------------------------------------------------------------
 
 INIT_MMX mmx
-cglobal vp8_idct_dc_add4uv, 3, 3, 0, dst, block, stride
+cglobal vp8_idct_dc_add4uv, 3, 3, 0, "p", dst, "p", block, "p-", stride
     ; load data
     movd      m0, [blockq+32*0] ; A
     movd      m1, [blockq+32*2] ; C
@@ -1118,7 +1118,7 @@ cglobal vp8_idct_dc_add4uv, 3, 3, 0, dst, block, stride
 %endmacro
 
 %macro VP8_IDCT_ADD 0
-cglobal vp8_idct_add, 3, 3, 0, dst, block, stride
+cglobal vp8_idct_add, 3, 3, 0, "p", dst, "p", block, "p-", stride
     ; load block data
     movq         m0, [blockq+ 0]
     movq         m1, [blockq+ 8]
@@ -1194,7 +1194,7 @@ VP8_IDCT_ADD
 %endmacro
 
 %macro VP8_DC_WHT 0
-cglobal vp8_luma_dc_wht, 2, 3, 0, block, dc1, dc2
+cglobal vp8_luma_dc_wht, 2, 3, 0, "p", block, "p", dc1, dc2
     movq          m0, [dc1q]
     movq          m1, [dc1q+8]
     movq          m2, [dc1q+16]
diff --git a/libavcodec/x86/vp8dsp_loopfilter.asm b/libavcodec/x86/vp8dsp_loopfilter.asm
index caeb405267..51b2b2df54 100644
--- a/libavcodec/x86/vp8dsp_loopfilter.asm
+++ b/libavcodec/x86/vp8dsp_loopfilter.asm
@@ -268,7 +268,7 @@ SECTION .text
 %endmacro
 
 %macro SIMPLE_LOOPFILTER 2
-cglobal vp8_%1_loop_filter_simple, 3, %2, 8, dst, stride, flim, cntr
+cglobal vp8_%1_loop_filter_simple, 3, %2, 8, "p", dst, "p-", stride, "d", flim, cntr
 %if mmsize == 8 ; mmx/mmxext
     mov         cntrq, 2
 %endif
@@ -444,9 +444,9 @@ SIMPLE_LOOPFILTER h, 5
 %endif
 
 %if %2 == 8 ; chroma
-cglobal vp8_%1_loop_filter8uv_inner, 6, 6, 13, stack_size, dst, dst8, stride, flimE, flimI, hevthr
+cglobal vp8_%1_loop_filter8uv_inner, 6, 6, 13, stack_size, "p", dst, "p", dst8, "p-", stride, "d", flimE, "d", flimI, "d", hevthr
 %else ; luma
-cglobal vp8_%1_loop_filter16y_inner, 5, 5, 13, stack_size, dst, stride, flimE, flimI, hevthr
+cglobal vp8_%1_loop_filter16y_inner, 5, 5, 13, stack_size, "p", dst, "p-", stride, "d", flimE, "d", flimI, "d", hevthr
 %endif
 
 %if cpuflag(ssse3)
@@ -939,9 +939,9 @@ INNER_LOOPFILTER h,  8
 %endif
 
 %if %2 == 8 ; chroma
-cglobal vp8_%1_loop_filter8uv_mbedge, 6, 6, 15, stack_size, dst1, dst8, stride, flimE, flimI, hevthr
+cglobal vp8_%1_loop_filter8uv_mbedge, 6, 6, 15, stack_size, "p", dst1, "p", dst8, "p-", stride, "d", flimE, "d", flimI, "d", hevthr
 %else ; luma
-cglobal vp8_%1_loop_filter16y_mbedge, 5, 5, 15, stack_size, dst1, stride, flimE, flimI, hevthr
+cglobal vp8_%1_loop_filter16y_mbedge, 5, 5, 15, stack_size, "p", dst1, "p-", stride, "d", flimE, "d", flimI, "d", hevthr
 %endif
 
 %if cpuflag(ssse3)
diff --git a/libavcodec/x86/vp9intrapred.asm b/libavcodec/x86/vp9intrapred.asm
index 31f7d449fd..ed34e6be57 100644
--- a/libavcodec/x86/vp9intrapred.asm
+++ b/libavcodec/x86/vp9intrapred.asm
@@ -93,7 +93,7 @@ SECTION .text
 ; dc_NxN(uint8_t *dst, ptrdiff_t stride, const uint8_t *l, const uint8_t *a)
 
 %macro DC_4to8_FUNCS 0
-cglobal vp9_ipred_dc_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dc_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movd                    m0, [lq]
     punpckldq               m0, [aq]
     pxor                    m1, m1
@@ -114,7 +114,7 @@ cglobal vp9_ipred_dc_4x4, 4, 4, 0, dst, stride, l, a
     movd      [dstq+strideq*1], m0
     RET
 
-cglobal vp9_ipred_dc_8x8, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dc_8x8, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [lq]
     movq                    m1, [aq]
     DEFINE_ARGS dst, stride, stride3
@@ -150,7 +150,7 @@ INIT_MMX ssse3
 DC_4to8_FUNCS
 
 %macro DC_16to32_FUNCS 0
-cglobal vp9_ipred_dc_16x16, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dc_16x16, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     mova                    m1, [aq]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -182,7 +182,7 @@ cglobal vp9_ipred_dc_16x16, 4, 4, 3, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dc_32x32, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_dc_32x32, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     mova                    m1, [lq+16]
     mova                    m2, [aq]
@@ -232,7 +232,7 @@ DC_16to32_FUNCS
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_ipred_dc_32x32, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dc_32x32, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     mova                    m1, [aq]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -267,7 +267,7 @@ cglobal vp9_ipred_dc_32x32, 4, 4, 3, dst, stride, l, a
 ; dc_top/left_NxN(uint8_t *dst, ptrdiff_t stride, const uint8_t *l, const uint8_t *a)
 
 %macro DC_1D_4to8_FUNCS 2 ; dir (top or left), arg (a or l)
-cglobal vp9_ipred_dc_%1_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movd                    m0, [%2q]
     pxor                    m1, m1
     psadbw                  m0, m1
@@ -287,7 +287,7 @@ cglobal vp9_ipred_dc_%1_4x4, 4, 4, 0, dst, stride, l, a
     movd      [dstq+strideq*1], m0
     RET
 
-cglobal vp9_ipred_dc_%1_8x8, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_8x8, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [%2q]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -322,7 +322,7 @@ DC_1D_4to8_FUNCS top,  a
 DC_1D_4to8_FUNCS left, l
 
 %macro DC_1D_16to32_FUNCS 2; dir (top or left), arg (a or l)
-cglobal vp9_ipred_dc_%1_16x16, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_16x16, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2q]
     DEFINE_ARGS dst, stride, stride3, cnt
     lea               stride3q, [strideq*3]
@@ -351,7 +351,7 @@ cglobal vp9_ipred_dc_%1_16x16, 4, 4, 3, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2q]
     mova                    m1, [%2q+16]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -397,7 +397,7 @@ DC_1D_16to32_FUNCS left, l
 
 %macro DC_1D_AVX2_FUNCS 2 ; dir (top or left), arg (a or l)
 %if HAVE_AVX2_EXTERNAL
-cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_32x32, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2q]
     DEFINE_ARGS dst, stride, stride3, cnt
     lea               stride3q, [strideq*3]
@@ -434,7 +434,7 @@ DC_1D_AVX2_FUNCS left, l
 ; v
 
 INIT_MMX mmx
-cglobal vp9_ipred_v_8x8, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_v_8x8, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -450,7 +450,7 @@ cglobal vp9_ipred_v_8x8, 4, 4, 0, dst, stride, l, a
     RET
 
 INIT_XMM sse
-cglobal vp9_ipred_v_16x16, 4, 4, 1, dst, stride, l, a
+cglobal vp9_ipred_v_16x16, 4, 4, 1, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3, cnt
     lea               stride3q, [strideq*3]
@@ -466,7 +466,7 @@ cglobal vp9_ipred_v_16x16, 4, 4, 1, dst, stride, l, a
     RET
 
 INIT_XMM sse
-cglobal vp9_ipred_v_32x32, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_v_32x32, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     mova                    m1, [aq+16]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -487,7 +487,7 @@ cglobal vp9_ipred_v_32x32, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_YMM avx
-cglobal vp9_ipred_v_32x32, 4, 4, 1, dst, stride, l, a
+cglobal vp9_ipred_v_32x32, 4, 4, 1, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3, cnt
     lea               stride3q, [strideq*3]
@@ -511,7 +511,7 @@ cglobal vp9_ipred_v_32x32, 4, 4, 1, dst, stride, l, a
 
 %macro H_XMM_FUNCS 2
 %if notcpuflag(avx)
-cglobal vp9_ipred_h_4x4, 3, 4, 1, dst, stride, l, stride3
+cglobal vp9_ipred_h_4x4, 3, 4, 1, "p", dst, "p-", stride, "p", l, stride3
     movd                    m0, [lq]
 %if cpuflag(ssse3)
     pshufb                  m0, [pb_4x3_4x2_4x1_4x0]
@@ -531,7 +531,7 @@ cglobal vp9_ipred_h_4x4, 3, 4, 1, dst, stride, l, stride3
     RET
 %endif
 
-cglobal vp9_ipred_h_8x8, 3, 5, %1, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_8x8, 3, 5, %1, "p", dst, "p-", stride, "p", l, stride3, cnt
 %if cpuflag(ssse3)
     mova                    m2, [pb_8x1_8x0]
     mova                    m3, [pb_8x3_8x2]
@@ -558,7 +558,7 @@ cglobal vp9_ipred_h_8x8, 3, 5, %1, dst, stride, l, stride3, cnt
     jge .loop
     RET
 
-cglobal vp9_ipred_h_16x16, 3, 5, %2, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_16x16, 3, 5, %2, "p", dst, "p-", stride, "p", l, stride3, cnt
 %if cpuflag(ssse3)
     mova                    m5, [pb_1]
     mova                    m6, [pb_2]
@@ -594,7 +594,7 @@ cglobal vp9_ipred_h_16x16, 3, 5, %2, dst, stride, l, stride3, cnt
     jge .loop
     RET
 
-cglobal vp9_ipred_h_32x32, 3, 5, %2, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_32x32, 3, 5, %2, "p", dst, "p-", stride, "p", l, stride3, cnt
 %if cpuflag(ssse3)
     mova                    m5, [pb_1]
     mova                    m6, [pb_2]
@@ -644,7 +644,7 @@ H_XMM_FUNCS 4, 8
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_ipred_h_32x32, 3, 5, 8, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_32x32, 3, 5, 8, "p", dst, "p-", stride, "p", l, stride3, cnt
     mova                    m5, [pb_1]
     mova                    m6, [pb_2]
     mova                    m7, [pb_3]
@@ -671,7 +671,7 @@ cglobal vp9_ipred_h_32x32, 3, 5, 8, dst, stride, l, stride3, cnt
 ; tm
 
 %macro TM_MMX_FUNCS 0
-cglobal vp9_ipred_tm_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_tm_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     pxor                    m1, m1
     movd                    m0, [aq]
     pinsrw                  m2, [aq-1], 0
@@ -715,7 +715,7 @@ INIT_MMX ssse3
 TM_MMX_FUNCS
 
 %macro TM_XMM_FUNCS 0
-cglobal vp9_ipred_tm_8x8, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_tm_8x8, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     pxor                    m1, m1
     movh                    m0, [aq]
     pinsrw                  m2, [aq-1], 0
@@ -753,7 +753,7 @@ cglobal vp9_ipred_tm_8x8, 4, 4, 5, dst, stride, l, a
     jge .loop
     RET
 
-cglobal vp9_ipred_tm_16x16, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_tm_16x16, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     pxor                    m3, m3
     mova                    m0, [aq]
     pinsrw                  m2, [aq-1], 0
@@ -801,7 +801,7 @@ cglobal vp9_ipred_tm_16x16, 4, 4, 8, dst, stride, l, a
 %else
 %define mem 64
 %endif
-cglobal vp9_ipred_tm_32x32, 4, 4, 14, mem, dst, stride, l, a
+cglobal vp9_ipred_tm_32x32, 4, 4, 14, mem, "p", dst, "p-", stride, "p", l, "p", a
     pxor                    m5, m5
     pinsrw                  m4, [aq-1], 0
     mova                    m0, [aq]
@@ -900,7 +900,7 @@ TM_XMM_FUNCS
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_ipred_tm_32x32, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_tm_32x32, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     pxor                    m3, m3
     pinsrw                 xm2, [aq-1], 0
     vinserti128             m2, m2, xm2, 1
@@ -944,7 +944,7 @@ cglobal vp9_ipred_tm_32x32, 4, 4, 8, dst, stride, l, a
 %endmacro
 
 %macro DL_MMX_FUNCS 0
-cglobal vp9_ipred_dl_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dl_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m1, [aq]
 %if cpuflag(ssse3)
     pshufb                  m0, m1, [pb_0to5_2x7]
@@ -977,7 +977,7 @@ INIT_MMX ssse3
 DL_MMX_FUNCS
 
 %macro DL_XMM_FUNCS 0
-cglobal vp9_ipred_dl_8x8, 4, 4, 4, dst, stride, stride5, a
+cglobal vp9_ipred_dl_8x8, 4, 4, 4, "p", dst, "p-", stride, "p*", stride5, "p", a
     movq                    m0, [aq]
     lea               stride5q, [strideq*5]
 %if cpuflag(ssse3)
@@ -1012,7 +1012,7 @@ cglobal vp9_ipred_dl_8x8, 4, 4, 4, dst, stride, stride5, a
     movq      [dstq+stride5q ], m1
     RET
 
-cglobal vp9_ipred_dl_16x16, 4, 4, 6, dst, stride, l, a
+cglobal vp9_ipred_dl_16x16, 4, 4, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
 %if cpuflag(ssse3)
     mova                    m5, [pb_1toE_2xF]
@@ -1056,7 +1056,7 @@ cglobal vp9_ipred_dl_16x16, 4, 4, 6, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dl_32x32, 4, 5, 8, dst, stride, cnt, a, dst16
+cglobal vp9_ipred_dl_32x32, 4, 5, 8, "p", dst, "p-", stride, "p*", cnt, "p", a, dst16
     mova                    m0, [aq]
     mova                    m1, [aq+16]
     PALIGNR                 m2, m1, m0, 1, m4
@@ -1124,7 +1124,7 @@ DL_XMM_FUNCS
 ; dr
 
 %macro DR_MMX_FUNCS 0
-cglobal vp9_ipred_dr_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_dr_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movd                    m0, [lq]
     punpckldq               m0, [aq-1]
     movd                    m1, [aq+3]
@@ -1150,7 +1150,7 @@ INIT_MMX ssse3
 DR_MMX_FUNCS
 
 %macro DR_XMM_FUNCS 0
-cglobal vp9_ipred_dr_8x8, 4, 4, 4, dst, stride, l, a
+cglobal vp9_ipred_dr_8x8, 4, 4, 4, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m1, [lq]
     movhps                  m1, [aq-1]
     movd                    m2, [aq+7]
@@ -1178,7 +1178,7 @@ cglobal vp9_ipred_dr_8x8, 4, 4, 4, dst, stride, l, a
     movhps    [dstq+stride3q ], m0
     RET
 
-cglobal vp9_ipred_dr_16x16, 4, 4, 6, dst, stride, l, a
+cglobal vp9_ipred_dr_16x16, 4, 4, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m1, [lq]
     movu                    m2, [aq-1]
     movd                    m4, [aq+15]
@@ -1209,7 +1209,7 @@ cglobal vp9_ipred_dr_16x16, 4, 4, 6, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dr_32x32, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_dr_32x32, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m1, [lq]
     mova                    m2, [lq+16]
     movu                    m3, [aq-1]
@@ -1257,7 +1257,7 @@ DR_XMM_FUNCS
 ; vl
 
 INIT_MMX mmxext
-cglobal vp9_ipred_vl_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_vl_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [aq]
     psrlq                   m1, m0, 8
     psrlq                   m2, m1, 8
@@ -1273,7 +1273,7 @@ cglobal vp9_ipred_vl_4x4, 4, 4, 0, dst, stride, l, a
     RET
 
 %macro VL_XMM_FUNCS 0
-cglobal vp9_ipred_vl_8x8, 4, 4, 4, dst, stride, l, a
+cglobal vp9_ipred_vl_8x8, 4, 4, 4, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [aq]
 %if cpuflag(ssse3)
     pshufb                  m0, [pb_0to6_9x7]
@@ -1306,7 +1306,7 @@ cglobal vp9_ipred_vl_8x8, 4, 4, 4, dst, stride, l, a
     movq      [dstq+stride3q ], m2
     RET
 
-cglobal vp9_ipred_vl_16x16, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_vl_16x16, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3, cnt
     lea               stride3q, [strideq*3]
@@ -1352,7 +1352,7 @@ cglobal vp9_ipred_vl_16x16, 4, 4, 5, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_vl_32x32, 4, 4, 7, dst, stride, l, a
+cglobal vp9_ipred_vl_32x32, 4, 4, 7, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     mova                    m5, [aq+16]
     DEFINE_ARGS dst, stride, dst16, cnt
@@ -1426,7 +1426,7 @@ VL_XMM_FUNCS
 ; vr
 
 %macro VR_MMX_FUNCS 0
-cglobal vp9_ipred_vr_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_vr_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m1, [aq-1]
     punpckldq               m2, [lq]
     movd                    m0, [aq]
@@ -1472,7 +1472,7 @@ INIT_MMX ssse3
 VR_MMX_FUNCS
 
 %macro VR_XMM_FUNCS 1 ; n_xmm_regs for 16x16
-cglobal vp9_ipred_vr_8x8, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_vr_8x8, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     movu                    m1, [aq-1]
     movhps                  m2, [lq]
     movq                    m0, [aq]
@@ -1524,7 +1524,7 @@ cglobal vp9_ipred_vr_8x8, 4, 4, 5, dst, stride, l, a
     movhps    [dstq+stride3q ], m1
     RET
 
-cglobal vp9_ipred_vr_16x16, 4, 4, %1, dst, stride, l, a
+cglobal vp9_ipred_vr_16x16, 4, 4, %1, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     movu                    m1, [aq-1]
     mova                    m2, [lq]
@@ -1561,7 +1561,7 @@ cglobal vp9_ipred_vr_16x16, 4, 4, %1, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_vr_32x32, 4, 4, 9, dst, stride, l, a
+cglobal vp9_ipred_vr_32x32, 4, 4, 9, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     mova                    m2, [aq+16]
     movu                    m1, [aq-1]
@@ -1640,7 +1640,7 @@ VR_XMM_FUNCS 6
 ; hd
 
 INIT_MMX mmxext
-cglobal vp9_ipred_hd_4x4, 4, 4, 0, dst, stride, l, a
+cglobal vp9_ipred_hd_4x4, 4, 4, 0, "p", dst, "p-", stride, "p", l, "p", a
     movd                    m0, [lq]
     punpckldq               m0, [aq-1]
     DEFINE_ARGS dst, stride, stride3
@@ -1670,7 +1670,7 @@ cglobal vp9_ipred_hd_4x4, 4, 4, 0, dst, stride, l, a
     RET
 
 %macro HD_XMM_FUNCS 0
-cglobal vp9_ipred_hd_8x8, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_hd_8x8, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     movq                    m0, [lq]
     movhps                  m0, [aq-1]
     DEFINE_ARGS dst, stride, stride3, dst4
@@ -1709,7 +1709,7 @@ cglobal vp9_ipred_hd_8x8, 4, 4, 5, dst, stride, l, a
     movq     [dst4q+strideq*0], m2
     RET
 
-cglobal vp9_ipred_hd_16x16, 4, 6, 7, dst, stride, l, a
+cglobal vp9_ipred_hd_16x16, 4, 6, 7, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     movu                    m3, [aq-1]
     DEFINE_ARGS dst, stride, stride4, dst4, dst8, dst12
@@ -1759,7 +1759,7 @@ cglobal vp9_ipred_hd_16x16, 4, 6, 7, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_hd_32x32, 4, 6, 8, dst, stride, l, a
+cglobal vp9_ipred_hd_32x32, 4, 6, 8, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     mova                    m1, [lq+16]
     movu                    m2, [aq-1]
@@ -1848,7 +1848,7 @@ INIT_XMM avx
 HD_XMM_FUNCS
 
 %macro HU_MMX_FUNCS 0
-cglobal vp9_ipred_hu_4x4, 3, 3, 0, dst, stride, l
+cglobal vp9_ipred_hu_4x4, 3, 3, 0, "p", dst, "p-", stride, "p", l
     movd                    m0, [lq]
 %if cpuflag(ssse3)
     pshufb                  m0, [pb_0to2_5x3]
@@ -1880,7 +1880,7 @@ INIT_MMX ssse3
 HU_MMX_FUNCS
 
 %macro HU_XMM_FUNCS 1 ; n_xmm_regs in hu_32x32
-cglobal vp9_ipred_hu_8x8, 3, 4, 4, dst, stride, l
+cglobal vp9_ipred_hu_8x8, 3, 4, 4, "p", dst, "p-", stride, "p", l
     movq                    m0, [lq]
 %if cpuflag(ssse3)
     pshufb                  m0, [pb_0to6_9x7]
@@ -1910,7 +1910,7 @@ cglobal vp9_ipred_hu_8x8, 3, 4, 4, dst, stride, l
     movhps   [dst4q+stride3q ], m2
     RET
 
-cglobal vp9_ipred_hu_16x16, 3, 4, 5, dst, stride, l
+cglobal vp9_ipred_hu_16x16, 3, 4, 5, "p", dst, "p-", stride, "p", l
     mova                    m0, [lq]
 %if cpuflag(ssse3)
     mova                    m3, [pb_2toE_3xF]
@@ -1955,7 +1955,7 @@ cglobal vp9_ipred_hu_16x16, 3, 4, 5, dst, stride, l
     jg .loop
     RET
 
-cglobal vp9_ipred_hu_32x32, 3, 7, %1, dst, stride, l
+cglobal vp9_ipred_hu_32x32, 3, 7, %1, "p", dst, "p-", stride, "p", l
     mova                    m1, [lq]
     mova                    m0, [lq+16]
     PALIGNR                 m2, m0, m1,  1, m5
diff --git a/libavcodec/x86/vp9intrapred_16bpp.asm b/libavcodec/x86/vp9intrapred_16bpp.asm
index 32b698243a..76a72e5264 100644
--- a/libavcodec/x86/vp9intrapred_16bpp.asm
+++ b/libavcodec/x86/vp9intrapred_16bpp.asm
@@ -83,8 +83,7 @@ SECTION .text
 %endmacro
 
 INIT_MMX mmx
-cglobal vp9_ipred_v_4x4_16, 2, 4, 1, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_v_4x4_16, 4, 4, 1, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -95,8 +94,7 @@ cglobal vp9_ipred_v_4x4_16, 2, 4, 1, dst, stride, l, a
     RET
 
 INIT_XMM sse
-cglobal vp9_ipred_v_8x8_16, 2, 4, 1, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_v_8x8_16, 4, 4, 1, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -112,8 +110,7 @@ cglobal vp9_ipred_v_8x8_16, 2, 4, 1, dst, stride, l, a
     RET
 
 INIT_XMM sse
-cglobal vp9_ipred_v_16x16_16, 2, 4, 2, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_v_16x16_16, 4, 4, 2, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]
     mova                    m1, [aq+mmsize]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -134,8 +131,7 @@ cglobal vp9_ipred_v_16x16_16, 2, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse
-cglobal vp9_ipred_v_32x32_16, 2, 4, 4, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_v_32x32_16, 4, 4, 4, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq+mmsize*0]
     mova                    m1, [aq+mmsize*1]
     mova                    m2, [aq+mmsize*2]
@@ -157,7 +153,7 @@ cglobal vp9_ipred_v_32x32_16, 2, 4, 4, dst, stride, l, a
     RET
 
 INIT_MMX mmxext
-cglobal vp9_ipred_h_4x4_16, 3, 3, 4, dst, stride, l, a
+cglobal vp9_ipred_h_4x4_16, 3, 3, 4, "p", dst, "p-", stride, "p", l, "p*", a
     mova                    m3, [lq]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -172,7 +168,7 @@ cglobal vp9_ipred_h_4x4_16, 3, 3, 4, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_h_8x8_16, 3, 3, 4, dst, stride, l, a
+cglobal vp9_ipred_h_8x8_16, 3, 3, 4, "p", dst, "p-", stride, "p", l, "p*", a
     mova                    m2, [lq]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -198,7 +194,7 @@ cglobal vp9_ipred_h_8x8_16, 3, 3, 4, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_h_16x16_16, 3, 5, 4, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_16x16_16, 3, 5, 4, "p", dst, "p-", stride, "p", l, stride3, cnt
     mov                   cntd, 3
     lea               stride3q, [strideq*3]
 .loop:
@@ -222,7 +218,7 @@ cglobal vp9_ipred_h_16x16_16, 3, 5, 4, dst, stride, l, stride3, cnt
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_h_32x32_16, 3, 5, 4, dst, stride, l, stride3, cnt
+cglobal vp9_ipred_h_32x32_16, 3, 5, 4, "p", dst, "p-", stride, "p", l, stride3, cnt
     mov                   cntd, 7
     lea               stride3q, [strideq*3]
 .loop:
@@ -254,7 +250,7 @@ cglobal vp9_ipred_h_32x32_16, 3, 5, 4, dst, stride, l, stride3, cnt
     RET
 
 INIT_MMX mmxext
-cglobal vp9_ipred_dc_4x4_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_4x4_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     paddw                   m0, [aq]
     DEFINE_ARGS dst, stride, stride3
@@ -272,7 +268,7 @@ cglobal vp9_ipred_dc_4x4_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_8x8_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_8x8_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     paddw                   m0, [aq]
     DEFINE_ARGS dst, stride, stride3
@@ -298,7 +294,7 @@ cglobal vp9_ipred_dc_8x8_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_16x16_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_16x16_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     paddw                   m0, [lq+mmsize]
     paddw                   m0, [aq]
@@ -330,7 +326,7 @@ cglobal vp9_ipred_dc_16x16_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_32x32_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_32x32_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq+mmsize*0]
     paddw                   m0, [lq+mmsize*1]
     paddw                   m0, [lq+mmsize*2]
@@ -367,7 +363,7 @@ cglobal vp9_ipred_dc_32x32_16, 4, 4, 2, dst, stride, l, a
 
 %macro DC_1D_FNS 2
 INIT_MMX mmxext
-cglobal vp9_ipred_dc_%1_4x4_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_4x4_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -384,7 +380,7 @@ cglobal vp9_ipred_dc_%1_4x4_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_8x8_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_8x8_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2]
     DEFINE_ARGS dst, stride, stride3
     lea               stride3q, [strideq*3]
@@ -409,7 +405,7 @@ cglobal vp9_ipred_dc_%1_8x8_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_16x16_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_16x16_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2]
     paddw                   m0, [%2+mmsize]
     DEFINE_ARGS dst, stride, stride3, cnt
@@ -439,7 +435,7 @@ cglobal vp9_ipred_dc_%1_16x16_16, 4, 4, 2, dst, stride, l, a
     RET
 
 INIT_XMM sse2
-cglobal vp9_ipred_dc_%1_32x32_16, 4, 4, 2, dst, stride, l, a
+cglobal vp9_ipred_dc_%1_32x32_16, 4, 4, 2, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [%2+mmsize*0]
     paddw                   m0, [%2+mmsize*1]
     paddw                   m0, [%2+mmsize*2]
@@ -474,7 +470,7 @@ DC_1D_FNS top,  aq
 DC_1D_FNS left, lq
 
 INIT_MMX mmxext
-cglobal vp9_ipred_tm_4x4_10, 4, 4, 6, dst, stride, l, a
+cglobal vp9_ipred_tm_4x4_10, 4, 4, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m5, [pw_1023]
 .body:
     mova                    m4, [aq]
@@ -507,12 +503,12 @@ cglobal vp9_ipred_tm_4x4_10, 4, 4, 6, dst, stride, l, a
     mova      [dstq+stride3q ], m3
     RET
 
-cglobal vp9_ipred_tm_4x4_12, 4, 4, 6, dst, stride, l, a
+cglobal vp9_ipred_tm_4x4_12, 4, 4, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m5, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_4x4_10 %+ SUFFIX).body
 
 INIT_XMM sse2
-cglobal vp9_ipred_tm_8x8_10, 4, 5, 7, dst, stride, l, a
+cglobal vp9_ipred_tm_8x8_10, 4, 5, 7, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m4, [pw_1023]
 .body:
     pxor                    m6, m6
@@ -552,12 +548,12 @@ cglobal vp9_ipred_tm_8x8_10, 4, 5, 7, dst, stride, l, a
     jge .loop
     RET
 
-cglobal vp9_ipred_tm_8x8_12, 4, 5, 7, dst, stride, l, a
+cglobal vp9_ipred_tm_8x8_12, 4, 5, 7, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m4, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_8x8_10 %+ SUFFIX).body
 
 INIT_XMM sse2
-cglobal vp9_ipred_tm_16x16_10, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_tm_16x16_10, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m7, [pw_1023]
 .body:
     pxor                    m6, m6
@@ -596,12 +592,12 @@ cglobal vp9_ipred_tm_16x16_10, 4, 4, 8, dst, stride, l, a
     jge .loop
     RET
 
-cglobal vp9_ipred_tm_16x16_12, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_tm_16x16_12, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m7, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_16x16_10 %+ SUFFIX).body
 
 INIT_XMM sse2
-cglobal vp9_ipred_tm_32x32_10, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
+cglobal vp9_ipred_tm_32x32_10, 4, 4, 10, 32 * -ARCH_X86_32, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [pw_1023]
 .body:
     pxor                    m1, m1
@@ -655,7 +651,7 @@ cglobal vp9_ipred_tm_32x32_10, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
     jge .loop
     RET
 
-cglobal vp9_ipred_tm_32x32_12, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
+cglobal vp9_ipred_tm_32x32_12, 4, 4, 10, 32 * -ARCH_X86_32, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_ipred_tm_32x32_10 %+ SUFFIX).body
 
@@ -699,8 +695,7 @@ cglobal vp9_ipred_tm_32x32_12, 4, 4, 10, 32 * -ARCH_X86_32, dst, stride, l, a
 %endmacro
 
 %macro DL_FUNCS 0
-cglobal vp9_ipred_dl_4x4_16, 2, 4, 3, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_4x4_16, 4, 4, 3, "p", dst, "p-", stride, "p*", l, "p", a
     movu                    m1, [aq]                ; abcdefgh
     pshufhw                 m0, m1, q3310           ; abcdefhh
     SHIFT_RIGHT             m1, m1                  ; bcdefghh
@@ -716,8 +711,7 @@ cglobal vp9_ipred_dl_4x4_16, 2, 4, 3, dst, stride, l, a
     movh      [dstq+strideq*2], m1
     RET
 
-cglobal vp9_ipred_dl_8x8_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_8x8_16, 4, 4, 5, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]                ; abcdefgh
 %if cpuflag(ssse3)
     mova                    m4, [pb_2to15_14_15]
@@ -745,8 +739,7 @@ cglobal vp9_ipred_dl_8x8_16, 2, 4, 5, dst, stride, l, a
     mova      [dstq+stride5q ], m1
     RET
 
-cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_16x16_16, 4, 4, 5, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]                ; abcdefgh
     mova                    m3, [aq+mmsize]         ; ijklmnop
     PALIGNR                 m1, m3, m0, 2, m4       ; bcdefghi
@@ -778,8 +771,7 @@ cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dl_32x32_16, 2, 5, 7, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_32x32_16, 4, 5, 7, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq+mmsize*0]       ; abcdefgh
     mova                    m1, [aq+mmsize*1]       ; ijklmnop
     mova                    m2, [aq+mmsize*2]       ; qrstuvwx
@@ -849,8 +841,7 @@ DL_FUNCS
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_16x16_16, 4, 4, 5, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]                   ; abcdefghijklmnop
     vpbroadcastw           xm1, [aq+30]                ; pppppppp
     vperm2i128              m2, m0, m1, q0201          ; ijklmnoppppppppp
@@ -886,8 +877,7 @@ cglobal vp9_ipred_dl_16x16_16, 2, 4, 5, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_dl_32x32_16, 2, 6, 7, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_dl_32x32_16, 4, 6, 7, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq+mmsize*0+ 0]       ; abcdefghijklmnop
     mova                    m1, [aq+mmsize*1+ 0]       ; qrstuvwxyz012345
     vpbroadcastw           xm4, [aq+mmsize*1+30]       ; 55555555
@@ -950,7 +940,7 @@ cglobal vp9_ipred_dl_32x32_16, 2, 6, 7, dst, stride, l, a
 %endif
 
 %macro DR_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_dr_4x4_16, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_dr_4x4_16, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     movh                    m0, [lq]                ; wxyz....
     movhps                  m0, [aq-2]              ; wxyz*abc
     movd                    m1, [aq+6]              ; d.......
@@ -969,7 +959,7 @@ cglobal vp9_ipred_dr_4x4_16, 4, 4, 3, dst, stride, l, a
     movh      [dstq+strideq*0], m0
     RET
 
-cglobal vp9_ipred_dr_8x8_16, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_dr_8x8_16, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]                ; stuvwxyz
     movu                    m1, [aq-2]              ; *abcdefg
     mova                    m2, [aq]                ; abcdefgh
@@ -1002,7 +992,7 @@ cglobal vp9_ipred_dr_8x8_16, 4, 4, 5, dst, stride, l, a
     mova   [dst4q+strideq*0+0], m1
     RET
 
-cglobal vp9_ipred_dr_16x16_16, 4, 4, 7, dst, stride, l, a
+cglobal vp9_ipred_dr_16x16_16, 4, 4, 7, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]                ; klmnopqr
     mova                    m1, [lq+mmsize]         ; stuvwxyz
     movu                    m2, [aq-2]              ; *abcdefg
@@ -1047,7 +1037,7 @@ cglobal vp9_ipred_dr_16x16_16, 4, 4, 7, dst, stride, l, a
     RET
 
 cglobal vp9_ipred_dr_32x32_16, 4, 5, 10 + notcpuflag(ssse3), \
-                               %1 * ARCH_X86_32 * -mmsize, dst, stride, l, a
+                               %1 * ARCH_X86_32 * -mmsize, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq+mmsize*3]       ; a[24-31]
     movu                    m1, [aq+mmsize*3-2]     ; a[23-30]
     psrldq                  m2, m0, 2               ; a[25-31].
@@ -1172,7 +1162,7 @@ DR_FUNCS 2
 
 %if HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_ipred_dr_16x16_16, 4, 5, 6, dst, stride, l, a
+cglobal vp9_ipred_dr_16x16_16, 4, 5, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]                   ; klmnopqrstuvwxyz
     movu                    m1, [aq-2]                 ; *abcdefghijklmno
     mova                    m2, [aq]                   ; abcdefghijklmnop
@@ -1223,7 +1213,7 @@ cglobal vp9_ipred_dr_16x16_16, 4, 5, 6, dst, stride, l, a
     RET
 
 %if ARCH_X86_64
-cglobal vp9_ipred_dr_32x32_16, 4, 7, 10, dst, stride, l, a
+cglobal vp9_ipred_dr_32x32_16, 4, 7, 10, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq+mmsize*0+0]        ; l[0-15]
     mova                    m1, [lq+mmsize*1+0]        ; l[16-31]
     movu                    m2, [aq+mmsize*0-2]        ; *abcdefghijklmno
@@ -1326,8 +1316,7 @@ cglobal vp9_ipred_dr_32x32_16, 4, 7, 10, dst, stride, l, a
 %endif
 
 %macro VL_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_vl_4x4_16, 2, 4, 3, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_vl_4x4_16, 4, 4, 3, "p", dst, "p-", stride, "p*", l, "p", a
     movu                    m0, [aq]                ; abcdefgh
     psrldq                  m1, m0, 2               ; bcdefgh.
     psrldq                  m2, m0, 4               ; cdefgh..
@@ -1344,8 +1333,7 @@ cglobal vp9_ipred_vl_4x4_16, 2, 4, 3, dst, stride, l, a
     movh      [dstq+stride3q ], m2
     RET
 
-cglobal vp9_ipred_vl_8x8_16, 2, 4, 4, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_vl_8x8_16, 4, 4, 4, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq]                ; abcdefgh
 %if cpuflag(ssse3)
     mova                    m3, [pb_2to15_14_15]
@@ -1373,8 +1361,7 @@ cglobal vp9_ipred_vl_8x8_16, 2, 4, 4, dst, stride, l, a
     mova      [dstq+stride3q ], m2
     RET
 
-cglobal vp9_ipred_vl_16x16_16, 2, 4, 6, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_vl_16x16_16, 4, 4, 6, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [aq]
     mova                    m1, [aq+mmsize]
     PALIGNR                 m2, m1, m0, 2, m3
@@ -1411,8 +1398,7 @@ cglobal vp9_ipred_vl_16x16_16, 2, 4, 6, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_vl_32x32_16, 2, 5, 11, %1 * mmsize * ARCH_X86_32, dst, stride, l, a
-    movifnidn               aq, amp
+cglobal vp9_ipred_vl_32x32_16, 4, 5, 11, %1 * mmsize * ARCH_X86_32, "p", dst, "p-", stride, "p*", l, "p", a
     mova                    m0, [aq+mmsize*0]
     mova                    m1, [aq+mmsize*1]
     mova                    m2, [aq+mmsize*2]
@@ -1539,7 +1525,7 @@ INIT_XMM avx
 VL_FUNCS 1
 
 %macro VR_FUNCS 0
-cglobal vp9_ipred_vr_4x4_16, 4, 4, 3, dst, stride, l, a
+cglobal vp9_ipred_vr_4x4_16, 4, 4, 3, "p", dst, "p-", stride, "p", l, "p", a
     movu                    m0, [aq-2]
     movhps                  m1, [lq]
     PALIGNR                 m0, m1, 10, m2          ; xyz*abcd
@@ -1564,7 +1550,7 @@ cglobal vp9_ipred_vr_4x4_16, 4, 4, 3, dst, stride, l, a
     movh      [dstq+stride3q ], m2
     RET
 
-cglobal vp9_ipred_vr_8x8_16, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_vr_8x8_16, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     movu                    m1, [aq-2]              ; *abcdefg
     movu                    m2, [lq]                ; stuvwxyz
     mova                    m0, [aq]                ; abcdefgh
@@ -1599,7 +1585,7 @@ cglobal vp9_ipred_vr_8x8_16, 4, 4, 5, dst, stride, l, a
     mova      [dstq+stride3q ], m3
     RET
 
-cglobal vp9_ipred_vr_16x16_16, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_vr_16x16_16, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     movu                    m1, [aq-2]              ; *abcdefg
     movu                    m2, [aq+mmsize-2]       ; hijklmno
     mova                    m3, [aq]                ; abcdefgh
@@ -1643,7 +1629,7 @@ cglobal vp9_ipred_vr_16x16_16, 4, 4, 8, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_vr_32x32_16, 4, 5, 14, 6 * mmsize * ARCH_X86_32, dst, stride, l, a
+cglobal vp9_ipred_vr_32x32_16, 4, 5, 14, 6 * mmsize * ARCH_X86_32, "p", dst, "p-", stride, "p", l, "p", a
     movu                    m0, [aq+mmsize*0-2]     ; *a[0-6]
     movu                    m1, [aq+mmsize*1-2]     ; a[7-14]
     movu                    m2, [aq+mmsize*2-2]     ; a[15-22]
@@ -1785,7 +1771,7 @@ INIT_XMM avx
 VR_FUNCS
 
 %macro HU_FUNCS 1 ; stack_mem_for_32x32_32bit_function
-cglobal vp9_ipred_hu_4x4_16, 3, 3, 3, dst, stride, l, a
+cglobal vp9_ipred_hu_4x4_16, 3, 3, 3, "p", dst, "p-", stride, "p", l, "p*", a
     movh                    m0, [lq]                ; abcd
 %if cpuflag(ssse3)
     pshufb                  m0, [pb_0to7_67x4]      ; abcddddd
@@ -1808,7 +1794,7 @@ cglobal vp9_ipred_hu_4x4_16, 3, 3, 3, dst, stride, l, a
     movhps    [dstq+stride3q ], m2                  ; dddd
     RET
 
-cglobal vp9_ipred_hu_8x8_16, 3, 3, 4, dst, stride, l, a
+cglobal vp9_ipred_hu_8x8_16, 3, 3, 4, "p", dst, "p-", stride, "p", l, "p*", a
     mova                    m0, [lq]
 %if cpuflag(ssse3)
     mova                    m3, [pb_2to15_14_15]
@@ -1842,7 +1828,7 @@ cglobal vp9_ipred_hu_8x8_16, 3, 3, 4, dst, stride, l, a
     mova     [dstq+stride3q*2], m3
     RET
 
-cglobal vp9_ipred_hu_16x16_16, 3, 4, 6 + notcpuflag(ssse3), dst, stride, l, a
+cglobal vp9_ipred_hu_16x16_16, 3, 4, 6 + notcpuflag(ssse3), "p", dst, "p-", stride, "p", l, "p*", a
     mova                    m0, [lq]
     mova                    m3, [lq+mmsize]
     movu                    m1, [lq+2]
@@ -1892,7 +1878,7 @@ cglobal vp9_ipred_hu_16x16_16, 3, 4, 6 + notcpuflag(ssse3), dst, stride, l, a
     RET
 
 cglobal vp9_ipred_hu_32x32_16, 3, 7, 10 + notcpuflag(ssse3), \
-                               %1 * -mmsize * ARCH_X86_32, dst, stride, l, a
+                               %1 * -mmsize * ARCH_X86_32, "p", dst, "p-", stride, "p", l, "p*", a
     mova                    m2, [lq+mmsize*0+0]
     movu                    m1, [lq+mmsize*0+2]
     movu                    m0, [lq+mmsize*0+4]
@@ -2034,7 +2020,7 @@ INIT_XMM avx
 HU_FUNCS 2
 
 %macro HD_FUNCS 0
-cglobal vp9_ipred_hd_4x4_16, 4, 4, 4, dst, stride, l, a
+cglobal vp9_ipred_hd_4x4_16, 4, 4, 4, "p", dst, "p-", stride, "p", l, "p", a
     movh                    m0, [lq]
     movhps                  m0, [aq-2]
     psrldq                  m1, m0, 2
@@ -2053,7 +2039,7 @@ cglobal vp9_ipred_hd_4x4_16, 4, 4, 4, dst, stride, l, a
     movhps    [dstq+strideq*0], m2
     RET
 
-cglobal vp9_ipred_hd_8x8_16, 4, 4, 5, dst, stride, l, a
+cglobal vp9_ipred_hd_8x8_16, 4, 4, 5, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m0, [lq]
     movu                    m1, [aq-2]
     PALIGNR                 m2, m1, m0, 2, m3
@@ -2087,7 +2073,7 @@ cglobal vp9_ipred_hd_8x8_16, 4, 4, 5, dst, stride, l, a
     jg .loop
     RET
 
-cglobal vp9_ipred_hd_16x16_16, 4, 4, 8, dst, stride, l, a
+cglobal vp9_ipred_hd_16x16_16, 4, 4, 8, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m2, [lq]
     movu                    m1, [lq+2]
     movu                    m0, [lq+4]
@@ -2149,7 +2135,7 @@ cglobal vp9_ipred_hd_16x16_16, 4, 4, 8, dst, stride, l, a
     RET
 
 cglobal vp9_ipred_hd_32x32_16, 4, 4 + 3 * ARCH_X86_64, 14, \
-                               10 * -mmsize * ARCH_X86_32, dst, stride, l, a
+                               10 * -mmsize * ARCH_X86_32, "p", dst, "p-", stride, "p", l, "p", a
     mova                    m2, [lq+mmsize*0+0]
     movu                    m1, [lq+mmsize*0+2]
     movu                    m0, [lq+mmsize*0+4]
diff --git a/libavcodec/x86/vp9itxfm.asm b/libavcodec/x86/vp9itxfm.asm
index 2c63fe514a..7f22a1866e 100644
--- a/libavcodec/x86/vp9itxfm.asm
+++ b/libavcodec/x86/vp9itxfm.asm
@@ -173,7 +173,7 @@ SECTION .text
 ;-------------------------------------------------------------------------------------------
 
 INIT_MMX mmx
-cglobal vp9_iwht_iwht_4x4_add, 3, 3, 0, dst, stride, block, eob
+cglobal vp9_iwht_iwht_4x4_add, 3, 3, 0, "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [blockq+0*8]
     mova                m1, [blockq+1*8]
     mova                m2, [blockq+2*8]
@@ -236,7 +236,7 @@ cglobal vp9_iwht_iwht_4x4_add, 3, 3, 0, dst, stride, block, eob
 
 %macro IDCT_4x4_FN 1
 INIT_MMX %1
-cglobal vp9_idct_idct_4x4_add, 4, 4, 0, dst, stride, block, eob
+cglobal vp9_idct_idct_4x4_add, 4, 4, 0, "p", dst, "p-", stride, "p", block, "d", eob
 
 %if cpuflag(ssse3)
     cmp eobd, 4 ; 2x2 or smaller
@@ -328,7 +328,7 @@ IDCT_4x4_FN ssse3
 
 %macro IADST4_FN 5
 INIT_MMX %5
-cglobal vp9_%1_%3_4x4_add, 3, 3, 0, dst, stride, block, eob
+cglobal vp9_%1_%3_4x4_add, 3, 3, 0, "p", dst, "p-", stride, "p", block, "d", eob
 %if WIN64 && notcpuflag(ssse3)
     WIN64_SPILL_XMM 8
 %endif
@@ -549,7 +549,7 @@ IADST4_FN iadst, IADST4, iadst, IADST4, ssse3
 
 %macro VP9_IDCT_IDCT_8x8_ADD_XMM 2
 INIT_XMM %1
-cglobal vp9_idct_idct_8x8_add, 4, 4, %2, dst, stride, block, eob
+cglobal vp9_idct_idct_8x8_add, 4, 4, %2, "p", dst, "p-", stride, "p", block, "d", eob
 
 %if cpuflag(ssse3)
 %if ARCH_X86_64
@@ -817,7 +817,7 @@ VP9_IDCT_IDCT_8x8_ADD_XMM avx, 13
 
 %macro IADST8_FN 6
 INIT_XMM %5
-cglobal vp9_%1_%3_8x8_add, 3, 3, %6, dst, stride, block, eob
+cglobal vp9_%1_%3_8x8_add, 3, 3, %6, "p", dst, "p-", stride, "p", block, "d", eob
 
 %ifidn %1, idct
 %define first_is_idct 1
@@ -1344,7 +1344,7 @@ IADST8_FN iadst, IADST8, iadst, IADST8, avx, 16
 
 %macro VP9_IDCT_IDCT_16x16_ADD_XMM 1
 INIT_XMM %1
-cglobal vp9_idct_idct_16x16_add, 4, 6, 16, 512, dst, stride, block, eob
+cglobal vp9_idct_idct_16x16_add, 4, 6, 16, 512, "p", dst, "p-", stride, "p", block, "d", eob
 %if cpuflag(ssse3)
     ; 2x2=eob=3, 4x4=eob=10
     cmp eobd, 38
@@ -1534,7 +1534,7 @@ VP9_IDCT_IDCT_16x16_ADD_XMM avx
 
 %if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_idct_idct_16x16_add, 4, 4, 16, dst, stride, block, eob
+cglobal vp9_idct_idct_16x16_add, 4, 4, 16, "p", dst, "p-", stride, "p", block, "d", eob
     cmp eobd, 1 ; faster path for when only DC is set
     jg .idctfull
 
@@ -1947,7 +1947,7 @@ cglobal vp9_idct_idct_16x16_add, 4, 4, 16, dst, stride, block, eob
 
 %macro IADST16_FN 5
 INIT_XMM %5
-cglobal vp9_%1_%3_16x16_add, 3, 6, 16, 512, dst, stride, block, cnt, dst_bak, tmp
+cglobal vp9_%1_%3_16x16_add, 3, 6, 16, 512, "p", dst, "p-", stride, "p", block, cnt, dst_bak, tmp
     mov               cntd, 2
     mov               tmpq, rsp
 .loop1_full:
@@ -2095,7 +2095,7 @@ IADST16_FN iadst, IADST16, iadst, IADST16, avx
 %if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
 %macro IADST16_YMM_FN 4
 INIT_YMM avx2
-cglobal vp9_%1_%3_16x16_add, 4, 4, 16, dst, stride, block, eob
+cglobal vp9_%1_%3_16x16_add, 4, 4, 16, "p", dst, "p-", stride, "p", block, "d", eob
     mova                m1, [blockq+ 32]
     mova                m2, [blockq+ 64]
     mova                m3, [blockq+ 96]
@@ -2906,8 +2906,7 @@ IADST16_YMM_FN iadst, IADST16, iadst, IADST16
 
 %macro VP9_IDCT_IDCT_32x32_ADD_XMM 1
 INIT_XMM %1
-cglobal vp9_idct_idct_32x32_add, 0, 6 + ARCH_X86_64 * 3, 16, 2048, dst, stride, block, eob
-    movifnidn         eobd, dword eobm
+cglobal vp9_idct_idct_32x32_add, 4, 6 + ARCH_X86_64 * 3, 16, 2048, "p*", dst, "p-", stride, "p*", block, "d", eob
 %if cpuflag(ssse3)
     cmp eobd, 135
     jg .idctfull
@@ -2921,9 +2920,9 @@ cglobal vp9_idct_idct_32x32_add, 0, 6 + ARCH_X86_64 * 3, 16, 2048, dst, stride,
 %endif
 
     ; dc-only case
-    movifnidn       blockq, blockmp
-    movifnidn         dstq, dstmp
-    movifnidn      strideq, stridemp
+    movifnidn       blockp, blockmp
+    movifnidn         dstp, dstmp
+    movifnidn      stridep, stridemp
 %if cpuflag(ssse3)
     movd                m0, [blockq]
     mova                m1, [pw_11585x2]
@@ -3115,7 +3114,7 @@ VP9_IDCT_IDCT_32x32_ADD_XMM avx
 
 %if ARCH_X86_64 && HAVE_AVX2_EXTERNAL
 INIT_YMM avx2
-cglobal vp9_idct_idct_32x32_add, 4, 9, 16, 2048, dst, stride, block, eob
+cglobal vp9_idct_idct_32x32_add, 4, 9, 16, 2048, "p", dst, "p-", stride, "p", block, "d", eob
     cmp eobd, 135
     jg .idctfull
     cmp eobd, 1
diff --git a/libavcodec/x86/vp9itxfm_16bpp.asm b/libavcodec/x86/vp9itxfm_16bpp.asm
index 902685edf6..7ad0bc923b 100644
--- a/libavcodec/x86/vp9itxfm_16bpp.asm
+++ b/libavcodec/x86/vp9itxfm_16bpp.asm
@@ -151,7 +151,7 @@ SECTION .text
 ; since the input is only 14+sign bit, which fits in 15+sign words directly.
 
 %macro IWHT4_FN 2 ; bpp, max
-cglobal vp9_iwht_iwht_4x4_add_%1, 3, 3, 8, dst, stride, block, eob
+cglobal vp9_iwht_iwht_4x4_add_%1, 3, 3, 8, "p", dst, "p-", stride, "p", block, "d", eob
     mova                m7, [pw_%2]
     mova                m0, [blockq+0*16+0]
     mova                m1, [blockq+1*16+0]
@@ -244,7 +244,7 @@ IWHT4_FN 12, 4095
 ; in 15+1 words without additional effort, since the coefficients are 15bpp.
 
 %macro IDCT4_10_FN 0
-cglobal vp9_idct_idct_4x4_add_10, 4, 4, 8, dst, stride, block, eob
+cglobal vp9_idct_idct_4x4_add_10, 4, 4, 8, "p", dst, "p-", stride, "p", block, "d", eob
     cmp               eobd, 1
     jg .idctfull
 
@@ -301,7 +301,7 @@ INIT_MMX ssse3
 IDCT4_10_FN
 
 %macro IADST4_FN 4
-cglobal vp9_%1_%3_4x4_add_10, 3, 3, 0, dst, stride, block, eob
+cglobal vp9_%1_%3_4x4_add_10, 3, 3, 0, "p", dst, "p-", stride, "p", block, "d", eob
 %if WIN64 && notcpuflag(ssse3)
     WIN64_SPILL_XMM 8
 %endif
@@ -406,7 +406,7 @@ IADST4_FN iadst, IADST4, iadst, IADST4
 %endmacro
 
 INIT_XMM sse2
-cglobal vp9_idct_idct_4x4_add_12, 4, 4, 8, dst, stride, block, eob
+cglobal vp9_idct_idct_4x4_add_12, 4, 4, 8, "p", dst, "p-", stride, "p", block, "d", eob
     cmp               eobd, 1
     jg .idctfull
 
@@ -565,7 +565,7 @@ cglobal vp9_idct_idct_4x4_add_12, 4, 4, 8, dst, stride, block, eob
 %endmacro
 
 %macro IADST4_12BPP_FN 4
-cglobal vp9_%1_%3_4x4_add_12, 3, 3, 12, 2 * ARCH_X86_32 * mmsize, dst, stride, block, eob
+cglobal vp9_%1_%3_4x4_add_12, 3, 3, 12, 2 * ARCH_X86_32 * mmsize, "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [blockq+0*16]
     mova                m1, [blockq+1*16]
     mova                m2, [blockq+2*16]
@@ -642,7 +642,7 @@ IADST4_12BPP_FN iadst, IADST4, iadst, IADST4
 INIT_XMM sse2
 cglobal vp9_idct_idct_8x8_add_10, 4, 6 + ARCH_X86_64, 14, \
                                   16 * mmsize + 3 * ARCH_X86_32 * mmsize, \
-                                  dst, stride, block, eob
+                                  "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_1023]
     cmp               eobd, 1
     jg .idctfull
@@ -781,7 +781,7 @@ cglobal vp9_idct_idct_8x8_add_10, 4, 6 + ARCH_X86_64, 14, \
 INIT_XMM sse2
 cglobal vp9_idct_idct_8x8_add_12, 4, 6 + ARCH_X86_64, 14, \
                                   16 * mmsize + 3 * ARCH_X86_32 * mmsize, \
-                                  dst, stride, block, eob
+                                  "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_4095]
     cmp               eobd, 1
     jg mangle(private_prefix %+ _ %+ vp9_idct_idct_8x8_add_10 %+ SUFFIX).idctfull
@@ -911,7 +911,7 @@ cglobal vp9_idct_idct_8x8_add_12, 4, 6 + ARCH_X86_64, 14, \
 %macro IADST8_FN 5
 cglobal vp9_%1_%3_8x8_add_10, 4, 6 + ARCH_X86_64, 16, \
                               16 * mmsize + ARCH_X86_32 * 6 * mmsize, \
-                              dst, stride, block, eob
+                              "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_1023]
 
 .body:
@@ -998,7 +998,7 @@ cglobal vp9_%1_%3_8x8_add_10, 4, 6 + ARCH_X86_64, 16, \
 
 cglobal vp9_%1_%3_8x8_add_12, 4, 6 + ARCH_X86_64, 16, \
                               16 * mmsize + ARCH_X86_32 * 6 * mmsize, \
-                              dst, stride, block, eob
+                              "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_%1_%3_8x8_add_10 %+ SUFFIX).body
 %endmacro
@@ -1097,7 +1097,7 @@ IADST8_FN iadst, IADST8, iadst, IADST8, default
 INIT_XMM sse2
 cglobal vp9_idct_idct_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
                                     67 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
+                                    "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_1023]
     cmp               eobd, 1
     jg .idctfull
@@ -1250,7 +1250,7 @@ cglobal vp9_idct_idct_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
 INIT_XMM sse2
 cglobal vp9_idct_idct_16x16_add_12, 4, 6 + ARCH_X86_64, 16, \
                                     67 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
+                                    "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_4095]
     cmp               eobd, 1
     jg mangle(private_prefix %+ _ %+ vp9_idct_idct_16x16_add_10 %+ SUFFIX).idctfull
@@ -1435,7 +1435,7 @@ cglobal vp9_idct_idct_16x16_add_12, 4, 6 + ARCH_X86_64, 16, \
 %macro IADST16_FN 7
 cglobal vp9_%1_%4_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
                                 70 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                dst, stride, block, eob
+                                "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_1023]
 
 .body:
@@ -1566,7 +1566,7 @@ cglobal vp9_%1_%4_16x16_add_10, 4, 6 + ARCH_X86_64, 16, \
 
 cglobal vp9_%1_%4_16x16_add_12, 4, 6 + ARCH_X86_64, 16, \
                                 70 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                dst, stride, block, eob
+                                "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_%1_%4_16x16_add_10 %+ SUFFIX).body
 %endmacro
@@ -1927,7 +1927,7 @@ IADST16_FN iadst, IADST16, 70, iadst, IADST16, 70, default
 INIT_XMM sse2
 cglobal vp9_idct_idct_32x32_add_10, 4, 6 + ARCH_X86_64, 16, \
                                     275 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
+                                    "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_1023]
     cmp               eobd, 1
     jg .idctfull
@@ -2020,7 +2020,7 @@ cglobal vp9_idct_idct_32x32_add_10, 4, 6 + ARCH_X86_64, 16, \
 INIT_XMM sse2
 cglobal vp9_idct_idct_32x32_add_12, 4, 6 + ARCH_X86_64, 16, \
                                     275 * mmsize + ARCH_X86_32 * 8 * mmsize, \
-                                    dst, stride, block, eob
+                                    "p", dst, "p-", stride, "p", block, "d", eob
     mova                m0, [pw_4095]
     cmp               eobd, 1
     jg mangle(private_prefix %+ _ %+ vp9_idct_idct_32x32_add_10 %+ SUFFIX).idctfull
diff --git a/libavcodec/x86/vp9lpf.asm b/libavcodec/x86/vp9lpf.asm
index 4e7ede2235..f81e1d3612 100644
--- a/libavcodec/x86/vp9lpf.asm
+++ b/libavcodec/x86/vp9lpf.asm
@@ -372,12 +372,12 @@ SECTION .text
 %endif
 
 %if UNIX64
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 5, 9, 16, %3 + %4 + %%ext, dst, stride, E, I, H, mstride, dst2, stride3, mstride3
+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 5, 9, 16, %3 + %4 + %%ext, "p", dst, "p-", stride, "d", E, "d", I, "d", H, mstride, dst2, stride3, mstride3
 %else
 %if WIN64
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 4, 8, 16, %3 + %4 + %%ext, dst, stride, E, I, mstride, dst2, stride3, mstride3
+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 4, 8, 16, %3 + %4 + %%ext, "p", dst, "p-", stride, "d", E, "d", I, mstride, dst2, stride3, mstride3
 %else
-cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 2, 6, 16, %3 + %4 + %%ext, dst, stride, mstride, dst2, stride3, mstride3
+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 2, 6, 16, %3 + %4 + %%ext, "p", dst, "p-", stride, mstride, dst2, stride3, mstride3
 %define Ed dword r2m
 %define Id dword r3m
 %endif
diff --git a/libavcodec/x86/vp9lpf_16bpp.asm b/libavcodec/x86/vp9lpf_16bpp.asm
index c0888170c9..8f2f6a57bf 100644
--- a/libavcodec/x86/vp9lpf_16bpp.asm
+++ b/libavcodec/x86/vp9lpf_16bpp.asm
@@ -218,7 +218,7 @@ SECTION .text
 %define %%maxf 16
 %endif ; %3
 
-cglobal vp9_loop_filter_%1_%2_%3, 5, %%num_gpr_regs, %%num_xmm_regs, %%stack_mem, dst, stride, E, I, H
+cglobal vp9_loop_filter_%1_%2_%3, 5, %%num_gpr_regs, %%num_xmm_regs, %%stack_mem, "p", dst, "p-", stride, "d", E, "d", I, "d", H
     ; prepare E, I and H masks
     shl                 Ed, %3-8
     shl                 Id, %3-8
diff --git a/libavcodec/x86/vp9mc.asm b/libavcodec/x86/vp9mc.asm
index f64161b2c2..28959211d5 100644
--- a/libavcodec/x86/vp9mc.asm
+++ b/libavcodec/x86/vp9mc.asm
@@ -117,7 +117,7 @@ SECTION .text
 
 %macro filter_sse2_h_fn 1
 %assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 15, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 15, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     pxor        m5, m5
     mova        m6, [pw_64]
     mova        m7, [filteryq+  0]
@@ -202,7 +202,7 @@ filter_sse2_h_fn avg
 
 %macro filter_h_fn 1
 %assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 11, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 11, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova        m6, [pw_256]
     mova        m7, [filteryq+ 0]
 %if ARCH_X86_64 && mmsize > 8
@@ -263,7 +263,7 @@ filter_h_fn avg
 %if ARCH_X86_64
 %macro filter_hx2_fn 1
 %assign %%px mmsize
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 14, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _8, 6, 6, 14, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova       m13, [pw_256]
     mova        m8, [filteryq+ 0]
     mova        m9, [filteryq+32]
@@ -325,9 +325,9 @@ filter_hx2_fn avg
 %macro filter_sse2_v_fn 1
 %assign %%px mmsize/2
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 15, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 15, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 15, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 15, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %define hd r4mp
 %endif
@@ -423,9 +423,9 @@ filter_sse2_v_fn avg
 %macro filter_v_fn 1
 %assign %%px mmsize/2
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 11, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 11, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 11, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 4, 7, 11, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %define hd r4mp
 %endif
@@ -496,7 +496,7 @@ filter_v_fn avg
 
 %macro filter_vx2_fn 1
 %assign %%px mmsize
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 14, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _8, 6, 8, 14, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
     mova       m13, [pw_256]
     lea  sstride3q, [sstrideq*3]
     lea      src4q, [srcq+sstrideq]
@@ -582,11 +582,11 @@ filter_vx2_fn avg
 %endif
 
 %if %2 <= mmsize
-cglobal vp9_%1%2 %+ %%szsuf, 5, 7, 4, dst, dstride, src, sstride, h, dstride3, sstride3
+cglobal vp9_%1%2 %+ %%szsuf, 5, 7, 4, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, dstride3, sstride3
     lea  sstride3q, [sstrideq*3]
     lea  dstride3q, [dstrideq*3]
 %else
-cglobal vp9_%1%2 %+ %%szsuf, 5, 5, %8, dst, dstride, src, sstride, h
+cglobal vp9_%1%2 %+ %%szsuf, 5, 5, %8, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h
 %endif
 .loop:
     %%srcfn     m0, [srcq]
diff --git a/libavcodec/x86/vp9mc_16bpp.asm b/libavcodec/x86/vp9mc_16bpp.asm
index 9a462eaf80..e91ca3b64f 100644
--- a/libavcodec/x86/vp9mc_16bpp.asm
+++ b/libavcodec/x86/vp9mc_16bpp.asm
@@ -32,7 +32,7 @@ cextern pw_4095
 SECTION .text
 
 %macro filter_h4_fn 1-2 12
-cglobal vp9_%1_8tap_1d_h_4_10, 6, 6, %2, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_4_10, 6, 6, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova        m5, [pw_1023]
 .body:
 %if notcpuflag(sse4) && ARCH_X86_64
@@ -103,7 +103,7 @@ cglobal vp9_%1_8tap_1d_h_4_10, 6, 6, %2, dst, dstride, src, sstride, h, filtery
     jg .loop
     RET
 
-cglobal vp9_%1_8tap_1d_h_4_12, 6, 6, %2, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_4_12, 6, 6, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova        m5, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_h_4_10 %+ SUFFIX).body
 %endmacro
@@ -114,7 +114,7 @@ filter_h4_fn avg
 
 %macro filter_h_fn 1-2 12
 %assign %%px mmsize/2
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _10, 6, 6, %2, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _10, 6, 6, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova        m5, [pw_1023]
 .body:
 %if notcpuflag(sse4) && ARCH_X86_64
@@ -193,7 +193,7 @@ cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _10, 6, 6, %2, dst, dstride, src, sstride,
     jg .loop
     RET
 
-cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _12, 6, 6, %2, dst, dstride, src, sstride, h, filtery
+cglobal vp9_%1_8tap_1d_h_ %+ %%px %+ _12, 6, 6, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery
     mova        m5, [pw_4095]
     jmp mangle(private_prefix %+ _ %+ vp9_%1_8tap_1d_h_ %+ %%px %+ _10 %+ SUFFIX).body
 %endmacro
@@ -209,9 +209,9 @@ filter_h_fn avg
 
 %macro filter_v4_fn 1-2 12
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_4_10, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_4_10, 6, 8, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_4_10, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_4_10, 4, 7, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %define hd r4mp
 %endif
@@ -293,9 +293,9 @@ cglobal vp9_%1_8tap_1d_v_4_10, 4, 7, %2, dst, dstride, src, sstride, filtery, sr
     RET
 
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_4_12, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_4_12, 6, 8, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_4_12, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_4_12, 4, 7, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %endif
     mova        m5, [pw_4095]
@@ -309,9 +309,9 @@ filter_v4_fn avg
 %macro filter_v_fn 1-2 13
 %assign %%px mmsize/2
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 6, 8, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 4, 7, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %define hd r4mp
 %endif
@@ -412,9 +412,9 @@ cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _10, 4, 7, %2, dst, dstride, src, sstride,
     RET
 
 %if ARCH_X86_64
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 6, 8, %2, dst, dstride, src, sstride, h, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 6, 8, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, "d", h, "p", filtery, src4, sstride3
 %else
-cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 4, 7, %2, dst, dstride, src, sstride, filtery, src4, sstride3
+cglobal vp9_%1_8tap_1d_v_ %+ %%px %+ _12, 4, 7, %2, "p", dst, "p-", dstride, "p", src, "p-", sstride, filtery, src4, sstride3
     mov   filteryq, r5mp
 %endif
     mova        m5, [pw_4095]
diff --git a/libavcodec/x86/xvididct.asm b/libavcodec/x86/xvididct.asm
index 0220885da6..6039f32e53 100644
--- a/libavcodec/x86/xvididct.asm
+++ b/libavcodec/x86/xvididct.asm
@@ -651,13 +651,13 @@ SECTION .text
     %define NUM_GPRS 7
 %endif
 %if %1 == 0
-cglobal xvid_idct, 1, NUM_GPRS, 8+7*ARCH_X86_64, block
+cglobal xvid_idct, 1, NUM_GPRS, 8+7*ARCH_X86_64, "p", block
 %xdefine BLOCK blockq
 %else
     %if %1 == 1
-cglobal xvid_idct_put, 0, NUM_GPRS, 8+7*ARCH_X86_64, dest, stride, block
+cglobal xvid_idct_put, 0, NUM_GPRS, 8+7*ARCH_X86_64, "p", dest, "p-", stride, "p", block
     %else
-cglobal xvid_idct_add, 0, NUM_GPRS, 8+7*ARCH_X86_64, dest, stride, block
+cglobal xvid_idct_add, 0, NUM_GPRS, 8+7*ARCH_X86_64, "p", dest, "p-", stride, "p", block
     %endif
     %if ARCH_X86_64
     %xdefine BLOCK blockq
@@ -952,7 +952,7 @@ IDCT_SSE2 2
 %endmacro
 
 %macro XVID_IDCT_MMX 0
-cglobal xvid_idct, 1, 1, 0, block
+cglobal xvid_idct, 1, 1, 0, "p", block
 %if cpuflag(mmxext)
 %define TAB tab_i_04_xmm
 %else
-- 
2.21.0

